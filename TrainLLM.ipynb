{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pretrain on unlabeled data",
   "id": "96029241f3323c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:27:41.043436Z",
     "start_time": "2025-06-13T00:27:39.532373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from gpt2_v1 import load_model, complete_text, text_to_tensor\n",
    "\n",
    "start_context = \"Hello, my name is\"\n",
    "model = load_model()\n",
    "print(complete_text(start_context, model,10))"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is hauled manyrikesdeconder ZionJS partingachusetts Civic\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate Text without training",
   "id": "59d0d184b619cfd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:27:45.935989Z",
     "start_time": "2025-06-13T00:27:45.850065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v1 import dataloader_v1\n",
    "\n",
    "with open(\"world_war_ii.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = dataloader_v1(raw_text,batch_size=2, context_size=4,stride=1)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"shape of input: \",inputs.shape)\n",
    "print(\"first batch, input: \\n\", inputs,\"\\n targets: \\n\", targets)"
   ],
   "id": "4421717b10a6f375",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input:  torch.Size([2, 4])\n",
      "first batch, input: \n",
      " tensor([[10603,  1810,   314,   393],\n",
      "        [ 1810,   314,   393,   262]]) \n",
      " targets: \n",
      " tensor([[1810,  314,  393,  262],\n",
      "        [ 314,  393,  262, 3274]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:27:51.642912Z",
     "start_time": "2025-06-13T00:27:51.639187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v1 import tensor_to_text,build_tokenizer\n",
    "tokenizer = build_tokenizer()\n",
    "for i in range(inputs.size(0)):\n",
    "    text = tensor_to_text(inputs[i].unsqueeze(0), tokenizer)\n",
    "    print(f\"Input {i}: {text}\")\n",
    "\n",
    "for i in range(targets.size(0)):\n",
    "    text = tensor_to_text(targets[i].unsqueeze(0), tokenizer)\n",
    "    print(f\"target {i}: {text}\")"
   ],
   "id": "ecd964b80608361d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 0: World War I or\n",
      "Input 1:  War I or the\n",
      "target 0:  War I or the\n",
      "target 1:  I or the First\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:28:33.238393Z",
     "start_time": "2025-06-13T00:28:33.175346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(\"shape of logits: \",logits.shape)\n",
    "print(\"shape of probas: \",probas.shape)"
   ],
   "id": "2a0917e20a6cdcf0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of logits:  torch.Size([2, 4, 50257])\n",
      "shape of probas:  torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:28:35.382580Z",
     "start_time": "2025-06-13T00:28:35.378160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_token_ids = torch.argmax(probas, dim=-1) # Replace probas with logits yield same result\n",
    "print(\"shape of output_token_ids: \",output_token_ids.shape)\n",
    "print(\"output_token_ids: \\n\",output_token_ids)\n",
    "\n",
    "for i in range(output_token_ids.size(0)):\n",
    "    text = tensor_to_text(output_token_ids[i].unsqueeze(0), tokenizer)\n",
    "    print(f\"output {i}: {text}\")"
   ],
   "id": "215416f63d005e46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output_token_ids:  torch.Size([2, 4])\n",
      "output_token_ids: \n",
      " tensor([[38491,  2448, 36069, 24862],\n",
      "        [36397, 15489, 10460, 18747]])\n",
      "output 0:  constants Per Rebels myriad\n",
      "output 1:  Gathering bay 800array\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loss: Cross-Entropy and Perplexity",
   "id": "e550ca1ef5764591"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:28:37.649197Z",
     "start_time": "2025-06-13T00:28:37.646643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"shape of probas: \",probas.shape)\n",
    "print(\"shape of targets: \",targets.shape)\n",
    "print(targets)\n"
   ],
   "id": "adbeb698f6e1d36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of probas:  torch.Size([2, 4, 50257])\n",
      "shape of targets:  torch.Size([2, 4])\n",
      "tensor([[1810,  314,  393,  262],\n",
      "        [ 314,  393,  262, 3274]])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:28:39.730141Z",
     "start_time": "2025-06-13T00:28:39.725395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size, seq_len = targets.shape\n",
    "target_probas = torch.empty(batch_size, seq_len)\n",
    "\n",
    "for text_idx in range(batch_size):\n",
    "    positions = torch.arange(seq_len)\n",
    "    #same as probas[0,[0,1,2,3],[2402,  257,  640,  612]], advanced indexing\n",
    "    target_probas[text_idx] = probas[text_idx, positions, targets[text_idx]]\n",
    "    #Note that even for the same token ID, the predicted probabilities can vary across positions or sequences because the model's output depends heavily on the surrounding context.\n",
    "    print(f\"Text {text_idx + 1} target_probas:\", target_probas[text_idx])\n"
   ],
   "id": "f21ae3e75ca8da82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 target_probas: tensor([9.9453e-06, 1.9406e-05, 1.4083e-05, 1.9301e-05])\n",
      "Text 2 target_probas: tensor([1.6926e-05, 2.1976e-05, 1.0548e-05, 1.9390e-05])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Cross Entropy Loss\n",
    "\n",
    "For a single classification sample, assume:\n",
    "\n",
    "- True label (one-hot):\n",
    "  $$\\mathbf{y} = (y_1, y_2, \\dots, y_C), \\quad y_i \\in \\{0, 1\\}$$\n",
    "\n",
    "- Predicted probabilities:\n",
    "  $$\\hat{\\mathbf{y}} = (\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_C), \\quad \\sum_{i=1}^C \\hat{y}_i = 1$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Cross Entropy Loss (General Form)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^C y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "If the true class is \\(k\\), the formula simplifies to:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\log(\\hat{y}_k)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Cross Entropy Over a Batch of \\(N\\) Samples\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{batch}} = - \\frac{1}{N} \\sum_{n=1}^N \\sum_{i=1}^C y_i^{(n)} \\log \\left( \\hat{y}_i^{(n)} \\right)\n",
    "$$\n"
   ],
   "id": "529a75dc72059a40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:28:44.109728Z",
     "start_time": "2025-06-13T00:28:44.105482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "neg_log_probas = torch.log(target_probas) * -1\n",
    "print(\"neg_log_probas: \",neg_log_probas)\n",
    "loss = torch.mean(neg_log_probas)\n",
    "print(\"loss: \",loss)"
   ],
   "id": "a7f711896f0272f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_log_probas:  tensor([[11.5184, 10.8499, 11.1706, 10.8554],\n",
      "        [10.9867, 10.7256, 11.4596, 10.8508]])\n",
      "loss:  tensor(11.0521)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:28:45.751437Z",
     "start_time": "2025-06-13T00:28:45.746120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"shape of inputs: \",logits.shape) #(batch_size, seq_len, vocab_size)\n",
    "print(\"shape of targets: \",targets.shape)\n",
    "print(\"targets: \\n\",targets) #(batch_size, seq_len)\n",
    "# inputs must be raw logits (unnormalized scores), NOT probabilities\n",
    "# inputs shape: (batch_size * seq_len, vocab_size)\n",
    "# targets shape: (batch_size * seq_len,), containing class indices\n",
    "loss = torch.nn.functional.cross_entropy(logits.view(-1,logits.size(-1)), targets.view(-1))\n",
    "print(\"loss: \",loss)"
   ],
   "id": "eec9532018efd4a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inputs:  torch.Size([2, 4, 50257])\n",
      "shape of targets:  torch.Size([2, 4])\n",
      "targets: \n",
      " tensor([[1810,  314,  393,  262],\n",
      "        [ 314,  393,  262, 3274]])\n",
      "loss:  tensor(11.0521)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Perplexity and Cross Entropy Loss\n",
    "\n",
    "Given the average cross entropy loss \\(\\mathcal{L}\\) defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^N \\log P(w_i)\n",
    "$$\n",
    "\n",
    "The **perplexity** is computed by exponentiating the loss:\n",
    "\n",
    "$$\n",
    "\\mathrm{Perplexity} = e^{\\mathcal{L}}\n",
    "$$\n",
    "\n",
    "Perplexity can indeed be larger than the vocabulary size, though usually it’s not.\n"
   ],
   "id": "4a0efade7bf3a4fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:28:48.016241Z",
     "start_time": "2025-06-13T00:28:48.012747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "perplexity = torch.exp(loss)\n",
    "#Note perplexity is larger than vocab_size, which is expected, since the model is not trained yet.\n",
    "print(\"perplexity: \",perplexity)"
   ],
   "id": "7bd5f352b1a8ff22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:  tensor(63076.7070)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Losses on the training and validation sets\n",
   "id": "43242757c26e2a3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:28:49.696030Z",
     "start_time": "2025-06-13T00:28:49.684709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If you didn't clean the empty lines, LLM may learn to add too many blanks.\n",
    "def clean_text_remove_empty_lines(text: str) -> str:\n",
    "    lines = text.splitlines()\n",
    "    non_empty_lines = [line.strip() for line in lines if line.strip() != \"\"]\n",
    "    return \"\\n\".join(non_empty_lines)\n",
    "\n",
    "with open(\"world_war_ii.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "cleaned_text = clean_text_remove_empty_lines(raw_text)\n",
    "\n",
    "print(cleaned_text[:200])\n",
    "tokens = tokenizer.encode(cleaned_text)\n",
    "print(\"Characters: \",len(cleaned_text))\n",
    "print(\"Tokens: \",len(tokens))"
   ],
   "id": "54ac217af9e00aa1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World War I or the First World War (28 July 1914 – 11 November 1918), also known as the Great War, was a global conflict between two coalitions: the Allies (or Entente) and the Central Powers. Fightin\n",
      "Characters:  88775\n",
      "Tokens:  18134\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:29:00.510059Z",
     "start_time": "2025-06-13T00:29:00.493531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v1 import GPT_CONFIG_124M\n",
    "\n",
    "# Split text data into training and validation sets\n",
    "train_ratio = 0.8\n",
    "split_idx = int(len(cleaned_text) * train_ratio)\n",
    "train_data, val_data = cleaned_text[:split_idx], cleaned_text[split_idx:]\n",
    "print(\"Train data: \", len(train_data))\n",
    "print(\"Val data: \", len(val_data))\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loader = dataloader_v1(\n",
    "    train_data, batch_size=2,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True, shuffle=True)\n",
    "val_loader = dataloader_v1(\n",
    "    val_data, batch_size=2,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False, shuffle=False)\n"
   ],
   "id": "89541052c1784c96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  71020\n",
      "Val data:  17755\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:29:11.780852Z",
     "start_time": "2025-06-13T00:29:11.777039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Train dataloader: \", len(train_loader))\n",
    "train_first_batch = next(iter(train_loader))\n",
    "print(train_first_batch[0].shape, train_first_batch[1].shape)\n",
    "print(\"Val dataloader: \", len(val_loader))\n",
    "val_first_batch = next(iter(val_loader))\n",
    "print(val_first_batch[0].shape, val_first_batch[1].shape)\n"
   ],
   "id": "5e5c630757c48d16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataloader:  7\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "Val dataloader:  2\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:29:13.484122Z",
     "start_time": "2025-06-13T00:29:13.480778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loss_batch(inputs, targets, model, device):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    logits = model(inputs)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), targets.flatten(0))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_loader(loader, model, device, num_batches=None):\n",
    "    if len(loader) == 0:\n",
    "        return float('nan')\n",
    "\n",
    "    total_loss = 0.0\n",
    "    # num_batches no more than len(loader), default to len(loader)\n",
    "    num_batches = min(num_batches or len(loader), len(loader))\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        loss = loss_batch(inputs, targets, model, device)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches"
   ],
   "id": "15723db4b3ce982b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T09:24:39.395690Z",
     "start_time": "2025-06-08T09:24:30.144117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MPS may have some issues when training, using cpu for play\n",
    "# device = (\n",
    "#     torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "#     else torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "#     else torch.device(\"cpu\")\n",
    "# )\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "with torch.no_grad():\n",
    "    train_loss = loss_loader(train_loader, model, device)\n",
    "    val_loss = loss_loader(val_loader, model, device)\n",
    "print(\"Train loss: \", train_loss)\n",
    "print(\"Val loss: \", val_loss)"
   ],
   "id": "4e1fb5a7899e0d29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  11.002508980887276\n",
      "Val loss:  10.987592697143555\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train",
   "id": "96731e56198e205c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:29:21.832442Z",
     "start_time": "2025-06-13T00:29:21.826961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, tokens_seen_track = [], [], []\n",
    "    tokens_seen, step = 0, 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tokens_seen += input_batch.numel()\n",
    "            step += 1\n",
    "\n",
    "            if step % eval_freq == 0:\n",
    "                train_loss = loss_loader(train_loader, model, device, eval_iter)\n",
    "                val_loss = loss_loader(val_loader, model, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                tokens_seen_track.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, tokens_seen_track\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = complete_text(start_context, model,20)\n",
    "        print(result)\n",
    "    model.train()\n"
   ],
   "id": "370c1fe6bc408a88",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T09:36:02.306696Z",
     "start_time": "2025-06-08T09:26:55.229465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v1 import GPT2Model\n",
    "import time\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "# Initialize model and optimizer\n",
    "model = GPT2Model(GPT_CONFIG_124M).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=0.1)\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"at the start of\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Report execution time\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f\"Training completed in {elapsed:.2f} minutes.\")\n"
   ],
   "id": "cb32329ab923e333",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000005): Train loss 8.430, Val loss 8.680\n",
      "at the start of the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ep 2 (Step 000010): Train loss 7.085, Val loss 7.874\n",
      "at the start of the the, the, the, the, the, the, the, the, the, the, the, the, the, the the the the, the, the, the the the, the, the, the, the, the,\n",
      "Ep 3 (Step 000015): Train loss 6.544, Val loss 7.768\n",
      "Ep 3 (Step 000020): Train loss 6.333, Val loss 7.867\n",
      "at the start of the war, the war, and the war, the war, and the war, the war, the war, and the war, and the war, and the war, the war, the war, the war, the war, and the war\n",
      "Ep 4 (Step 000025): Train loss 6.149, Val loss 7.955\n",
      "at the start of the war, and French, and the war, and the war, and the war, and French, and the war, and the war, and the war, and French, and the war, and the war, which, and the war,\n",
      "Ep 5 (Step 000030): Train loss 5.999, Val loss 7.856\n",
      "Ep 5 (Step 000035): Train loss 6.005, Val loss 8.046\n",
      "at the start of the war, and, and, and, and the war, and, and the war, and the war, and the war, and the war.\n",
      "===\n",
      "===\n",
      "===\n",
      "===\n",
      " of the war, the war.\n",
      "===\n",
      "\n",
      "Ep 6 (Step 000040): Train loss 5.695, Val loss 7.746\n",
      "at the start of the war, and French, and the war. The. The, and the war, and French, and the German, and the war, the war, and the war. The German,000, the war. The. The, and the\n",
      "Ep 7 (Step 000045): Train loss 5.378, Val loss 7.734\n",
      "at the start of the war on the war.\n",
      "===\n",
      "==== of the war on the war.\n",
      "===\n",
      "==== of the war.\n",
      "==== of the war.\n",
      "==== British of the war on the war.\n",
      "==== German and the Battle of the war\n",
      "Ep 8 (Step 000050): Train loss 5.066, Val loss 7.661\n",
      "Ep 8 (Step 000055): Train loss 4.779, Val loss 7.621\n",
      "at the start of the war on the war, the war, the German Army, the war, the war, the war on the German, the war on the war on the war on the war, the war. The German Army to the war, the war,\n",
      "Ep 9 (Step 000060): Train loss 4.463, Val loss 7.625\n",
      "at the start of the Battle of the Battle of the war.\n",
      "The, the war.\n",
      "The by the war.\n",
      "==== German, the war on the war. The German forces.\n",
      "The Ottoman Empire, and the war.\n",
      "==== German and the war\n",
      "Ep 10 (Step 000065): Train loss 4.226, Val loss 7.587\n",
      "Ep 10 (Step 000070): Train loss 3.661, Val loss 7.597\n",
      "at the start of the war, and Austria-Hungary and the German Army, and the war.\n",
      "===\n",
      "The German forces to the war, the war, the German, and the war, the war.\n",
      "The German Army, and the war, and\n",
      "Training completed in 9.11 minutes.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T09:26:47.598343Z",
     "start_time": "2025-06-08T09:21:18.847313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs, tokens, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    ax1.plot(epochs, train_losses, label=\"Train loss\")\n",
    "    ax1.plot(epochs, val_losses, linestyle=\"--\", label=\"Val loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n"
   ],
   "id": "7d21c133b9acb67f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decoding Strategies to control randomness",
   "id": "2259581e4a6b8cae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Sample decoding:\n",
    "- Greedy decoding: Select the word with the highest probability (argmax) at each step.\n",
    "\n",
    "- Sampling decoding: Randomly sample the next word from the probability distribution, for example using torch.multinomial."
   ],
   "id": "1e8356fb7cec29a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:29:56.186915Z",
     "start_time": "2025-06-13T00:29:55.483228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)\n"
   ],
   "id": "3d1a23fc72ed2dfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of damsiman Byeswick Exit In despise forcefully flash tit fire adop marched barric sponsoring\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:30:07.586038Z",
     "start_time": "2025-06-13T00:30:07.518192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "input_text = \"At the start of the\"\n",
    "input_tensor = text_to_tensor(input_text, tokenizer)\n",
    "print(\"Input tensor: \", input_tensor)\n",
    "\n",
    "logits = model(input_tensor)\n",
    "print(\"Shape of logits: \", logits.shape)\n",
    "\n",
    "next_token_logits = logits[:, -1, :]\n",
    "print(\"Shape of next_token_logits: \", next_token_logits.shape)\n",
    "print(\"next_token_logits: \", next_token_logits)\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=-1)\n",
    "next_token_id = torch.argmax(probas, dim=-1).item()\n",
    "print(\"Next token id: \", next_token_id)\n",
    "\n",
    "next_token = tokenizer.decode(next_token_id)\n",
    "print(\"Next token: \", next_token)"
   ],
   "id": "889955a772ef919",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:  tensor([[2953,  262,  923,  286,  262]])\n",
      "Shape of logits:  torch.Size([1, 5, 50257])\n",
      "Shape of next_token_logits:  torch.Size([1, 50257])\n",
      "next_token_logits:  tensor([[ 1.0101,  0.1772, -0.5975,  ..., -0.0588,  0.1193, -0.3845]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Next token id:  12481\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'tokens': 'int' object cannot be converted to 'Sequence'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m next_token_id \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(probas, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNext token id: \u001B[39m\u001B[38;5;124m\"\u001B[39m, next_token_id)\n\u001B[0;32m---> 18\u001B[0m next_token \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_token_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNext token: \u001B[39m\u001B[38;5;124m\"\u001B[39m, next_token)\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/tiktoken/core.py:284\u001B[0m, in \u001B[0;36mEncoding.decode\u001B[0;34m(self, tokens, errors)\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokens: Sequence[\u001B[38;5;28mint\u001B[39m], errors: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreplace\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    273\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Decodes a list of tokens into a string.\u001B[39;00m\n\u001B[1;32m    274\u001B[0m \n\u001B[1;32m    275\u001B[0m \u001B[38;5;124;03m    WARNING: the default behaviour of this function is lossy, since decoded bytes are not\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_core_bpe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m, errors\u001B[38;5;241m=\u001B[39merrors)\n",
      "\u001B[0;31mTypeError\u001B[0m: argument 'tokens': 'int' object cannot be converted to 'Sequence'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:30:14.367413Z",
     "start_time": "2025-06-13T00:30:14.350487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(\"Next token id: \", next_token_id)\n",
    "next_token = tokenizer.decode(next_token_id)\n",
    "print(\"Next token: \", next_token)"
   ],
   "id": "9372b48e12516e8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token id:  44028\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'tokens': 'int' object cannot be converted to 'Sequence'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m next_token_id \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmultinomial(probas, num_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNext token id: \u001B[39m\u001B[38;5;124m\"\u001B[39m, next_token_id)\n\u001B[0;32m----> 4\u001B[0m next_token \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_token_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNext token: \u001B[39m\u001B[38;5;124m\"\u001B[39m, next_token)\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/tiktoken/core.py:284\u001B[0m, in \u001B[0;36mEncoding.decode\u001B[0;34m(self, tokens, errors)\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokens: Sequence[\u001B[38;5;28mint\u001B[39m], errors: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreplace\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    273\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Decodes a list of tokens into a string.\u001B[39;00m\n\u001B[1;32m    274\u001B[0m \n\u001B[1;32m    275\u001B[0m \u001B[38;5;124;03m    WARNING: the default behaviour of this function is lossy, since decoded bytes are not\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_core_bpe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m, errors\u001B[38;5;241m=\u001B[39merrors)\n",
      "\u001B[0;31mTypeError\u001B[0m: argument 'tokens': 'int' object cannot be converted to 'Sequence'"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:30:22.926211Z",
     "start_time": "2025-06-13T00:30:22.698657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for _ in range(100)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=probas.shape[-1])\n",
    "    for id, freq in enumerate(sampled_ids):\n",
    "        if freq > 1:\n",
    "            print(f\"{freq} x {tokenizer.decode(id)}\")\n",
    "\n",
    "print_sampled_tokens(probas)\n"
   ],
   "id": "7bf3b0001a1e6267",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T11:52:23.134510Z",
     "start_time": "2025-06-08T11:52:23.128800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "#Complete 'At the start of the'\n",
    "possible_text = \"war battle revolution novel experiment day journey movement\"\n",
    "words = possible_text.lower().split()\n",
    "vocab = {word: idx for idx, word in enumerate(words)}\n",
    "inverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Step 2: Generate random logits for each vocab token\n",
    "vocab_size = len(vocab)\n",
    "torch.manual_seed(123)\n",
    "next_token_logits = torch.normal(mean=0.0, std=4.0, size=(vocab_size,))  # increase std to increase randomness\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "\n",
    "# Pick next token by argmax\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# Decode and print the predicted token\n",
    "print(f\"Next generated token: {inverse_vocab[next_token_id]}\")\n"
   ],
   "id": "7fc286a187b0e82c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next generated token: day\n"
     ]
    }
   ],
   "execution_count": 351
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T11:52:36.569641Z",
     "start_time": "2025-06-08T11:52:36.563458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for _ in range(100)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=probas.shape[-1])\n",
    "    for id, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[id]}\")\n",
    "\n",
    "print_sampled_tokens(probas)\n"
   ],
   "id": "d346f6d29efff149",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 x war\n",
      "31 x battle\n",
      "7 x revolution\n",
      "4 x novel\n",
      "0 x experiment\n",
      "46 x day\n",
      "1 x journey\n",
      "0 x movement\n"
     ]
    }
   ],
   "execution_count": 353
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Temperature",
   "id": "922be060512e70e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T12:50:34.000089Z",
     "start_time": "2025-06-08T12:50:33.991877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1.0, 0.3, 1.5]\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ],
   "id": "607b02e4c0c3d030",
   "outputs": [],
   "execution_count": 357
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T12:50:36.595894Z",
     "start_time": "2025-06-08T12:50:36.511974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "for i, T in enumerate(temperatures):\n",
    "    ax.bar(x + i * bar_width, scaled_probas[i], width=bar_width, label=f\"T = {T}\")\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "4a75ae4015c447b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 358
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### top-k Sampling",
   "id": "b04afc1b1df6d198"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T12:54:08.072509Z",
     "start_time": "2025-06-08T12:54:08.068868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(next_token_logits)\n",
    "top_k = 3\n",
    "top_k_logits, top_k_indices = torch.topk(next_token_logits, k=top_k, dim=-1)\n",
    "print(\"top_k_logits: \", top_k_logits)\n",
    "print(\"top_k_indices: \", top_k_indices)"
   ],
   "id": "e02a8d293b34c6dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4459,  0.4815, -1.4785, -0.9617, -4.7877,  0.8371, -3.8894, -3.0202])\n",
      "top_k_logits:  tensor([ 0.8371,  0.4815, -0.4459])\n",
      "top_k_indices:  tensor([5, 1, 0])\n"
     ]
    }
   ],
   "execution_count": 365
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:02:04.323445Z",
     "start_time": "2025-06-08T13:02:04.316715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Mask out logits that are not in the top-k by setting them to -inf\n",
    "threshold = top_k_logits[-1]\n",
    "new_logits = torch.where(\n",
    "    next_token_logits < threshold,\n",
    "    torch.full_like(next_token_logits, float('-inf')),\n",
    "    next_token_logits\n",
    ")\n",
    "\n",
    "print(\"new_logits: \", new_logits)\n",
    "topk_probas = torch.softmax(new_logits, dim=-1)\n",
    "print(\"topk_probas: \", topk_probas)\n",
    "\n",
    "print_sampled_tokens(topk_probas)"
   ],
   "id": "ddc553491edd9050",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_logits:  tensor([-0.4459,  0.4815,    -inf,    -inf,    -inf,  0.8371,    -inf,    -inf])\n",
      "topk_probas:  tensor([0.1402, 0.3543, 0.0000, 0.0000, 0.0000, 0.5056, 0.0000, 0.0000])\n",
      "13 x war\n",
      "34 x battle\n",
      "0 x revolution\n",
      "0 x novel\n",
      "0 x experiment\n",
      "53 x day\n",
      "0 x journey\n",
      "0 x movement\n"
     ]
    }
   ],
   "execution_count": 377
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate text with temperature and top_k",
   "id": "7ffe86906f6a8a84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:30:34.445963Z",
     "start_time": "2025-06-13T00:30:34.440879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get logits from model\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Take logits for the last time step\n",
    "        # (batch, n_tokens, vocab_size) -> (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k, dim=-1)  # (batch, top_k)\n",
    "            threshold = top_logits[:, -1].unsqueeze(-1) # (batch, ) -> (batch, 1)\n",
    "            logits = torch.where(\n",
    "                logits < threshold,\n",
    "                torch.full_like(logits, float('-inf')),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "            # Sample from distribution\n",
    "            idx_next = torch.multinomial(probas, num_samples=1)  # (batch, 1)\n",
    "        else:\n",
    "            # Greedy sampling\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        if eos_id is not None and idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ],
   "id": "ff5f829832467cff",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:30:38.548930Z",
     "start_time": "2025-06-13T00:30:37.747309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(\"At the start of the\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", tensor_to_text(token_ids, tokenizer))"
   ],
   "id": "ac9b0ba2afa03748",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " At the start of theConnection visa regularsuck dissatisfied HEREComeMus snail smack propheticffer ministries%). radically\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train on larger datasets",
   "id": "556ed64f5725c7f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:30:57.610661Z",
     "start_time": "2025-06-13T00:30:43.934011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n"
   ],
   "id": "ff5faf1f8441eadf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/bytedance/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:30:59.505924Z",
     "start_time": "2025-06-13T00:30:59.503515Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset)\n",
   "id": "c280d9385abb5ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:31:02.512413Z",
     "start_time": "2025-06-13T00:31:02.454477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = dataset['train']\n",
    "print(train_dataset['text'][:5])\n",
    "\n",
    "val_dataset = dataset['validation']\n",
    "print(val_dataset['text'][:5])"
   ],
   "id": "7f2e6212c35aff58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', ' = Valkyria Chronicles III = \\n', '', ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]\n",
      "['', ' = Homarus gammarus = \\n', '', ' Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into planktonic larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \\n', '']\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:31:10.629391Z",
     "start_time": "2025-06-13T00:31:10.594818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = \" \".join(train_dataset['text'])\n",
    "print(\"train_data: \", train_data[:100])\n",
    "\n",
    "val_data = \" \".join(val_dataset['text'])\n",
    "print(\"val_data: \", val_data[:100])"
   ],
   "id": "da296d1126813a91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:    = Valkyria Chronicles III = \n",
      "   Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリ\n",
      "val_data:    = Homarus gammarus = \n",
      "   Homarus gammarus , known as the European lobster or common lobster , is a\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T02:02:56.859068Z",
     "start_time": "2025-06-09T14:32:28.051532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "from gpt2_v1 import text_to_tensor, tensor_to_text, complete_text\n",
    "import time\n",
    "from gpt2_v2 import dataloader_v1, GPT2Model, GPT_CONFIG_124M, train_model_simple, build_tokenizer\n",
    "\n",
    "train_loader = dataloader_v1(\n",
    "    train_data, batch_size=16,\n",
    "    context_size=256,\n",
    "    stride=256,\n",
    "    drop_last=True, shuffle=True)\n",
    "val_loader = dataloader_v1(\n",
    "    val_data, batch_size=16,\n",
    "    context_size=256,\n",
    "    stride=256,\n",
    "    drop_last=False, shuffle=False)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "torch.set_num_threads(8)\n",
    "# Initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2Model(GPT_CONFIG_124M).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=0.1)\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=200,\n",
    "    eval_iter=50,\n",
    "    start_context=\"at the start of\",\n",
    "    tokenizer=build_tokenizer()\n",
    ")\n",
    "\n",
    "# Report execution time\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f\"Training completed in {elapsed:.2f} minutes.\")\n"
   ],
   "id": "58cc79b344f82f2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000005): Train loss 8.580, Val loss 8.531, Tokens seen: 20480\n",
      "at the start of the , , the , , and , and , , , , the , , and , , ,\n",
      "Ep 1 (Step 000010): Train loss 7.611, Val loss 7.520, Tokens seen: 40960\n",
      "at the start of the , and the , and the , and the of the , and the , and the , and\n",
      "Ep 1 (Step 000015): Train loss 7.292, Val loss 7.259, Tokens seen: 61440\n",
      "at the start of the                   \n",
      "Ep 1 (Step 000020): Train loss 7.162, Val loss 7.165, Tokens seen: 81920\n",
      "at the start of the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "Ep 1 (Step 000025): Train loss 7.185, Val loss 7.099, Tokens seen: 102400\n",
      "at the start of the \" .                 \n",
      "Ep 1 (Step 000030): Train loss 7.123, Val loss 7.060, Tokens seen: 122880\n",
      "at the start of the first . The . The the first , and the first , and the first , and the first\n",
      "Ep 1 (Step 000035): Train loss 6.979, Val loss 7.002, Tokens seen: 143360\n",
      "at the start of the first , and the first , the first , the first , and the first , the first ,\n",
      "Ep 1 (Step 000040): Train loss 6.961, Val loss 6.967, Tokens seen: 163840\n",
      "at the start of the first .                 \n",
      "Ep 1 (Step 000045): Train loss 7.012, Val loss 6.911, Tokens seen: 184320\n",
      "at the start of the first . The first , and the first , and the first , and the game , and the\n",
      "Ep 1 (Step 000050): Train loss 6.942, Val loss 6.876, Tokens seen: 204800\n",
      "at the start of the first . \n",
      " \n",
      "   \n",
      " \n",
      "       \n",
      "Ep 1 (Step 000055): Train loss 6.886, Val loss 6.846, Tokens seen: 225280\n",
      "at the start of the first . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "    \n",
      "  \n",
      "Ep 1 (Step 000060): Train loss 6.783, Val loss 6.809, Tokens seen: 245760\n",
      "at the start of the first . \n",
      " \n",
      "             \n",
      "Ep 1 (Step 000065): Train loss 6.804, Val loss 6.793, Tokens seen: 266240\n",
      "at the start of the first . The first of the first of the first of the first of the first of the first\n",
      "Ep 1 (Step 000070): Train loss 6.838, Val loss 6.762, Tokens seen: 286720\n",
      "at the start of the song . \n",
      " \n",
      "   \n",
      "         \n",
      "Ep 1 (Step 000075): Train loss 6.780, Val loss 6.766, Tokens seen: 307200\n",
      "at the start of the first . The first of the first of the first of the first of the first of the first\n",
      "Ep 1 (Step 000080): Train loss 6.701, Val loss 6.748, Tokens seen: 327680\n",
      "at the start of the first . The first of the first of the first of the first of the first of the first\n",
      "Ep 1 (Step 000085): Train loss 6.811, Val loss 6.728, Tokens seen: 348160\n",
      "at the start of the first . \n",
      " \n",
      "             \n",
      "Ep 1 (Step 000090): Train loss 6.684, Val loss 6.727, Tokens seen: 368640\n",
      "at the start of the first . The first of the first of the first of the first of the game of the first\n",
      "Ep 1 (Step 000095): Train loss 6.777, Val loss 6.701, Tokens seen: 389120\n",
      "at the start of the first . The first of the first of the first of the first of the first of the first\n",
      "Ep 1 (Step 000100): Train loss 6.770, Val loss 6.684, Tokens seen: 409600\n",
      "at the start of the first . The first to the first to the first to the first to the first to the first\n",
      "Ep 1 (Step 000105): Train loss 6.619, Val loss 6.662, Tokens seen: 430080\n",
      "at the start of the first first first first first first first first first first first first first first first first first first first\n",
      "Ep 1 (Step 000110): Train loss 6.580, Val loss 6.641, Tokens seen: 450560\n",
      "at the start of the first . \n",
      "  = = = = = = = = = = = = = =\n",
      "Ep 1 (Step 000115): Train loss 6.587, Val loss 6.622, Tokens seen: 471040\n",
      "at the start of the first first of the first first first first first first first first first first first first first first first\n",
      "Ep 1 (Step 000120): Train loss 6.502, Val loss 6.614, Tokens seen: 491520\n",
      "at the start of the first . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 1 (Step 000125): Train loss 6.568, Val loss 6.622, Tokens seen: 512000\n",
      "at the start of the first first first first first first first first first first first first . \n",
      "  = = =\n",
      "Ep 1 (Step 000130): Train loss 6.583, Val loss 6.593, Tokens seen: 532480\n",
      "at the start of the first , the first , the first , the first , the first , the first , the first\n",
      "Ep 1 (Step 000135): Train loss 6.546, Val loss 6.558, Tokens seen: 552960\n",
      "at the start of the first the first of the first of the first of the first . \n",
      "  = = =\n",
      "Ep 1 (Step 000140): Train loss 6.570, Val loss 6.554, Tokens seen: 573440\n",
      "at the start of the first of the first of the first of the first of the first of the first of the first\n",
      "Ep 1 (Step 000145): Train loss 6.470, Val loss 6.545, Tokens seen: 593920\n",
      "at the start of the first of the first of the first of the first of the first of the first of the first\n",
      "Ep 1 (Step 000150): Train loss 6.513, Val loss 6.541, Tokens seen: 614400\n",
      "at the start of the song . \n",
      "  = = = = = = = = = = = = = =\n",
      "Ep 1 (Step 000155): Train loss 6.509, Val loss 6.504, Tokens seen: 634880\n",
      "at the start of the first time of the first time of the first time . The first time of the first time of\n",
      "Ep 1 (Step 000160): Train loss 6.389, Val loss 6.502, Tokens seen: 655360\n",
      "at the start of the first time of the first time of the first time of the first time of the first time .\n",
      "Ep 1 (Step 000165): Train loss 6.490, Val loss 6.498, Tokens seen: 675840\n",
      "at the start of the first time of the first time . The first time of the first time of the first time of\n",
      "Ep 1 (Step 000170): Train loss 6.436, Val loss 6.485, Tokens seen: 696320\n",
      "at the start of the first time of the first time of the first of the first time of the same time of the\n",
      "Ep 1 (Step 000175): Train loss 6.450, Val loss 6.448, Tokens seen: 716800\n",
      "at the start of the first time of the first time of the first time of the first time of the first time of\n",
      "Ep 1 (Step 000180): Train loss 6.434, Val loss 6.435, Tokens seen: 737280\n",
      "at the start of the first time of the first time of the first time of the first time of the first time of\n",
      "Ep 1 (Step 000185): Train loss 6.420, Val loss 6.425, Tokens seen: 757760\n",
      "at the start of the first time . The first time , the first time , the first time , the first time ,\n",
      "Ep 1 (Step 000190): Train loss 6.375, Val loss 6.409, Tokens seen: 778240\n",
      "at the start of the first of the first of the first of the first of the first of the first of the first\n",
      "Ep 1 (Step 000195): Train loss 6.369, Val loss 6.399, Tokens seen: 798720\n",
      "at the start of the first of the first of the first time . The first time , the first time , the first\n",
      "Ep 1 (Step 000200): Train loss 6.437, Val loss 6.396, Tokens seen: 819200\n",
      "at the start of the game of the first time of the same of the British . The first time of the British @\n",
      "Ep 1 (Step 000205): Train loss 6.401, Val loss 6.375, Tokens seen: 839680\n",
      "at the start of the first time of the first time of the first time of the first time of the first time of\n",
      "Ep 1 (Step 000210): Train loss 6.298, Val loss 6.368, Tokens seen: 860160\n",
      "at the start of the first time . The first time of the first time , the British season , the first time ,\n",
      "Ep 1 (Step 000215): Train loss 6.300, Val loss 6.355, Tokens seen: 880640\n",
      "at the start of the first time . \n",
      "         = = = = = =\n",
      "Ep 1 (Step 000220): Train loss 6.282, Val loss 6.357, Tokens seen: 901120\n",
      "at the start of the United States . The first time of the United States , the United States , the United States ,\n",
      "Ep 1 (Step 000225): Train loss 6.247, Val loss 6.360, Tokens seen: 921600\n",
      "at the start of the first time . The first time of the first time of the first time of the first time of\n",
      "Ep 1 (Step 000230): Train loss 6.236, Val loss 6.342, Tokens seen: 942080\n",
      "at the start of the first time . The first time of the first time of the first time , the first time of\n",
      "Ep 1 (Step 000235): Train loss 6.286, Val loss 6.336, Tokens seen: 962560\n",
      "at the start of the first of the first of the same of the same of the same of the first of the same\n",
      "Ep 1 (Step 000240): Train loss 6.217, Val loss 6.319, Tokens seen: 983040\n",
      "at the start of the first time . The first time the first time of the first time of the first time to the\n",
      "Ep 1 (Step 000245): Train loss 6.228, Val loss 6.305, Tokens seen: 1003520\n",
      "at the start of the first @-@ year @-@ 000 @-@ 000 @-@ 000 @-\n",
      "Ep 1 (Step 000250): Train loss 6.277, Val loss 6.338, Tokens seen: 1024000\n",
      "at the start of the first time of the first time of the first time of the first time of the first time of\n",
      "Ep 1 (Step 000255): Train loss 6.210, Val loss 6.293, Tokens seen: 1044480\n",
      "at the start of the first time . \n",
      "   = = = = = = = = = \n",
      " \n",
      "Ep 1 (Step 000260): Train loss 6.153, Val loss 6.293, Tokens seen: 1064960\n",
      "at the start of the first time . The first time , and the first time of the first time of the first time\n",
      "Ep 1 (Step 000265): Train loss 6.186, Val loss 6.281, Tokens seen: 1085440\n",
      "at the start of the first of the first of the first of the first of the first of the first of the first\n",
      "Ep 1 (Step 000270): Train loss 6.113, Val loss 6.250, Tokens seen: 1105920\n",
      "at the start of the first of the first of the first of the first of the first of the first of the first\n",
      "Ep 1 (Step 000275): Train loss 6.094, Val loss 6.253, Tokens seen: 1126400\n",
      "at the start of the first time . \n",
      "    = = = \n",
      "      \n",
      "Ep 1 (Step 000280): Train loss 6.160, Val loss 6.243, Tokens seen: 1146880\n",
      "at the start of the United States . The first time of the first time of the United States , the United States ,\n",
      "Ep 1 (Step 000285): Train loss 6.105, Val loss 6.245, Tokens seen: 1167360\n",
      "at the start of the first time . \n",
      "    = = = = = = = = \n",
      " \n",
      "Ep 1 (Step 000290): Train loss 6.135, Val loss 6.238, Tokens seen: 1187840\n",
      "at the start of the first time . \n",
      "   = = = = = = = = = \n",
      " \n",
      "Ep 1 (Step 000295): Train loss 6.097, Val loss 6.219, Tokens seen: 1208320\n",
      "at the start of the first time of the first time of the first time of the first time of the first time .\n",
      "Ep 1 (Step 000300): Train loss 6.148, Val loss 6.178, Tokens seen: 1228800\n",
      "at the start of the first time of the first time of the first time of the first time of the first time .\n",
      "Ep 1 (Step 000305): Train loss 6.057, Val loss 6.192, Tokens seen: 1249280\n",
      "at the start of the first time , and the first time , and the game , and the game , and the first\n",
      "Ep 1 (Step 000310): Train loss 6.095, Val loss 6.197, Tokens seen: 1269760\n",
      "at the start of the United States . \n",
      "      = = = = = = \n",
      " \n",
      "Ep 1 (Step 000315): Train loss 6.085, Val loss 6.178, Tokens seen: 1290240\n",
      "at the start of the United States . The first time , the first time , the first time , the first time ,\n",
      "Ep 1 (Step 000320): Train loss 6.005, Val loss 6.186, Tokens seen: 1310720\n",
      "at the start of the United States . \n",
      "   = = = = = = \n",
      "    \n",
      "Ep 1 (Step 000325): Train loss 6.077, Val loss 6.177, Tokens seen: 1331200\n",
      "at the start of the United States of the United States of the United States of the United States . The first time of\n",
      "Ep 1 (Step 000330): Train loss 5.990, Val loss 6.152, Tokens seen: 1351680\n",
      "at the start of the same time of the same time . The first time , the first time , the same time ,\n",
      "Ep 1 (Step 000335): Train loss 6.061, Val loss 6.132, Tokens seen: 1372160\n",
      "at the start of the first time , and the time , and the time of the time of the time of the time\n",
      "Ep 1 (Step 000340): Train loss 6.093, Val loss 6.140, Tokens seen: 1392640\n",
      "at the start of the first time , and the first time , and the time , and the first time , and the\n",
      "Ep 1 (Step 000345): Train loss 5.989, Val loss 6.111, Tokens seen: 1413120\n",
      "at the start of the city of the city of the city of the city of the city of the city of the city\n",
      "Ep 1 (Step 000350): Train loss 5.885, Val loss 6.101, Tokens seen: 1433600\n",
      "at the start of the city . The city was a new time , and the city was not be a new . The\n",
      "Ep 1 (Step 000355): Train loss 6.013, Val loss 6.107, Tokens seen: 1454080\n",
      "at the start of the first time . \n",
      "   = = = = = = = = = \n",
      " \n",
      "Ep 1 (Step 000360): Train loss 5.817, Val loss 6.091, Tokens seen: 1474560\n",
      "at the start of the first time . The first time , the first time , the first time , the first time ,\n",
      "Ep 1 (Step 000365): Train loss 5.965, Val loss 6.092, Tokens seen: 1495040\n",
      "at the start of the first time , and the first time . \n",
      "   = = = = = = =\n",
      "Ep 1 (Step 000370): Train loss 5.955, Val loss 6.122, Tokens seen: 1515520\n",
      "at the start of the first time , and the first time , and the first time , and the first time . The\n",
      "Ep 1 (Step 000375): Train loss 5.989, Val loss 6.085, Tokens seen: 1536000\n",
      "at the start of the main star , and the game . \n",
      "   = = = = = = = =\n",
      "Ep 1 (Step 000380): Train loss 5.938, Val loss 6.105, Tokens seen: 1556480\n",
      "at the start of the most of the other of the most of the other of the other of the other of the most\n",
      "Ep 1 (Step 000385): Train loss 5.955, Val loss 6.070, Tokens seen: 1576960\n",
      "at the start of the United States . The first time , the first time , the first time , the first time of\n",
      "Ep 1 (Step 000390): Train loss 5.888, Val loss 6.054, Tokens seen: 1597440\n",
      "at the start of the British ships . The first time , and the first time , and the first time , and the\n",
      "Ep 1 (Step 000395): Train loss 5.824, Val loss 6.071, Tokens seen: 1617920\n",
      "at the start of the British government . The government of the other of the other of the other the British government of the\n",
      "Ep 1 (Step 000400): Train loss 5.913, Val loss 6.050, Tokens seen: 1638400\n",
      "at the start of the British ships . The ship was the first time of the first to the game . \n",
      " \n",
      "Ep 1 (Step 000405): Train loss 5.861, Val loss 6.044, Tokens seen: 1658880\n",
      "at the start of the United States . The first time of the first time , the first season , the first season ,\n",
      "Ep 1 (Step 000410): Train loss 5.776, Val loss 6.021, Tokens seen: 1679360\n",
      "at the start of the United States , the United States , the United States , the United States , the United States ,\n",
      "Ep 1 (Step 000415): Train loss 5.834, Val loss 6.025, Tokens seen: 1699840\n",
      "at the start of the city of the city of the city of the city . The following the city of the city of\n",
      "Ep 1 (Step 000420): Train loss 5.691, Val loss 5.993, Tokens seen: 1720320\n",
      "at the start of the first time , the first time of the first time , and the first time , and the first\n",
      "Ep 1 (Step 000425): Train loss 5.802, Val loss 6.031, Tokens seen: 1740800\n",
      "at the start of the first time . The first time , the first time , the first time , the first time ,\n",
      "Ep 1 (Step 000430): Train loss 5.772, Val loss 6.022, Tokens seen: 1761280\n",
      "at the start of the first time , the first time , the first time , the first time , the first time ,\n",
      "Ep 1 (Step 000435): Train loss 5.843, Val loss 6.051, Tokens seen: 1781760\n",
      "at the start of the British ships . \n",
      "   = = = = = \n",
      "    = =\n",
      "Ep 1 (Step 000440): Train loss 5.758, Val loss 5.997, Tokens seen: 1802240\n",
      "at the start of the city 's \" . \" \n",
      "   = = = = = = = = \n",
      "Ep 1 (Step 000445): Train loss 5.802, Val loss 5.995, Tokens seen: 1822720\n",
      "at the start of the city of the city of the city of the city . \n",
      "   = = = =\n",
      "Ep 1 (Step 000450): Train loss 5.778, Val loss 6.007, Tokens seen: 1843200\n",
      "at the start of the city . \n",
      "   = = = = = = = = = \n",
      "  \n",
      "Ep 1 (Step 000455): Train loss 5.825, Val loss 6.006, Tokens seen: 1863680\n",
      "at the start of the first time . \n",
      "   = = = = = \n",
      "    = =\n",
      "Ep 1 (Step 000460): Train loss 5.697, Val loss 5.968, Tokens seen: 1884160\n",
      "at the start of the first time . The first of the first of the first time , the first of the first of\n",
      "Ep 1 (Step 000465): Train loss 5.683, Val loss 5.978, Tokens seen: 1904640\n",
      "at the start of the first of the first of the first of the first of the United States . The first of the\n",
      "Ep 1 (Step 000470): Train loss 5.686, Val loss 5.956, Tokens seen: 1925120\n",
      "at the start of the British forces . The first of the first of the first of the first of the first in the\n",
      "Ep 1 (Step 000475): Train loss 5.718, Val loss 5.946, Tokens seen: 1945600\n",
      "at the start of the first time , the first time , and the first season , and the first season . The first\n",
      "Ep 1 (Step 000480): Train loss 5.814, Val loss 5.942, Tokens seen: 1966080\n",
      "at the start of the first in the first time , the first season , the first season , the first season , and\n",
      "Ep 1 (Step 000485): Train loss 5.674, Val loss 5.961, Tokens seen: 1986560\n",
      "at the start of the end of the first time , the first time of the first time , the first time , and\n",
      "Ep 1 (Step 000490): Train loss 5.706, Val loss 5.946, Tokens seen: 2007040\n",
      "at the start of the city . The road was the first time in the first time , and the first time of the\n",
      "Ep 1 (Step 000495): Train loss 5.629, Val loss 5.949, Tokens seen: 2027520\n",
      "at the start of the \" . \n",
      "   = = = = \n",
      "    = = = =\n",
      "Ep 1 (Step 000500): Train loss 5.588, Val loss 5.929, Tokens seen: 2048000\n",
      "at the start of the city . The first of the city of the city of the city 's first in the United\n",
      "Ep 1 (Step 000505): Train loss 5.590, Val loss 5.918, Tokens seen: 2068480\n",
      "at the start of the city of the city . The first two of the United States , the United States , the United\n",
      "Ep 1 (Step 000510): Train loss 5.605, Val loss 5.903, Tokens seen: 2088960\n",
      "at the start of the city , the city of the city of the city of the city , the city of the city\n",
      "Ep 1 (Step 000515): Train loss 5.643, Val loss 5.904, Tokens seen: 2109440\n",
      "at the start of the same time , the first time , and the first time of the first time . The first of\n",
      "Ep 1 (Step 000520): Train loss 5.590, Val loss 5.890, Tokens seen: 2129920\n",
      "at the start of the first game , the game 's first game 's first game . \n",
      "   =\n",
      "Ep 1 (Step 000525): Train loss 5.705, Val loss 5.872, Tokens seen: 2150400\n",
      "at the start of the first time . \n",
      "   = = = = = \n",
      "    = =\n",
      "Ep 1 (Step 000530): Train loss 5.550, Val loss 5.912, Tokens seen: 2170880\n",
      "at the start of the first time . \n",
      "   = = = = = = = = = = = =\n",
      "Ep 1 (Step 000535): Train loss 5.539, Val loss 5.878, Tokens seen: 2191360\n",
      "at the start of the same time , the same time , the same time , the game , the game was the game\n",
      "Ep 1 (Step 000540): Train loss 5.620, Val loss 5.850, Tokens seen: 2211840\n",
      "at the start of the end of the end of the end of the end of the end of the end of the end\n",
      "Ep 1 (Step 000545): Train loss 5.501, Val loss 5.861, Tokens seen: 2232320\n",
      "at the start of the end of the most of the most of the most of the most of the most of the most\n",
      "Ep 1 (Step 000550): Train loss 5.511, Val loss 5.839, Tokens seen: 2252800\n",
      "at the start of the city . \n",
      "   = = = = = = = = = \n",
      "  \n",
      "Ep 1 (Step 000555): Train loss 5.571, Val loss 5.849, Tokens seen: 2273280\n",
      "at the start of the city . \n",
      "   = = = = = = = \n",
      "   The first\n",
      "Ep 1 (Step 000560): Train loss 5.518, Val loss 5.829, Tokens seen: 2293760\n",
      "at the start of the city , the city 's main distance of the city . The first two @-@ century\n",
      "Ep 1 (Step 000565): Train loss 5.582, Val loss 5.819, Tokens seen: 2314240\n",
      "at the start of the city . The first time of the most of the most of the most of the most of the\n",
      "Ep 1 (Step 000570): Train loss 5.500, Val loss 5.804, Tokens seen: 2334720\n",
      "at the start of the same time , and the same time of the same time , and the same time , and the\n",
      "Ep 1 (Step 000575): Train loss 5.484, Val loss 5.831, Tokens seen: 2355200\n",
      "at the start of the first time of the first time , and the first time of the first time of the first time\n",
      "Ep 1 (Step 000580): Train loss 5.484, Val loss 5.823, Tokens seen: 2375680\n",
      "at the start of the war . The first time , the first two @-@ century , and the first season .\n",
      "Ep 1 (Step 000585): Train loss 5.572, Val loss 5.828, Tokens seen: 2396160\n",
      "at the start of the end of the war . \n",
      "   = = = = = = = = = =\n",
      "Ep 1 (Step 000590): Train loss 5.501, Val loss 5.826, Tokens seen: 2416640\n",
      "at the start of the same year . The following year , the first time , the United States , the United States ,\n",
      "Ep 2 (Step 000595): Train loss 5.451, Val loss 5.825, Tokens seen: 2437120\n",
      "at the start of the same year . The first two @-@ yard line , the first time , the first two\n",
      "Ep 2 (Step 000600): Train loss 5.456, Val loss 5.825, Tokens seen: 2457600\n",
      "at the start of the United States . The first season , the first season , the season , the first season , the\n",
      "Ep 2 (Step 000605): Train loss 5.402, Val loss 5.829, Tokens seen: 2478080\n",
      "at the start of the United States . The first round of the first time , the first time of the first round of\n",
      "Ep 2 (Step 000610): Train loss 5.461, Val loss 5.801, Tokens seen: 2498560\n",
      "at the start of the first time , the first time of the first time , the first time , the first time ,\n",
      "Ep 2 (Step 000615): Train loss 5.484, Val loss 5.808, Tokens seen: 2519040\n",
      "at the start of the \" . The episode was the song 's \" . The song was a song 's song\n",
      "Ep 2 (Step 000620): Train loss 5.378, Val loss 5.796, Tokens seen: 2539520\n",
      "at the start of the first time . \n",
      "   = = = = = = \n",
      "    The\n",
      "Ep 2 (Step 000625): Train loss 5.290, Val loss 5.791, Tokens seen: 2560000\n",
      "at the start of the first time , the first time , and the first time , and the first time . \n",
      "\n",
      "Ep 2 (Step 000630): Train loss 5.456, Val loss 5.797, Tokens seen: 2580480\n",
      "at the start of the United States . \n",
      "   = = = = = \n",
      "    The first\n",
      "Ep 2 (Step 000635): Train loss 5.402, Val loss 5.780, Tokens seen: 2600960\n",
      "at the start of the first time , the first time , the first time , the first time , the first round ,\n",
      "Ep 2 (Step 000640): Train loss 5.332, Val loss 5.768, Tokens seen: 2621440\n",
      "at the start of the United States , and the United States , and the United States . \n",
      "   = =\n",
      "Ep 2 (Step 000645): Train loss 5.407, Val loss 5.766, Tokens seen: 2641920\n",
      "at the start of the city of the city of the city of the city of the city of the city of the city\n",
      "Ep 2 (Step 000650): Train loss 5.328, Val loss 5.756, Tokens seen: 2662400\n",
      "at the start of the city 's first time , the first time , the first down of the city 's first\n",
      "Ep 2 (Step 000655): Train loss 5.284, Val loss 5.751, Tokens seen: 2682880\n",
      "at the start of the city , the city of the Missouri River , and the Missouri River , and the Missouri River .\n",
      "Ep 2 (Step 000660): Train loss 5.365, Val loss 5.760, Tokens seen: 2703360\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = \n",
      "  \n",
      "Ep 2 (Step 000665): Train loss 5.263, Val loss 5.733, Tokens seen: 2723840\n",
      "at the start of the same time , the first time of the first time , and the first time of the first time\n",
      "Ep 2 (Step 000670): Train loss 5.281, Val loss 5.756, Tokens seen: 2744320\n",
      "at the start of the same time , the first time of the first time , the first time of the first time of\n",
      "Ep 2 (Step 000675): Train loss 5.275, Val loss 5.731, Tokens seen: 2764800\n",
      "at the start of the \" . The song was a \" by the song was a \" by the song 's \"\n",
      "Ep 2 (Step 000680): Train loss 5.331, Val loss 5.748, Tokens seen: 2785280\n",
      "at the start of the \" , \" , \" The \" was \" by the song 's \" , and \" ,\n",
      "Ep 2 (Step 000685): Train loss 5.373, Val loss 5.743, Tokens seen: 2805760\n",
      "at the start of the \" . The first time was the first time in the first time , and was the first down\n",
      "Ep 2 (Step 000690): Train loss 5.315, Val loss 5.739, Tokens seen: 2826240\n",
      "at the start of the \" , the \" The \" The \" was \" by the \" , and \" the \" ,\n",
      "Ep 2 (Step 000695): Train loss 5.327, Val loss 5.715, Tokens seen: 2846720\n",
      "at the start of the United States , and the United States , the United States , and the United States , and the\n",
      "Ep 2 (Step 000700): Train loss 5.368, Val loss 5.721, Tokens seen: 2867200\n",
      "at the start of the \" . The episode was the first time of the first time . \n",
      "   = =\n",
      "Ep 2 (Step 000705): Train loss 5.305, Val loss 5.728, Tokens seen: 2887680\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = = = = \n",
      "Ep 2 (Step 000710): Train loss 5.433, Val loss 5.714, Tokens seen: 2908160\n",
      "at the start of the \" . \n",
      "   = = = = = = = \n",
      "    The\n",
      "Ep 2 (Step 000715): Train loss 5.304, Val loss 5.705, Tokens seen: 2928640\n",
      "at the start of the same time , the first time to the first time , the first time to the first time of\n",
      "Ep 2 (Step 000720): Train loss 5.181, Val loss 5.691, Tokens seen: 2949120\n",
      "at the start of the same time , and the same time of the same time . \n",
      "   = = =\n",
      "Ep 2 (Step 000725): Train loss 5.218, Val loss 5.714, Tokens seen: 2969600\n",
      "at the start of the same time . The first time of the same time , the same time , the same time of\n",
      "Ep 2 (Step 000730): Train loss 5.377, Val loss 5.708, Tokens seen: 2990080\n",
      "at the start of the Song and the Song and the Song . The first of the Song and the Song dynasty of the\n",
      "Ep 2 (Step 000735): Train loss 5.218, Val loss 5.709, Tokens seen: 3010560\n",
      "at the start of the \" . The song was a \" a \" a \" a \" , \" , \" , \"\n",
      "Ep 2 (Step 000740): Train loss 5.224, Val loss 5.709, Tokens seen: 3031040\n",
      "at the start of the same year , and the first time of the first time of the first time . The first of\n",
      "Ep 2 (Step 000745): Train loss 5.234, Val loss 5.691, Tokens seen: 3051520\n",
      "at the start of the \" . The song was a \" , and \" was \" . The song was a \" ,\n",
      "Ep 2 (Step 000750): Train loss 5.218, Val loss 5.679, Tokens seen: 3072000\n",
      "at the start of the \" . \n",
      "   = = = = \n",
      "   In the early 1990s\n",
      "Ep 2 (Step 000755): Train loss 5.225, Val loss 5.687, Tokens seen: 3092480\n",
      "at the start of the series , the series was the first season . \n",
      "   = = = = = =\n",
      "Ep 2 (Step 000760): Train loss 5.204, Val loss 5.691, Tokens seen: 3112960\n",
      "at the start of the United States , the United States , the United States , and the United States , the United States\n",
      "Ep 2 (Step 000765): Train loss 5.275, Val loss 5.700, Tokens seen: 3133440\n",
      "at the start of the \" . \n",
      "   = = = = \n",
      "   The first time of the\n",
      "Ep 2 (Step 000770): Train loss 5.256, Val loss 5.701, Tokens seen: 3153920\n",
      "at the start of the song was a song . \n",
      "   = = = = = = \n",
      "  \n",
      "Ep 2 (Step 000775): Train loss 5.141, Val loss 5.662, Tokens seen: 3174400\n",
      "at the start of the first time . The first of the first time , the first down of the first time . The\n",
      "Ep 2 (Step 000780): Train loss 5.192, Val loss 5.689, Tokens seen: 3194880\n",
      "at the start of the first time , the first time of the first time , and was the first time . \n",
      "\n",
      "Ep 2 (Step 000785): Train loss 5.111, Val loss 5.655, Tokens seen: 3215360\n",
      "at the start of the city , the city was the first time of the first time . \n",
      "  The first @\n",
      "Ep 2 (Step 000790): Train loss 5.100, Val loss 5.664, Tokens seen: 3235840\n",
      "at the start of the city 's death . \n",
      "   = = = = = = = = = \n",
      "Ep 2 (Step 000795): Train loss 5.209, Val loss 5.644, Tokens seen: 3256320\n",
      "at the start of the United States , and the United States , and the United States , and the United States , and\n",
      "Ep 2 (Step 000800): Train loss 5.188, Val loss 5.677, Tokens seen: 3276800\n",
      "at the start of the United States , and the first phase of the United States . The first phase of the first phase\n",
      "Ep 2 (Step 000805): Train loss 5.155, Val loss 5.667, Tokens seen: 3297280\n",
      "at the start of the first season . \n",
      "   = = = = = = = = = \n",
      " \n",
      "Ep 2 (Step 000810): Train loss 5.119, Val loss 5.676, Tokens seen: 3317760\n",
      "at the start of the \" . \" \n",
      "   = = = = = = = = = = = =\n",
      "Ep 2 (Step 000815): Train loss 5.241, Val loss 5.654, Tokens seen: 3338240\n",
      "at the start of the city . \n",
      "   = = = = \n",
      "   The first of the first\n",
      "Ep 2 (Step 000820): Train loss 5.182, Val loss 5.641, Tokens seen: 3358720\n",
      "at the start of the city 's first time , and the first season , the season , and the season , and\n",
      "Ep 2 (Step 000825): Train loss 5.023, Val loss 5.638, Tokens seen: 3379200\n",
      "at the start of the city 's first time , the first ever @-@ yard line , and a total of\n",
      "Ep 2 (Step 000830): Train loss 5.167, Val loss 5.637, Tokens seen: 3399680\n",
      "at the start of the city . \n",
      "   = = = = = = = \n",
      "   The Assembly\n",
      "Ep 2 (Step 000835): Train loss 5.159, Val loss 5.621, Tokens seen: 3420160\n",
      "at the start of the same time , the first time , the first time , the first season , and the first season\n",
      "Ep 2 (Step 000840): Train loss 5.125, Val loss 5.617, Tokens seen: 3440640\n",
      "at the start of the original name , the original name of the Church of the Church of the Church of the Church of\n",
      "Ep 2 (Step 000845): Train loss 4.953, Val loss 5.631, Tokens seen: 3461120\n",
      "at the start of the same day . The two @-@ year , the first @-@ yard line of the\n",
      "Ep 2 (Step 000850): Train loss 5.141, Val loss 5.625, Tokens seen: 3481600\n",
      "at the start of the Church of the Church of the Church of the Church of the Church of the Church of the Church\n",
      "Ep 2 (Step 000855): Train loss 5.080, Val loss 5.602, Tokens seen: 3502080\n",
      "at the start of the United States . The first season , the first season , the first season , and was the first\n",
      "Ep 2 (Step 000860): Train loss 5.092, Val loss 5.590, Tokens seen: 3522560\n",
      "at the start of the \" . The song was released on the song , and was released on the song 's song\n",
      "Ep 2 (Step 000865): Train loss 5.018, Val loss 5.583, Tokens seen: 3543040\n",
      "at the start of the \" . The song was released in the United States . The song was released in the United States\n",
      "Ep 2 (Step 000870): Train loss 5.075, Val loss 5.603, Tokens seen: 3563520\n",
      "at the start of the same day , the same day , and the two of the same year . \n",
      "  \n",
      "Ep 2 (Step 000875): Train loss 5.071, Val loss 5.584, Tokens seen: 3584000\n",
      "at the start of the \" . \" \n",
      "   \" The song was released on the album 's \" The\n",
      "Ep 2 (Step 000880): Train loss 5.102, Val loss 5.593, Tokens seen: 3604480\n",
      "at the start of the series 's \" . \n",
      "   = = = = = = = = = =\n",
      "Ep 2 (Step 000885): Train loss 5.056, Val loss 5.580, Tokens seen: 3624960\n",
      "at the start of the \" . \n",
      "   = = = = = = \n",
      "   The earliest \"\n",
      "Ep 2 (Step 000890): Train loss 5.060, Val loss 5.559, Tokens seen: 3645440\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 2 (Step 000895): Train loss 5.020, Val loss 5.544, Tokens seen: 3665920\n",
      "at the start of the \" . The song was released on the song 's song , and the song was released on\n",
      "Ep 2 (Step 000900): Train loss 5.028, Val loss 5.575, Tokens seen: 3686400\n",
      "at the start of the \" . The first two @-@ year @-@ yard line , and a \" The\n",
      "Ep 2 (Step 000905): Train loss 4.943, Val loss 5.564, Tokens seen: 3706880\n",
      "at the start of the \" , \" The Times of the \" , \" The Times \" , \" The Times of the\n",
      "Ep 2 (Step 000910): Train loss 5.092, Val loss 5.573, Tokens seen: 3727360\n",
      "at the start of the city . The first two @-@ year @-@ yard line , and the first @\n",
      "Ep 2 (Step 000915): Train loss 4.998, Val loss 5.542, Tokens seen: 3747840\n",
      "at the start of the city 's body . \n",
      "   = = = = = = = = = \n",
      "Ep 2 (Step 000920): Train loss 4.962, Val loss 5.575, Tokens seen: 3768320\n",
      "at the start of the same year . The first two @-@ year @-@ year @-@ year @\n",
      "Ep 2 (Step 000925): Train loss 4.973, Val loss 5.537, Tokens seen: 3788800\n",
      "at the start of the city of the city of the city of the city of the city of the city of the city\n",
      "Ep 2 (Step 000930): Train loss 5.157, Val loss 5.558, Tokens seen: 3809280\n",
      "at the start of the series of the series 's \" The Times \" , and the song \" The song \" ,\n",
      "Ep 2 (Step 000935): Train loss 5.017, Val loss 5.551, Tokens seen: 3829760\n",
      "at the start of the \" , \" the \" , \" the \" , \" , \" the \" , \" The \"\n",
      "Ep 2 (Step 000940): Train loss 4.950, Val loss 5.546, Tokens seen: 3850240\n",
      "at the start of the \" . The song was released on the song \" , and \" was released on the song \"\n",
      "Ep 2 (Step 000945): Train loss 5.035, Val loss 5.533, Tokens seen: 3870720\n",
      "at the start of the \" . The song was released on the album , and the album , and the album , and\n",
      "Ep 2 (Step 000950): Train loss 4.857, Val loss 5.533, Tokens seen: 3891200\n",
      "at the start of the \" . The song was released on the album , and the album 's first single from the\n",
      "Ep 2 (Step 000955): Train loss 4.983, Val loss 5.533, Tokens seen: 3911680\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 2 (Step 000960): Train loss 4.937, Val loss 5.527, Tokens seen: 3932160\n",
      "at the start of the \" . The song was released on the album , and the album was released on the album '\n",
      "Ep 2 (Step 000965): Train loss 4.901, Val loss 5.520, Tokens seen: 3952640\n",
      "at the start of the \" . \n",
      "   = = = = = \n",
      "   The first aired on\n",
      "Ep 2 (Step 000970): Train loss 4.921, Val loss 5.514, Tokens seen: 3973120\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = \n",
      "  \n",
      "Ep 2 (Step 000975): Train loss 5.070, Val loss 5.522, Tokens seen: 3993600\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = \n",
      "  \n",
      "Ep 2 (Step 000980): Train loss 4.945, Val loss 5.504, Tokens seen: 4014080\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 2 (Step 000985): Train loss 4.926, Val loss 5.509, Tokens seen: 4034560\n",
      "at the start of the \" , \" \n",
      "   = = = = = = = = = = = =\n",
      "Ep 2 (Step 000990): Train loss 4.871, Val loss 5.493, Tokens seen: 4055040\n",
      "at the start of the \" . The song was \" The song \" , \" The song \" , \" The song \"\n",
      "Ep 2 (Step 000995): Train loss 4.871, Val loss 5.494, Tokens seen: 4075520\n",
      "at the start of the two @-@ year @-@ year @-@ off @-@ off @-\n",
      "Ep 2 (Step 001000): Train loss 4.942, Val loss 5.488, Tokens seen: 4096000\n",
      "at the start of the city . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 2 (Step 001005): Train loss 4.925, Val loss 5.481, Tokens seen: 4116480\n",
      "at the start of the same name , and the first time of the first time of the first time of the first time\n",
      "Ep 2 (Step 001010): Train loss 4.775, Val loss 5.476, Tokens seen: 4136960\n",
      "at the start of the same time , the first @-@ year @-@ year @-@ off @-\n",
      "Ep 2 (Step 001015): Train loss 4.812, Val loss 5.475, Tokens seen: 4157440\n",
      "at the start of the same name \" . \" \n",
      "   = = = = = \n",
      "   =\n",
      "Ep 2 (Step 001020): Train loss 4.904, Val loss 5.479, Tokens seen: 4177920\n",
      "at the start of the \" . The song was released on the album , and was released on the album . The album\n",
      "Ep 2 (Step 001025): Train loss 4.964, Val loss 5.467, Tokens seen: 4198400\n",
      "at the start of the \" . \" \n",
      "   = = = = \n",
      "   The first broadcast of\n",
      "Ep 2 (Step 001030): Train loss 4.940, Val loss 5.492, Tokens seen: 4218880\n",
      "at the start of the \" . \" \n",
      "   = = = = = = = = \n",
      "  \n",
      "Ep 2 (Step 001035): Train loss 4.862, Val loss 5.456, Tokens seen: 4239360\n",
      "at the start of the United States . \n",
      "   = = = = = = = = = = = =\n",
      "Ep 2 (Step 001040): Train loss 4.779, Val loss 5.449, Tokens seen: 4259840\n",
      "at the start of the city . The city was the first @-@ in the United States . The first four @\n",
      "Ep 2 (Step 001045): Train loss 4.862, Val loss 5.470, Tokens seen: 4280320\n",
      "at the start of the city . \n",
      "   = = = = = = \n",
      "   The first known\n",
      "Ep 2 (Step 001050): Train loss 4.835, Val loss 5.480, Tokens seen: 4300800\n",
      "at the start of the same name \" . \" \n",
      "   = = = = = = = = = =\n",
      "Ep 2 (Step 001055): Train loss 4.917, Val loss 5.460, Tokens seen: 4321280\n",
      "at the start of the two years of the two years of the two years . The two of the two of the two\n",
      "Ep 2 (Step 001060): Train loss 4.928, Val loss 5.438, Tokens seen: 4341760\n",
      "at the start of the main sequence of the main sequence of the main sequence . \n",
      "   = = = =\n",
      "Ep 2 (Step 001065): Train loss 4.812, Val loss 5.443, Tokens seen: 4362240\n",
      "at the start of the \" . The song was released on the Billboard Hot 100 chart . It was released on the Billboard\n",
      "Ep 2 (Step 001070): Train loss 4.731, Val loss 5.427, Tokens seen: 4382720\n",
      "at the start of the \" . The song was released on the Billboard Hot 100 chart . The album was released on the\n",
      "Ep 2 (Step 001075): Train loss 4.760, Val loss 5.431, Tokens seen: 4403200\n",
      "at the start of the main sequence of the kakapo was the first to be the first to be used to be\n",
      "Ep 2 (Step 001080): Train loss 4.726, Val loss 5.427, Tokens seen: 4423680\n",
      "at the start of the \" . The song was \" by the song 's \" Loverboy \" , and \" Lover\n",
      "Ep 2 (Step 001085): Train loss 4.899, Val loss 5.416, Tokens seen: 4444160\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = \n",
      "  \n",
      "Ep 2 (Step 001090): Train loss 4.800, Val loss 5.439, Tokens seen: 4464640\n",
      "at the start of the \" . \n",
      "   = = = = \n",
      "   The poem was a song\n",
      "Ep 2 (Step 001095): Train loss 4.667, Val loss 5.417, Tokens seen: 4485120\n",
      "at the start of the \" . \n",
      "   = = = = = \n",
      "   The poem was written\n",
      "Ep 2 (Step 001100): Train loss 4.874, Val loss 5.420, Tokens seen: 4505600\n",
      "at the start of the \" . The song was released on the Billboard 200 , and the album was released on the Billboard\n",
      "Ep 2 (Step 001105): Train loss 4.602, Val loss 5.416, Tokens seen: 4526080\n",
      "at the start of the \" . The song was released on the Billboard 200 , and the album was released on the Billboard\n",
      "Ep 2 (Step 001110): Train loss 4.863, Val loss 5.409, Tokens seen: 4546560\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = = \n",
      " \n",
      "Ep 2 (Step 001115): Train loss 4.745, Val loss 5.423, Tokens seen: 4567040\n",
      "at the start of the city , the city of the city . \n",
      "  The city of the city 's population\n",
      "Ep 2 (Step 001120): Train loss 4.738, Val loss 5.418, Tokens seen: 4587520\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = \n",
      "  \n",
      "Ep 2 (Step 001125): Train loss 4.680, Val loss 5.404, Tokens seen: 4608000\n",
      "at the start of the city 's population . The city 's population was the only to be the only of the\n",
      "Ep 2 (Step 001130): Train loss 4.722, Val loss 5.409, Tokens seen: 4628480\n",
      "at the start of the \" . \n",
      "   = = = = = = \n",
      "   The episode of\n",
      "Ep 2 (Step 001135): Train loss 4.685, Val loss 5.418, Tokens seen: 4648960\n",
      "at the start of the \" , \" the \" , \" The Good \" , \" , \" , \" , \" ,\n",
      "Ep 2 (Step 001140): Train loss 4.712, Val loss 5.385, Tokens seen: 4669440\n",
      "at the start of the city 's economy , and the city 's economy , and the city 's economy .\n",
      "Ep 2 (Step 001145): Train loss 4.588, Val loss 5.396, Tokens seen: 4689920\n",
      "at the start of the city . The city was the first time of the city of the city . The city was built\n",
      "Ep 2 (Step 001150): Train loss 4.714, Val loss 5.393, Tokens seen: 4710400\n",
      "at the start of the city of the city . The city was the first @-@ century , which was the first\n",
      "Ep 2 (Step 001155): Train loss 4.735, Val loss 5.373, Tokens seen: 4730880\n",
      "at the start of the first round of the first round of the first round of the first round of the first round of\n",
      "Ep 2 (Step 001160): Train loss 4.783, Val loss 5.419, Tokens seen: 4751360\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = = \n",
      " \n",
      "Ep 2 (Step 001165): Train loss 4.717, Val loss 5.391, Tokens seen: 4771840\n",
      "at the start of the film was released on the film , and was released on the film 's release . The film\n",
      "Ep 2 (Step 001170): Train loss 4.645, Val loss 5.392, Tokens seen: 4792320\n",
      "at the start of the \" . The song was written by the song \" Man Down \" , \" , \" , \"\n",
      "Ep 2 (Step 001175): Train loss 4.565, Val loss 5.360, Tokens seen: 4812800\n",
      "at the start of the first time , the first time , the first time , the first time in the first time ,\n",
      "Ep 2 (Step 001180): Train loss 4.679, Val loss 5.374, Tokens seen: 4833280\n",
      "at the start of the two years . \n",
      "  The first two @-@ year @-@ up the first\n",
      "Ep 3 (Step 001185): Train loss 4.754, Val loss 5.363, Tokens seen: 4853760\n",
      "at the start of the two years . The two of the two @-@ year @-@ year @-@\n",
      "Ep 3 (Step 001190): Train loss 4.807, Val loss 5.369, Tokens seen: 4874240\n",
      "at the start of the city , the city 's population of the city , and the city 's population , the\n",
      "Ep 3 (Step 001195): Train loss 4.598, Val loss 5.377, Tokens seen: 4894720\n",
      "at the start of the city . The city was the largest route to the city . The road was the largest route to\n",
      "Ep 3 (Step 001200): Train loss 4.550, Val loss 5.355, Tokens seen: 4915200\n",
      "at the start of the city . The storm was reported in the Gulf of the Gulf of the Gulf of the Gulf of\n",
      "Ep 3 (Step 001205): Train loss 4.732, Val loss 5.366, Tokens seen: 4935680\n",
      "at the start of the film , and the film was released on the film . \n",
      "   = = = =\n",
      "Ep 3 (Step 001210): Train loss 4.627, Val loss 5.355, Tokens seen: 4956160\n",
      "at the start of the country . The first two @-@ year @-@ Jamal was the first @-@\n",
      "Ep 3 (Step 001215): Train loss 4.696, Val loss 5.355, Tokens seen: 4976640\n",
      "at the start of the \" . The song was released on the song on the song on the song , and the song\n",
      "Ep 3 (Step 001220): Train loss 4.665, Val loss 5.360, Tokens seen: 4997120\n",
      "at the start of the film , and the film was released on the film , and the film was released on the film\n",
      "Ep 3 (Step 001225): Train loss 4.495, Val loss 5.344, Tokens seen: 5017600\n",
      "at the start of the song . \n",
      "   = = = = = = \n",
      "   The song was\n",
      "Ep 3 (Step 001230): Train loss 4.670, Val loss 5.354, Tokens seen: 5038080\n",
      "at the start of the song . \n",
      "   = = = = = = \n",
      "    = =\n",
      "Ep 3 (Step 001235): Train loss 4.519, Val loss 5.326, Tokens seen: 5058560\n",
      "at the start of the song . \n",
      "   = = = = Release = = \n",
      "    The\n",
      "Ep 3 (Step 001240): Train loss 4.650, Val loss 5.342, Tokens seen: 5079040\n",
      "at the start of the song . \n",
      "   = = = = = = = = = = \n",
      " \n",
      "Ep 3 (Step 001245): Train loss 4.513, Val loss 5.343, Tokens seen: 5099520\n",
      "at the start of the \" . The song was released in the United States , and was released in the United States .\n",
      "Ep 3 (Step 001250): Train loss 4.542, Val loss 5.330, Tokens seen: 5120000\n",
      "at the start of the city . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 3 (Step 001255): Train loss 4.471, Val loss 5.319, Tokens seen: 5140480\n",
      "at the start of the two @-@ year @-@ year @-@ off @-@ off @-\n",
      "Ep 3 (Step 001260): Train loss 4.592, Val loss 5.343, Tokens seen: 5160960\n",
      "at the start of the \" . The song was released on the song on the song . The song was released on the\n",
      "Ep 3 (Step 001265): Train loss 4.398, Val loss 5.367, Tokens seen: 5181440\n",
      "at the start of the \" . The song was released on the Billboard Hot 100 on the chart . The song was released\n",
      "Ep 3 (Step 001270): Train loss 4.432, Val loss 5.349, Tokens seen: 5201920\n",
      "at the start of the \" . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 3 (Step 001275): Train loss 4.639, Val loss 5.345, Tokens seen: 5222400\n",
      "at the start of the two @-@ year @-@ year @-@ year @-@ off the top\n",
      "Ep 3 (Step 001280): Train loss 4.586, Val loss 5.336, Tokens seen: 5242880\n",
      "at the start of the two years . \n",
      "   = = = = = = \n",
      "   The first\n",
      "Ep 3 (Step 001285): Train loss 4.374, Val loss 5.330, Tokens seen: 5263360\n",
      "at the start of the storm . The storm was also used to be a tropical storm . The storm was estimated at the\n",
      "Ep 3 (Step 001290): Train loss 4.476, Val loss 5.344, Tokens seen: 5283840\n",
      "at the start of the storm . The storm was the storm intensity with winds of the storm . The storm was the storm\n",
      "Ep 3 (Step 001295): Train loss 4.401, Val loss 5.325, Tokens seen: 5304320\n",
      "at the start of the song \" . \n",
      "   = = = Music video = = \n",
      "   \n",
      "Ep 3 (Step 001300): Train loss 4.601, Val loss 5.325, Tokens seen: 5324800\n",
      "at the start of the city 's body , and the city 's population was the largest city of the city '\n",
      "Ep 3 (Step 001305): Train loss 4.507, Val loss 5.321, Tokens seen: 5345280\n",
      "at the start of the song . \n",
      "   = = = = = = \n",
      "   The song was\n",
      "Ep 3 (Step 001310): Train loss 4.632, Val loss 5.329, Tokens seen: 5365760\n",
      "at the start of the ground . The first time , the first two @-@ year @-@ year @-\n",
      "Ep 3 (Step 001315): Train loss 4.346, Val loss 5.326, Tokens seen: 5386240\n",
      "at the start of the two @-@ year @-@ year @-@ old @-@ year @-\n",
      "Ep 3 (Step 001320): Train loss 4.485, Val loss 5.301, Tokens seen: 5406720\n",
      "at the start of the two @-@ year @-@ year @-@ old @-@ year @-\n",
      "Ep 3 (Step 001325): Train loss 4.428, Val loss 5.313, Tokens seen: 5427200\n",
      "at the start of the song . \n",
      "   = = = Release and personnel = = \n",
      "   \n",
      "Ep 3 (Step 001330): Train loss 4.500, Val loss 5.318, Tokens seen: 5447680\n",
      "at the start of the storm . The storm was not to be a tropical storm , but it was not enough to be\n",
      "Ep 3 (Step 001335): Train loss 4.525, Val loss 5.323, Tokens seen: 5468160\n",
      "at the start of the song . \n",
      "   = = = Release and music = = \n",
      "   \"\n",
      "Ep 3 (Step 001340): Train loss 4.458, Val loss 5.310, Tokens seen: 5488640\n",
      "at the start of the two @-@ year @-@ year @-@ old @-@ year @-\n",
      "Ep 3 (Step 001345): Train loss 4.352, Val loss 5.303, Tokens seen: 5509120\n",
      "at the start of the first time , the first time , the first time , was the first time , and the first\n",
      "Ep 3 (Step 001350): Train loss 4.390, Val loss 5.322, Tokens seen: 5529600\n",
      "at the start of the first time , the first time , the first time , was made of the first time , but\n",
      "Ep 3 (Step 001355): Train loss 4.413, Val loss 5.299, Tokens seen: 5550080\n",
      "at the start of the first time , the first time , the first time of the first time , the first time ,\n",
      "Ep 3 (Step 001360): Train loss 4.383, Val loss 5.311, Tokens seen: 5570560\n",
      "at the start of the war . The first time was the first to be the first to be the first to be used\n",
      "Ep 3 (Step 001365): Train loss 4.382, Val loss 5.303, Tokens seen: 5591040\n",
      "at the start of the first time , the first to the first time , the first to the first time , and the\n",
      "Ep 3 (Step 001370): Train loss 4.434, Val loss 5.293, Tokens seen: 5611520\n",
      "at the start of the two @-@ year @-@ old @-@ old @-@ old @-\n",
      "Ep 3 (Step 001375): Train loss 4.394, Val loss 5.282, Tokens seen: 5632000\n",
      "at the start of the city . The city was built in the city of the city of the city of the city of\n",
      "Ep 3 (Step 001380): Train loss 4.482, Val loss 5.303, Tokens seen: 5652480\n",
      "at the start of the two years of the three @-@ year @-@ year @-@ old . The\n",
      "Ep 3 (Step 001385): Train loss 4.360, Val loss 5.287, Tokens seen: 5672960\n",
      "at the start of the two @-@ year @-@ year @-@ old @-@ old @-\n",
      "Ep 3 (Step 001390): Train loss 4.311, Val loss 5.285, Tokens seen: 5693440\n",
      "at the start of the two years , and the first four @-@ year @-@ year @-@ old\n",
      "Ep 3 (Step 001395): Train loss 4.443, Val loss 5.298, Tokens seen: 5713920\n",
      "at the start of the first time , the first two @-@ year @-@ year @-@ old @\n",
      "Ep 3 (Step 001400): Train loss 4.311, Val loss 5.280, Tokens seen: 5734400\n",
      "at the start of the storm . The storm was also reported to be a tropical storm , and the storm was upgraded to\n",
      "Ep 3 (Step 001405): Train loss 4.445, Val loss 5.296, Tokens seen: 5754880\n",
      "at the start of the first time , the first two years , the first major leagues was the first to the first major\n",
      "Ep 3 (Step 001410): Train loss 4.469, Val loss 5.287, Tokens seen: 5775360\n",
      "at the start of the war . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 3 (Step 001415): Train loss 4.470, Val loss 5.291, Tokens seen: 5795840\n",
      "at the start of the war . The first two years of the first two years , the first two years later , the\n",
      "Ep 3 (Step 001420): Train loss 4.506, Val loss 5.278, Tokens seen: 5816320\n",
      "at the start of the war . \n",
      "   = = = = \n",
      "   The first phase of the\n",
      "Ep 3 (Step 001425): Train loss 4.503, Val loss 5.262, Tokens seen: 5836800\n",
      "at the start of the war . \n",
      "   = = = = = = \n",
      "   The first phase\n",
      "Ep 3 (Step 001430): Train loss 4.412, Val loss 5.267, Tokens seen: 5857280\n",
      "at the start of the war . The first two @-@ year @-@ year @-@ old @-\n",
      "Ep 3 (Step 001435): Train loss 4.381, Val loss 5.246, Tokens seen: 5877760\n",
      "at the start of the original name of the first century . \n",
      "   = = = = Early life = =\n",
      "Ep 3 (Step 001440): Train loss 4.446, Val loss 5.249, Tokens seen: 5898240\n",
      "at the start of the war . \n",
      "   = = = = = = = \n",
      "   The first\n",
      "Ep 3 (Step 001445): Train loss 4.367, Val loss 5.257, Tokens seen: 5918720\n",
      "at the start of the war . \n",
      "   = = = = = = = \n",
      "   The first\n",
      "Ep 3 (Step 001450): Train loss 4.330, Val loss 5.247, Tokens seen: 5939200\n",
      "at the start of the season , the season , the season , the season , and the season , the season , the\n",
      "Ep 3 (Step 001455): Train loss 4.360, Val loss 5.250, Tokens seen: 5959680\n",
      "at the start of the two @-@ year @-@ old @-@ old @-@ old @-\n",
      "Ep 3 (Step 001460): Train loss 4.332, Val loss 5.232, Tokens seen: 5980160\n",
      "at the start of the two @-@ year @-@ old @-@ old @-@ old @-\n",
      "Ep 3 (Step 001465): Train loss 4.238, Val loss 5.222, Tokens seen: 6000640\n",
      "at the start of the song , the song was the first single , and the song was the first single , and the\n",
      "Ep 3 (Step 001470): Train loss 4.272, Val loss 5.251, Tokens seen: 6021120\n",
      "at the start of the \" , \" The New York Times wrote that the song was \" a \" , and \" I\n",
      "Ep 3 (Step 001475): Train loss 4.247, Val loss 5.226, Tokens seen: 6041600\n",
      "at the start of the album 's lyrics , and the song 's lyrics of the song 's lyrics . \n",
      "Ep 3 (Step 001480): Train loss 4.278, Val loss 5.223, Tokens seen: 6062080\n",
      "at the start of the album 's music video for the song \" Freakum Dress \" , \" , \" , \"\n",
      "Ep 3 (Step 001485): Train loss 4.314, Val loss 5.238, Tokens seen: 6082560\n",
      "at the start of the album , \" The New York Times wrote that \" Freakum Dress \" was \" . The album\n",
      "Ep 3 (Step 001490): Train loss 4.327, Val loss 5.236, Tokens seen: 6103040\n",
      "at the start of the album 's music , \" The song was \" a \" , \" The New York Times \"\n",
      "Ep 3 (Step 001495): Train loss 4.423, Val loss 5.234, Tokens seen: 6123520\n",
      "at the start of the album , and the album 's second single , was released on the album , and the album\n",
      "Ep 3 (Step 001500): Train loss 4.217, Val loss 5.224, Tokens seen: 6144000\n",
      "at the start of the album , and the album 's first album , and the album was released on the album ,\n",
      "Ep 3 (Step 001505): Train loss 4.300, Val loss 5.213, Tokens seen: 6164480\n",
      "at the start of the first round of the first round of the first round of the race . The first round of the\n",
      "Ep 3 (Step 001510): Train loss 4.353, Val loss 5.237, Tokens seen: 6184960\n",
      "at the start of the first round of the first round of the first round of the season . \n",
      "   =\n",
      "Ep 3 (Step 001515): Train loss 4.278, Val loss 5.255, Tokens seen: 6205440\n",
      "at the start of the song \" . \" \n",
      "   = = = = Chart performance = = = \n",
      "\n",
      "Ep 3 (Step 001520): Train loss 4.343, Val loss 5.231, Tokens seen: 6225920\n",
      "at the start of the album , \" The album 's \" , \" The song was released on the Billboard Hot 100\n",
      "Ep 3 (Step 001525): Train loss 4.320, Val loss 5.220, Tokens seen: 6246400\n",
      "at the start of the film 's \" . \n",
      "   = = = = = = = = = =\n",
      "Ep 3 (Step 001530): Train loss 4.312, Val loss 5.214, Tokens seen: 6266880\n",
      "at the start of the film 's \" The Trouble with the film 's \" The Trouble with the film 's\n",
      "Ep 3 (Step 001535): Train loss 4.215, Val loss 5.223, Tokens seen: 6287360\n",
      "at the start of the first half of the first half of the first half of the first half of the first half of\n",
      "Ep 3 (Step 001540): Train loss 4.292, Val loss 5.229, Tokens seen: 6307840\n",
      "at the start of the film 's original soundtrack , and was released on the soundtrack . \n",
      "   = =\n",
      "Ep 3 (Step 001545): Train loss 4.321, Val loss 5.221, Tokens seen: 6328320\n",
      "at the start of the war , and the war , the New Jersey , the New Jersey , the New Jersey , the\n",
      "Ep 3 (Step 001550): Train loss 4.141, Val loss 5.225, Tokens seen: 6348800\n",
      "at the start of the film 's character , and the film was released as a member of the film . \n",
      "\n",
      "Ep 3 (Step 001555): Train loss 4.240, Val loss 5.234, Tokens seen: 6369280\n",
      "at the start of the film 's \" The Trouble with Tribble @-@ up to the film . The film\n",
      "Ep 3 (Step 001560): Train loss 4.144, Val loss 5.215, Tokens seen: 6389760\n",
      "at the start of the war . \n",
      "   = = = = = = = = = \n",
      "  \n",
      "Ep 3 (Step 001565): Train loss 4.155, Val loss 5.197, Tokens seen: 6410240\n",
      "at the start of the city , and the city of the city , and the city council , the city of the city\n",
      "Ep 3 (Step 001570): Train loss 4.258, Val loss 5.232, Tokens seen: 6430720\n",
      "at the start of the film 's character . He also performed the film 's soundtrack , but was not a \"\n",
      "Ep 3 (Step 001575): Train loss 4.241, Val loss 5.211, Tokens seen: 6451200\n",
      "at the start of the album , and the album was released on the album . The album was released on the album ,\n",
      "Ep 3 (Step 001580): Train loss 4.117, Val loss 5.216, Tokens seen: 6471680\n",
      "at the start of the season , and was the first season . \n",
      "   = = = = = = =\n",
      "Ep 3 (Step 001585): Train loss 4.293, Val loss 5.214, Tokens seen: 6492160\n",
      "at the start of the season , and the season , the team won the season . \n",
      "   = = =\n",
      "Ep 3 (Step 001590): Train loss 4.152, Val loss 5.193, Tokens seen: 6512640\n",
      "at the start of the album . The album was released on the album . The album was released on the album 's\n",
      "Ep 3 (Step 001595): Train loss 4.120, Val loss 5.199, Tokens seen: 6533120\n",
      "at the start of the night , and the first time , the first time of the first time , was the first time\n",
      "Ep 3 (Step 001600): Train loss 4.206, Val loss 5.193, Tokens seen: 6553600\n",
      "at the start of the first round of the first time , the first time of the first time , the first time of\n",
      "Ep 3 (Step 001605): Train loss 4.090, Val loss 5.203, Tokens seen: 6574080\n",
      "at the start of the storm . The storm watches was upgraded to the storm , and the storm caused flooding . The storm\n",
      "Ep 3 (Step 001610): Train loss 4.199, Val loss 5.187, Tokens seen: 6594560\n",
      "at the start of the first round of the first round of the second and the second half of the second half . The\n",
      "Ep 3 (Step 001615): Train loss 4.269, Val loss 5.202, Tokens seen: 6615040\n",
      "at the start of the first phase of the first two years of the second half of the second half of the second half\n",
      "Ep 3 (Step 001620): Train loss 4.200, Val loss 5.196, Tokens seen: 6635520\n",
      "at the start of the first two years of the first two years , the first two @-@ time , the first\n",
      "Ep 3 (Step 001625): Train loss 4.162, Val loss 5.196, Tokens seen: 6656000\n",
      "at the start of the likelihood of the likelihood of the likelihood of the likelihood of the king 's atmosphere . \n",
      "\n",
      "Ep 3 (Step 001630): Train loss 4.194, Val loss 5.190, Tokens seen: 6676480\n",
      "at the start of the war . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 3 (Step 001635): Train loss 4.143, Val loss 5.201, Tokens seen: 6696960\n",
      "at the start of the war . The ship was the first ship to be the first ship , and the first ship was\n",
      "Ep 3 (Step 001640): Train loss 4.085, Val loss 5.188, Tokens seen: 6717440\n",
      "at the start of the war . The ship was the first ship to be the first ship to be the first ship of\n",
      "Ep 3 (Step 001645): Train loss 4.039, Val loss 5.185, Tokens seen: 6737920\n",
      "at the start of the war , but the war was not to be the first time , but the German battle was unable\n",
      "Ep 3 (Step 001650): Train loss 4.172, Val loss 5.181, Tokens seen: 6758400\n",
      "at the start of the first time , but was the first time since the first time since the first time since the first\n",
      "Ep 3 (Step 001655): Train loss 4.081, Val loss 5.188, Tokens seen: 6778880\n",
      "at the start of the first two years of the first , and the first , and the first of the first two years\n",
      "Ep 3 (Step 001660): Train loss 4.200, Val loss 5.219, Tokens seen: 6799360\n",
      "at the start of the first season , the team 's team 's team , and the team 's team '\n",
      "Ep 3 (Step 001665): Train loss 4.021, Val loss 5.185, Tokens seen: 6819840\n",
      "at the start of the city of the city of the city of the city of the city of the city of the city\n",
      "Ep 3 (Step 001670): Train loss 4.068, Val loss 5.199, Tokens seen: 6840320\n",
      "at the start of the city . \n",
      "   = = = = = = \n",
      "   The city of\n",
      "Ep 3 (Step 001675): Train loss 4.098, Val loss 5.179, Tokens seen: 6860800\n",
      "at the start of the city . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 3 (Step 001680): Train loss 4.054, Val loss 5.171, Tokens seen: 6881280\n",
      "at the start of the war . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 3 (Step 001685): Train loss 4.102, Val loss 5.187, Tokens seen: 6901760\n",
      "at the start of the war , but the team 's first ever since the team 's first season , the team\n",
      "Ep 3 (Step 001690): Train loss 4.189, Val loss 5.171, Tokens seen: 6922240\n",
      "at the start of the war , and the war were forced to the war . The following year , the war had been\n",
      "Ep 3 (Step 001695): Train loss 4.088, Val loss 5.189, Tokens seen: 6942720\n",
      "at the start of the war , the war began to the war , and the war began to be the first to be\n",
      "Ep 3 (Step 001700): Train loss 4.133, Val loss 5.174, Tokens seen: 6963200\n",
      "at the start of the two years , and the first , the first two years of the first , and the first two\n",
      "Ep 3 (Step 001705): Train loss 4.023, Val loss 5.166, Tokens seen: 6983680\n",
      "at the start of the city 's population was the first time since the 19th century . \n",
      "   =\n",
      "Ep 3 (Step 001710): Train loss 4.179, Val loss 5.166, Tokens seen: 7004160\n",
      "at the start of the city , the city was the first city of the city of the city of the city . The\n",
      "Ep 3 (Step 001715): Train loss 4.176, Val loss 5.192, Tokens seen: 7024640\n",
      "at the start of the city , and the city was the first time of the city 's first city . \n",
      "\n",
      "Ep 3 (Step 001720): Train loss 4.058, Val loss 5.156, Tokens seen: 7045120\n",
      "at the start of the war . \n",
      "   = = = = = = = = = \n",
      "  \n",
      "Ep 3 (Step 001725): Train loss 4.077, Val loss 5.183, Tokens seen: 7065600\n",
      "at the start of the war , but the war was not to the war . The company had to be the first to\n",
      "Ep 3 (Step 001730): Train loss 3.982, Val loss 5.158, Tokens seen: 7086080\n",
      "at the start of the war , the war was to the city of the city , and the city was to be the\n",
      "Ep 3 (Step 001735): Train loss 4.031, Val loss 5.165, Tokens seen: 7106560\n",
      "at the start of the war , but the war was not to be the only way to be the only way to be\n",
      "Ep 3 (Step 001740): Train loss 4.078, Val loss 5.171, Tokens seen: 7127040\n",
      "at the start of the war , the war was to be the only to be the only to be the only way to\n",
      "Ep 3 (Step 001745): Train loss 4.019, Val loss 5.189, Tokens seen: 7147520\n",
      "at the start of the war . \n",
      "   = = = = = = = \n",
      "   The Romanian\n",
      "Ep 3 (Step 001750): Train loss 4.105, Val loss 5.198, Tokens seen: 7168000\n",
      "at the start of the war . \n",
      "   = = = = = \n",
      "    = = =\n",
      "Ep 3 (Step 001755): Train loss 3.990, Val loss 5.177, Tokens seen: 7188480\n",
      "at the start of the album , and the album 's release , and the album 's accompanying music , and the\n",
      "Ep 3 (Step 001760): Train loss 4.000, Val loss 5.173, Tokens seen: 7208960\n",
      "at the start of the city of the city of the city of the city . \n",
      "   = = = =\n",
      "Ep 3 (Step 001765): Train loss 3.979, Val loss 5.168, Tokens seen: 7229440\n",
      "at the start of the king of the king of the king of the king of the king , the king of the king\n",
      "Ep 3 (Step 001770): Train loss 3.942, Val loss 5.168, Tokens seen: 7249920\n",
      "at the start of the film 's film , and the film was a film of the film 's film . \n",
      "Ep 3 (Step 001775): Train loss 4.015, Val loss 5.157, Tokens seen: 7270400\n",
      "at the start of the film 's film , and the film was a film , and the film was released in the\n",
      "Ep 4 (Step 001780): Train loss 3.933, Val loss 5.154, Tokens seen: 7290880\n",
      "at the start of the song \" The song \" was written by the song \" My Boo \" . \n",
      "  \n",
      "Ep 4 (Step 001785): Train loss 3.991, Val loss 5.163, Tokens seen: 7311360\n",
      "at the start of the most powerful , the most famous and most prominent of the most prominent , the most famous and most\n",
      "Ep 4 (Step 001790): Train loss 4.029, Val loss 5.148, Tokens seen: 7331840\n",
      "at the start of the most important groups of the period of the species . \n",
      "   = = = = =\n",
      "Ep 4 (Step 001795): Train loss 4.037, Val loss 5.156, Tokens seen: 7352320\n",
      "at the start of the war . \n",
      "   = = = = = \n",
      "   The most important part\n",
      "Ep 4 (Step 001800): Train loss 3.968, Val loss 5.144, Tokens seen: 7372800\n",
      "at the start of the film , and the film was released in the United States . The film was released in the United\n",
      "Ep 4 (Step 001805): Train loss 3.872, Val loss 5.168, Tokens seen: 7393280\n",
      "at the start of the film , and the film was the first film in the film . The film was written by the\n",
      "Ep 4 (Step 001810): Train loss 3.888, Val loss 5.158, Tokens seen: 7413760\n",
      "at the start of the film 's film , and the film 's film was the first film 's second film\n",
      "Ep 4 (Step 001815): Train loss 3.918, Val loss 5.179, Tokens seen: 7434240\n",
      "at the start of the season , the season was the first season to be the first time since the season . \n",
      "\n",
      "Ep 4 (Step 001820): Train loss 3.910, Val loss 5.168, Tokens seen: 7454720\n",
      "at the start of the song was written by the song 's second single from the song . \n",
      "   =\n",
      "Ep 4 (Step 001825): Train loss 3.897, Val loss 5.170, Tokens seen: 7475200\n",
      "at the start of the song was released on the Hot 100 , and peaked at number one on the Hot 100 , and\n",
      "Ep 4 (Step 001830): Train loss 3.918, Val loss 5.163, Tokens seen: 7495680\n",
      "at the start of the season , the season began to be the first to be the first game in the season , but\n",
      "Ep 4 (Step 001835): Train loss 3.878, Val loss 5.164, Tokens seen: 7516160\n",
      "at the start of the season , the season began to be the first season premiere of the season , and the season began\n",
      "Ep 4 (Step 001840): Train loss 3.856, Val loss 5.169, Tokens seen: 7536640\n",
      "at the start of the season , the season began to be the first time since the season , when the season began to\n",
      "Ep 4 (Step 001845): Train loss 3.851, Val loss 5.168, Tokens seen: 7557120\n",
      "at the start of the season , the season began to be the first time since the season . The following season began to\n",
      "Ep 4 (Step 001850): Train loss 3.889, Val loss 5.163, Tokens seen: 7577600\n",
      "at the start of the season , he had a chance to the Yankees . \n",
      "   = = = = =\n",
      "Ep 4 (Step 001855): Train loss 3.952, Val loss 5.184, Tokens seen: 7598080\n",
      "at the start of the season , the season began to be the first time since the season . The following season began to\n",
      "Ep 4 (Step 001860): Train loss 3.881, Val loss 5.184, Tokens seen: 7618560\n",
      "at the start of the season , the team 's first season in the season , and finished the season . \n",
      "\n",
      "Ep 4 (Step 001865): Train loss 3.918, Val loss 5.168, Tokens seen: 7639040\n",
      "at the start of the song . \n",
      "   = = Reception = = \n",
      "   \" Moment of\n",
      "Ep 4 (Step 001870): Train loss 3.785, Val loss 5.150, Tokens seen: 7659520\n",
      "at the start of the song , and the song was performed by the song . The song was performed by the song ,\n",
      "Ep 4 (Step 001875): Train loss 3.763, Val loss 5.179, Tokens seen: 7680000\n",
      "at the start of the song , and the song was written by the song . \n",
      "   = = = Music\n",
      "Ep 4 (Step 001880): Train loss 3.849, Val loss 5.173, Tokens seen: 7700480\n",
      "at the start of the song . \n",
      "   = = = Music = = \n",
      "   The music video\n",
      "Ep 4 (Step 001885): Train loss 3.728, Val loss 5.174, Tokens seen: 7720960\n",
      "at the start of the nightingale and the song . \n",
      "   = = = Music = = \n",
      "\n",
      "Ep 4 (Step 001890): Train loss 3.757, Val loss 5.182, Tokens seen: 7741440\n",
      "at the start of the nightingale and the song . \n",
      "   = = = = = = = =\n",
      "Ep 4 (Step 001895): Train loss 3.751, Val loss 5.151, Tokens seen: 7761920\n",
      "at the start of the nightingale , the group 's first time in the second season , and the second season\n",
      "Ep 4 (Step 001900): Train loss 3.999, Val loss 5.159, Tokens seen: 7782400\n",
      "at the start of the season , the team had been given a team since the season . \n",
      "   = =\n",
      "Ep 4 (Step 001905): Train loss 3.733, Val loss 5.145, Tokens seen: 7802880\n",
      "at the start of the war , the team had been reduced to the team , but the team had been unable to make\n",
      "Ep 4 (Step 001910): Train loss 3.822, Val loss 5.149, Tokens seen: 7823360\n",
      "at the start of the nightingaleen , and the group were the first to be the first ceratopsians\n",
      "Ep 4 (Step 001915): Train loss 3.928, Val loss 5.149, Tokens seen: 7843840\n",
      "at the start of the nightingdon , the group had been used to the ceratopsians . \n",
      " \n",
      "Ep 4 (Step 001920): Train loss 3.813, Val loss 5.156, Tokens seen: 7864320\n",
      "at the start of the night , he was the first of the first time in the first time . \n",
      "  \n",
      "Ep 4 (Step 001925): Train loss 3.756, Val loss 5.154, Tokens seen: 7884800\n",
      "at the start of the night , he was appointed a group of the United States Army , and the United States Army ,\n",
      "Ep 4 (Step 001930): Train loss 3.705, Val loss 5.149, Tokens seen: 7905280\n",
      "at the start of the family 's name , and the family 's name , was the first to be the first\n",
      "Ep 4 (Step 001935): Train loss 3.837, Val loss 5.155, Tokens seen: 7925760\n",
      "at the start of the night of the war . \n",
      "   = = = = = = = = \n",
      "\n",
      "Ep 4 (Step 001940): Train loss 3.895, Val loss 5.158, Tokens seen: 7946240\n",
      "at the start of the night , which was the first of the first of the first ceratopsian group , which\n",
      "Ep 4 (Step 001945): Train loss 3.798, Val loss 5.166, Tokens seen: 7966720\n",
      "at the start of the king 's body . \n",
      "   = = = = = = = = \n",
      "\n",
      "Ep 4 (Step 001950): Train loss 3.860, Val loss 5.149, Tokens seen: 7987200\n",
      "at the start of the night of the war . \n",
      "   = = = = = = \n",
      "  \n",
      "Ep 4 (Step 001955): Train loss 3.824, Val loss 5.148, Tokens seen: 8007680\n",
      "at the start of the nightingale and the gods 't have been used to be the gods 't Help .\n",
      "Ep 4 (Step 001960): Train loss 3.706, Val loss 5.163, Tokens seen: 8028160\n",
      "at the start of the war . \n",
      "   = = = = First World War = = = \n",
      " \n",
      "Ep 4 (Step 001965): Train loss 3.786, Val loss 5.161, Tokens seen: 8048640\n",
      "at the start of the nightingale and the gods were the gods 'tze , and the gods were the same\n",
      "Ep 4 (Step 001970): Train loss 3.700, Val loss 5.173, Tokens seen: 8069120\n",
      "at the start of the night , which is a ceratopsian , which is a ceratopsian , which\n",
      "Ep 4 (Step 001975): Train loss 3.953, Val loss 5.160, Tokens seen: 8089600\n",
      "at the start of the night , which is the same as the same name , which is the same as the name of\n",
      "Ep 4 (Step 001980): Train loss 3.681, Val loss 5.149, Tokens seen: 8110080\n",
      "at the start of the war . \n",
      "   = = = = Chapel of the war = = = = \n",
      "Ep 4 (Step 001985): Train loss 3.684, Val loss 5.161, Tokens seen: 8130560\n",
      "at the start of the war . \n",
      "   = = = = Chapel of the war = = = = \n",
      "Ep 4 (Step 001990): Train loss 3.742, Val loss 5.152, Tokens seen: 8151040\n",
      "at the start of the war . \n",
      "   = = = First World War = = = \n",
      "  \n",
      "Ep 4 (Step 001995): Train loss 3.811, Val loss 5.147, Tokens seen: 8171520\n",
      "at the start of the war . \n",
      "   = = = = First World War = = = \n",
      " \n",
      "Ep 4 (Step 002000): Train loss 3.745, Val loss 5.157, Tokens seen: 8192000\n",
      "at the start of the war . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 4 (Step 002005): Train loss 3.838, Val loss 5.157, Tokens seen: 8212480\n",
      "at the start of the war . \n",
      "   = = = = First World War = = = \n",
      " \n",
      "Ep 4 (Step 002010): Train loss 3.853, Val loss 5.155, Tokens seen: 8232960\n",
      "at the start of the war . \n",
      "   = = = = = = = = = = = \n",
      "\n",
      "Ep 4 (Step 002015): Train loss 3.607, Val loss 5.167, Tokens seen: 8253440\n",
      "at the start of the season , which was the first team to win a three @-@ yard line . \n",
      "\n",
      "Ep 4 (Step 002020): Train loss 3.631, Val loss 5.158, Tokens seen: 8273920\n",
      "at the start of the war . \n",
      "   = = = = First World War = = = \n",
      " \n",
      "Ep 4 (Step 002025): Train loss 3.724, Val loss 5.161, Tokens seen: 8294400\n",
      "at the start of the season , he was named as the team 's team coach . \n",
      "   = =\n",
      "Ep 4 (Step 002030): Train loss 3.788, Val loss 5.147, Tokens seen: 8314880\n",
      "at the start of the night , and the group 's journey . \n",
      "   = = = = = \n",
      "Ep 4 (Step 002035): Train loss 3.606, Val loss 5.141, Tokens seen: 8335360\n",
      "at the start of the city , and the city was the first to be the first to be the largest city . \n",
      "Ep 4 (Step 002040): Train loss 3.697, Val loss 5.127, Tokens seen: 8355840\n",
      "at the start of the city , and the city was the first to be the first to be the first to be the\n",
      "Ep 4 (Step 002045): Train loss 3.673, Val loss 5.155, Tokens seen: 8376320\n",
      "at the start of the war . \n",
      "   = = = \n",
      "    = = = = =\n",
      "Ep 4 (Step 002050): Train loss 3.605, Val loss 5.167, Tokens seen: 8396800\n",
      "at the start of the war . \n",
      "   = = = = = = = \n",
      "    =\n",
      "Ep 4 (Step 002055): Train loss 3.669, Val loss 5.163, Tokens seen: 8417280\n",
      "at the start of the war . \n",
      "   = = = = = = = \n",
      "   The earliest\n",
      "Ep 4 (Step 002060): Train loss 3.713, Val loss 5.145, Tokens seen: 8437760\n",
      "at the start of the war . \n",
      "   = = = = = = = = = = \n",
      " \n",
      "Ep 4 (Step 002065): Train loss 3.640, Val loss 5.152, Tokens seen: 8458240\n",
      "at the start of the city . \n",
      "   = = = = = = = = = = = = \n",
      "Ep 4 (Step 002070): Train loss 3.670, Val loss 5.159, Tokens seen: 8478720\n",
      "at the start of the city . \n",
      "   = = = = \n",
      "   The first ceratops\n",
      "Ep 4 (Step 002075): Train loss 3.784, Val loss 5.133, Tokens seen: 8499200\n",
      "at the start of the city . \n",
      "   = = = = First World War = = = \n",
      " \n",
      "Ep 4 (Step 002080): Train loss 3.753, Val loss 5.122, Tokens seen: 8519680\n",
      "at the start of the ceratopsians , and the ceratopsians , and Mongolia , and Mongolia , and\n",
      "Ep 4 (Step 002085): Train loss 3.700, Val loss 5.148, Tokens seen: 8540160\n",
      "at the start of the ceratopsians , and the ceratopsians , and the ceratopsians ,\n",
      "Ep 4 (Step 002090): Train loss 3.755, Val loss 5.129, Tokens seen: 8560640\n",
      "at the start of the ceratopsians , and ceratopsians , and ceratopsians , and Mongolia\n",
      "Ep 4 (Step 002095): Train loss 3.675, Val loss 5.142, Tokens seen: 8581120\n",
      "at the start of the ceratopsians , and ceratopsians , ceratopsians , and Mongolia ,\n",
      "Ep 4 (Step 002100): Train loss 3.697, Val loss 5.134, Tokens seen: 8601600\n",
      "at the start of the ceratopsians , and the ceratopsians , which was the only ceratops\n",
      "Ep 4 (Step 002105): Train loss 3.605, Val loss 5.115, Tokens seen: 8622080\n",
      "at the start of the ceratopsians , and the ceratopsians , which was used for the cerat\n",
      "Ep 4 (Step 002110): Train loss 3.617, Val loss 5.136, Tokens seen: 8642560\n",
      "at the start of the city . \n",
      "   = = = = = = = \n",
      "   The city\n",
      "Ep 4 (Step 002115): Train loss 3.556, Val loss 5.133, Tokens seen: 8663040\n",
      "at the start of the city . \n",
      "   = = = = = = = = = = = = =\n",
      "Ep 4 (Step 002120): Train loss 3.577, Val loss 5.145, Tokens seen: 8683520\n",
      "at the start of the city . The city was constructed in the city of the city of the city of the city of\n",
      "Ep 4 (Step 002125): Train loss 3.597, Val loss 5.126, Tokens seen: 8704000\n",
      "at the start of the city . \n",
      "   = = = = = = = \n",
      "    =\n",
      "Ep 4 (Step 002130): Train loss 3.728, Val loss 5.144, Tokens seen: 8724480\n",
      "at the start of the night , the ceratopsidae , and the ceratopsian , and the cerat\n",
      "Ep 4 (Step 002135): Train loss 3.559, Val loss 5.142, Tokens seen: 8744960\n",
      "at the start of the night , the group 's body was to be used as a \" fairly large \" . \n",
      "Ep 4 (Step 002140): Train loss 3.688, Val loss 5.159, Tokens seen: 8765440\n",
      "at the start of the night , the group 's first , and the second @-@ known ceratopsian\n",
      "Ep 4 (Step 002145): Train loss 3.560, Val loss 5.148, Tokens seen: 8785920\n",
      "at the start of the season . \n",
      "   = = = = = = = \n",
      "    =\n",
      "Ep 4 (Step 002150): Train loss 3.623, Val loss 5.152, Tokens seen: 8806400\n",
      "at the start of the song . The song was written by the band 's second single , and the band 's\n",
      "Ep 4 (Step 002155): Train loss 3.598, Val loss 5.128, Tokens seen: 8826880\n",
      "at the start of the season . \n",
      "   = = = = = = = \n",
      "    =\n",
      "Ep 4 (Step 002160): Train loss 3.622, Val loss 5.144, Tokens seen: 8847360\n",
      "at the start of the season . \n",
      "   = = = = Return to the = = = = \n",
      "\n",
      "Ep 4 (Step 002165): Train loss 3.604, Val loss 5.135, Tokens seen: 8867840\n",
      "at the start of the season . \n",
      "   = = = = Return to the season = = = = \n",
      "Ep 4 (Step 002170): Train loss 3.724, Val loss 5.118, Tokens seen: 8888320\n",
      "at the start of the season . \n",
      "   = = = = Return to the season = = = = \n",
      "Ep 4 (Step 002175): Train loss 3.493, Val loss 5.132, Tokens seen: 8908800\n",
      "at the start of the season . \n",
      "   = = = Return to the season = = \n",
      "  \n",
      "Ep 4 (Step 002180): Train loss 3.565, Val loss 5.110, Tokens seen: 8929280\n",
      "at the start of the city . \n",
      "   = = = = = = \n",
      "   The city was\n",
      "Ep 4 (Step 002185): Train loss 3.624, Val loss 5.107, Tokens seen: 8949760\n",
      "at the start of the season . \n",
      "   = = = Return to the = = = \n",
      "  \n",
      "Ep 4 (Step 002190): Train loss 3.506, Val loss 5.116, Tokens seen: 8970240\n",
      "at the start of the season . \n",
      "   = = = Return to the = = \n",
      "   \n",
      "Ep 4 (Step 002195): Train loss 3.479, Val loss 5.108, Tokens seen: 8990720\n",
      "at the start of the season . \n",
      "   = = = = = = \n",
      "    = =\n",
      "Ep 4 (Step 002200): Train loss 3.468, Val loss 5.112, Tokens seen: 9011200\n",
      "at the start of the season . \n",
      "   = = = = = = = \n",
      "   The season\n",
      "Ep 4 (Step 002205): Train loss 3.567, Val loss 5.116, Tokens seen: 9031680\n",
      "at the start of the season . \n",
      "   = = = Return to the season = = \n",
      "  \n",
      "Ep 4 (Step 002210): Train loss 3.586, Val loss 5.110, Tokens seen: 9052160\n",
      "at the start of the season . \n",
      "   = = = Return to the = = = \n",
      "  \n",
      "Ep 4 (Step 002215): Train loss 3.475, Val loss 5.130, Tokens seen: 9072640\n",
      "at the start of the season . \n",
      "   = = = = = = \n",
      "   The first season\n",
      "Ep 4 (Step 002220): Train loss 3.499, Val loss 5.120, Tokens seen: 9093120\n",
      "at the start of the season . \n",
      "   = = = = = = \n",
      "   The first season\n",
      "Ep 4 (Step 002225): Train loss 3.552, Val loss 5.124, Tokens seen: 9113600\n",
      "at the start of the season . \n",
      "   = = = = = \n",
      "   The first two years\n",
      "Ep 4 (Step 002230): Train loss 3.434, Val loss 5.107, Tokens seen: 9134080\n",
      "at the start of the season . \n",
      "   = = = \n",
      "   In the early years of the\n",
      "Ep 4 (Step 002235): Train loss 3.406, Val loss 5.093, Tokens seen: 9154560\n",
      "at the start of the season . \n",
      "   = = = = = = \n",
      "   The first two\n",
      "Ep 4 (Step 002240): Train loss 3.533, Val loss 5.083, Tokens seen: 9175040\n",
      "at the start of the season . \n",
      "   = = = = = = Hurricane = = = = = \n",
      "Ep 4 (Step 002245): Train loss 3.575, Val loss 5.088, Tokens seen: 9195520\n",
      "at the start of the night , the city was the first to be a major city . \n",
      "   = =\n",
      "Ep 4 (Step 002250): Train loss 3.528, Val loss 5.081, Tokens seen: 9216000\n",
      "at the start of the night , and the ceratopsids are the most prominent ceratopsids . \n",
      "\n",
      "Ep 4 (Step 002255): Train loss 3.413, Val loss 5.096, Tokens seen: 9236480\n",
      "at the start of the night , and the ceratopsids were the most likely ceratopsids . \n",
      "\n",
      "Ep 4 (Step 002260): Train loss 3.483, Val loss 5.099, Tokens seen: 9256960\n",
      "at the start of the night , and the group was the first to be the first to be the first to mention the\n",
      "Ep 4 (Step 002265): Train loss 3.504, Val loss 5.103, Tokens seen: 9277440\n",
      "at the start of the group . \n",
      "   = = = Early life = = = \n",
      "   In\n",
      "Ep 4 (Step 002270): Train loss 3.451, Val loss 5.120, Tokens seen: 9297920\n",
      "at the start of the group . \n",
      "   = = = = Chapel of the = = = \n",
      " \n",
      "Ep 4 (Step 002275): Train loss 3.524, Val loss 5.101, Tokens seen: 9318400\n",
      "at the start of the war , the first time they were defeated . \n",
      "   = = = First World War\n",
      "🛑 Training interrupted. Saving checkpoint...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/gpt2_v2.py:260\u001B[0m, in \u001B[0;36mtrain_model_simple\u001B[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001B[0m\n\u001B[1;32m    259\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m--> 260\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    261\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/gpt2_v2.py:228\u001B[0m, in \u001B[0;36mloss_batch\u001B[0;34m(inputs, targets, model, device)\u001B[0m\n\u001B[1;32m    227\u001B[0m inputs, targets \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), targets\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m--> 228\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mcross_entropy(logits\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m), targets\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/gpt2_v2.py:144\u001B[0m, in \u001B[0;36mGPT2Model.forward\u001B[0;34m(self, token_ids)\u001B[0m\n\u001B[1;32m    143\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop(x)\n\u001B[0;32m--> 144\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    145\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal_norm(x)\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:240\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 240\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/gpt2_v2.py:121\u001B[0m, in \u001B[0;36mTransformerBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 121\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mln1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    122\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mff(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln2(x)))\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/gpt2_v2.py:86\u001B[0m, in \u001B[0;36mMultiHeadAttention.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     85\u001B[0m weights \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(scores, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 86\u001B[0m weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;66;03m# Compute output\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/modules/dropout.py:70\u001B[0m, in \u001B[0;36mDropout.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/nn/functional.py:1425\u001B[0m, in \u001B[0;36mdropout\u001B[0;34m(input, p, training, inplace)\u001B[0m\n\u001B[1;32m   1423\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdropout probability has to be between 0 and 1, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1424\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m-> 1425\u001B[0m     _VF\u001B[38;5;241m.\u001B[39mdropout_(\u001B[38;5;28minput\u001B[39m, p, training) \u001B[38;5;28;01mif\u001B[39;00m inplace \u001B[38;5;28;01melse\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1426\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 32\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m     31\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m---> 32\u001B[0m train_losses, val_losses, tokens_seen \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model_simple\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstart_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mat the start of\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuild_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Report execution time\u001B[39;00m\n\u001B[1;32m     46\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m (time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/gpt2_v2.py:284\u001B[0m, in \u001B[0;36mtrain_model_simple\u001B[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001B[0m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m🛑 Training interrupted. Saving checkpoint...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 284\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mepoch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel_state_dict\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moptimizer_state_dict\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcheckpoint_interrupted_epoch\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mepoch\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.pth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ Checkpoint saved. You can resume training later.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m train_losses, val_losses, tokens_seen_track\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/serialization.py:965\u001B[0m, in \u001B[0;36msave\u001B[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[0m\n\u001B[1;32m    963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[1;32m    964\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[0;32m--> 965\u001B[0m         \u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[43m            \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    967\u001B[0m \u001B[43m            \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    968\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    969\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    970\u001B[0m \u001B[43m            \u001B[49m\u001B[43m_disable_byteorder_record\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    972\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    973\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/torch/serialization.py:1266\u001B[0m, in \u001B[0;36m_save\u001B[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001B[0m\n\u001B[1;32m   1264\u001B[0m         storage \u001B[38;5;241m=\u001B[39m storage\u001B[38;5;241m.\u001B[39mcpu()\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001B[39;00m\n\u001B[0;32m-> 1266\u001B[0m \u001B[43mzip_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_record\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T23:34:18.509498Z",
     "start_time": "2025-06-08T23:34:13.269419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                   num_epochs=10, eval_freq=200, eval_iter=50,\n",
    "                   start_context=\"Once upon a time\", tokenizer=tokenizer,\n",
    "                   resume_path=\"checkpoints/checkpoint_epoch4.pth\")\n"
   ],
   "id": "eb674658e5f90631",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T00:50:55.696200Z",
     "start_time": "2025-06-13T00:50:53.065334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import GPT2Model, GPT_CONFIG_124M\n",
    "checkpoint = torch.load(\"checkpoint_epoch3.pth\", weights_only=True)\n",
    "\n",
    "model = GPT2Model(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)\n"
   ],
   "id": "f9864e8c15596da6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of the war , the first time they were defeated . \n",
      "   =\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
