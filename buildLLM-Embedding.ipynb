{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Tokenize: from text to words/tokens",
   "id": "d4a5938e2d81b6ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:39.963215Z",
     "start_time": "2025-06-02T07:39:39.960003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # Split by punctuation and whitespace\n",
    "    tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    # Remove empty strings and strip whitespace\n",
    "    tokens = [t.strip() for t in tokens if t.strip()]\n",
    "    return tokens\n"
   ],
   "id": "c4a6bbc632f286c9",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:39.975998Z",
     "start_time": "2025-06-02T07:39:39.972804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"Peter_Rabbit.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "tokens = tokenize(raw_text)\n",
    "print(tokens[:10])"
   ],
   "id": "e6aa2711f365bdd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once', 'upon', 'a', 'time', 'there', 'were', 'four', 'little', 'Rabbits', ',']\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Encode: from text to IDs",
   "id": "7558775bd9656d83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:39.997581Z",
     "start_time": "2025-06-02T07:39:39.995216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_vocab(whole_text):\n",
    "    tokens = tokenize(whole_text)\n",
    "    vocab = {token:id for id,token in enumerate(sorted(set(tokens)))}\n",
    "    return vocab"
   ],
   "id": "4b76a18fb297c3db",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.020613Z",
     "start_time": "2025-06-02T07:39:40.018104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = build_vocab(raw_text)\n",
    "\n",
    "print(len(vocab))\n",
    "print(list(vocab.items())[:20])"
   ],
   "id": "f15dbcec534444e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405\n",
      "[('!', 0), (\"'\", 1), (',', 2), ('--', 3), ('.', 4), (':', 5), (';', 6), ('A', 7), ('After', 8), ('Also', 9), ('An', 10), ('And', 11), ('Benjamin', 12), ('Bunny', 13), ('But', 14), ('Cotton-tail', 15), ('Cottontail', 16), ('END', 17), ('Father', 18), ('First', 19)]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.040784Z",
     "start_time": "2025-06-02T07:39:40.038898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode(vocab, text):\n",
    "    return [vocab[token] for token in tokenize(text)]"
   ],
   "id": "8b3ced424ad47ee",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.056527Z",
     "start_time": "2025-06-02T07:39:40.054699Z"
    }
   },
   "cell_type": "code",
   "source": "print(encode(vocab, \"Once upon a time there were four little Rabbits\"))",
   "id": "c57db1c693853921",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 373, 46, 354, 346, 386, 155, 210, 38]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Decode: from IDs to text",
   "id": "284dfdbcdf78e990"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.069061Z",
     "start_time": "2025-06-02T07:39:40.067328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def decode(vocab, ids):\n",
    "    vocab_inverse = {id:token for token,id in vocab.items()}\n",
    "    text= \" \".join([vocab_inverse[id] for id in ids])\n",
    "    return text"
   ],
   "id": "d86c9480f829c1eb",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.084403Z",
     "start_time": "2025-06-02T07:39:40.082737Z"
    }
   },
   "cell_type": "code",
   "source": "print(decode(vocab,[33, 373, 46, 354, 346, 386, 155, 210, 38]))",
   "id": "668dc099e7a3cfe9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there were four little Rabbits\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer: vocab, encode, decode",
   "id": "d042afabdd555ae8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.102635Z",
     "start_time": "2025-06-02T07:39:40.100111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_inverse = {id:token for token,id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.vocab[token] for token in tokenize(text)]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \" \".join([self.vocab_inverse[id] for id in ids])"
   ],
   "id": "46451b112ec3d1a1",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.116785Z",
     "start_time": "2025-06-02T07:39:40.114955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "print(tokenizer.decode(tokenizer.encode(\"Once upon a time there were four little Rabbits\")))"
   ],
   "id": "aa852172c00f9d23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there were four little Rabbits\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Special token: UNKnown/EndOfSentence",
   "id": "2b53633d19c0e08a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.133361Z",
     "start_time": "2025-06-02T07:39:40.132041Z"
    }
   },
   "cell_type": "code",
   "source": "# print(tokenizer.decode(tokenizer.encode(\"Once upon a time there were four little Rabbits, and they were all very happy.\")))",
   "id": "95bf32b02e4b68d9",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.147894Z",
     "start_time": "2025-06-02T07:39:40.146028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab['<unk>'] = len(vocab)\n",
    "\n",
    "print(list(vocab.items())[-5:])"
   ],
   "id": "810ea518c1ed354e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wriggled', 401), ('you', 402), ('young', 403), ('your', 404), ('<unk>', 405)]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.166668Z",
     "start_time": "2025-06-02T07:39:40.164357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_inverse = {id:token for token,id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        unk_id = self.vocab.get(\"<unk>\")\n",
    "        return [self.vocab.get(token,unk_id) for token in tokenize(text)]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \" \".join([self.vocab_inverse[id] for id in ids])"
   ],
   "id": "87bbe2366edc30bf",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.181420Z",
     "start_time": "2025-06-02T07:39:40.179354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.decode(tokenizer.encode(\"Once upon a time there were four little Rabbits, and they were all very happy.\")))"
   ],
   "id": "3262c8e34a54b248",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there were four little Rabbits , and <unk> were all very <unk> .\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# BytePair Encoding: break words into chunks/subwords\n",
    "- tiktoken: https://tiktokenizer.vercel.app/\n"
   ],
   "id": "4a12c8f16e8fb1eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.199849Z",
     "start_time": "2025-06-02T07:39:40.197945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"unbelievability\"))"
   ],
   "id": "212a05ef2faedfee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[403, 6667, 11203, 1799]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.216661Z",
     "start_time": "2025-06-02T07:39:40.214742Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.decode([403,12,6667,12,11203,12,1799]))",
   "id": "b784d6e40b1e2b41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un-bel-iev-ability\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.232875Z",
     "start_time": "2025-06-02T07:39:40.231129Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"vocab size of gpt2: \",tokenizer.n_vocab)",
   "id": "d630ed1d088ca736",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size of gpt2:  50257\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Sampling with Sliding Window",
   "id": "31185b10b4044f89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.251144Z",
     "start_time": "2025-06-02T07:39:40.247216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"Peter_Rabbit.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(\"Raw characters: \",len(raw_text))\n",
    "print(\"Raw unique characters: \",len(set(raw_text)))\n",
    "print(\"Raw words:\", len(raw_text.split()))\n",
    "print(\"Raw unique words:\", len(set(raw_text.split())))\n",
    "print(\"tokens: \",len(enc_text))\n",
    "print(\"first 15 token IDs: \", enc_text[:15])\n",
    "print(\"first 15 tokens: \",\"|\".join(tokenizer.decode([token]) for token in enc_text[:15]))"
   ],
   "id": "6646cde6c05bc0d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw characters:  5583\n",
      "Raw unique characters:  54\n",
      "Raw words: 975\n",
      "Raw unique words: 440\n",
      "tokens:  1547\n",
      "first 15 token IDs:  [7454, 2402, 257, 640, 612, 547, 1440, 1310, 22502, 896, 11, 290, 511, 3891, 198]\n",
      "first 15 tokens:  Once| upon| a| time| there| were| four| little| Rabb|its|,| and| their| names|\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.266167Z",
     "start_time": "2025-06-02T07:39:40.264242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_size = 5\n",
    "for i in range(1,context_size+1):\n",
    "    context = enc_text[:i]\n",
    "    desired = enc_text[i]\n",
    "    print(context, \"-->\", desired)"
   ],
   "id": "5cab4a0d79f29e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7454] --> 2402\n",
      "[7454, 2402] --> 257\n",
      "[7454, 2402, 257] --> 640\n",
      "[7454, 2402, 257, 640] --> 612\n",
      "[7454, 2402, 257, 640, 612] --> 547\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:40.284707Z",
     "start_time": "2025-06-02T07:39:40.282345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_size = 5\n",
    "for i in range(1,context_size+1):\n",
    "    context = enc_text[:i]\n",
    "    desired = enc_text[i]\n",
    "    print(tokenizer.decode(context), \"-->\", tokenizer.decode([desired]))"
   ],
   "id": "201746e24de0783f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once -->  upon\n",
      "Once upon -->  a\n",
      "Once upon a -->  time\n",
      "Once upon a time -->  there\n",
      "Once upon a time there -->  were\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.042642Z",
     "start_time": "2025-06-02T07:39:40.311698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt,tokenizer, context_size, stride):\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        assert len(token_ids) > context_size, \"Text is too short\"\n",
    "\n",
    "        self.input_ids = [torch.tensor(token_ids[i:i+context_size])\n",
    "                          for i in range(0, len(token_ids)-context_size, stride)]\n",
    "        self.target_ids = [torch.tensor(token_ids[i+1:i+context_size+1])\n",
    "                          for i in range(0, len(token_ids)-context_size, stride)]\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ],
   "id": "617a950a7c5c58db",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.052010Z",
     "start_time": "2025-06-02T07:39:43.049795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "def dataloader_v1(txt,batch_size=3,context_size=5,stride=2,shuffle=False,drop_last=True,num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt,tokenizer,context_size,stride)\n",
    "    return DataLoader(dataset, batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n"
   ],
   "id": "c4740a78d074e8fb",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.090584Z",
     "start_time": "2025-06-02T07:39:43.068562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"Peter_Rabbit.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = dataloader_v1(raw_text)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"shape of input: \",inputs.shape)\n",
    "print(\"first batch, input: \\n\", inputs,\"\\n targets: \\n\", targets)"
   ],
   "id": "90a51a257d8a3b58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input:  torch.Size([3, 5])\n",
      "first batch, input: \n",
      " tensor([[ 7454,  2402,   257,   640,   612],\n",
      "        [  257,   640,   612,   547,  1440],\n",
      "        [  612,   547,  1440,  1310, 22502]]) \n",
      " targets: \n",
      " tensor([[ 2402,   257,   640,   612,   547],\n",
      "        [  640,   612,   547,  1440,  1310],\n",
      "        [  547,  1440,  1310, 22502,   896]])\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Token Embedding: From Words to Vectors\n",
    "Vectors are\n",
    "- high-dimensional\n",
    "- dense\n",
    "- learnable\n",
    "\n",
    "Embedding is\n",
    "- looking up vectors from a big table\n",
    "- usually a matrix with shape (vocab_size, embed_dim)\n",
    "- initialized with random values\n",
    "- updated during training"
   ],
   "id": "eb77bd0444049311"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.125927Z",
     "start_time": "2025-06-02T07:39:43.101800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "vocab_size = 10\n",
    "embed_dim = 4\n",
    "torch.manual_seed(123)\n",
    "token_embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
    "print(\"token_embedding_layer shape: \", token_embedding_layer.weight.shape)\n",
    "print(\"token_embedding_layer weight: \", token_embedding_layer.weight)"
   ],
   "id": "419af6b6392929c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding_layer shape:  torch.Size([10, 4])\n",
      "token_embedding_layer weight:  Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880],\n",
      "        [ 0.3486,  0.6603, -0.2196, -0.3792],\n",
      "        [ 0.7671, -1.1925,  0.6984, -1.4097],\n",
      "        [ 0.1794,  1.8951,  0.4954,  0.2692],\n",
      "        [-0.0770, -1.0205, -0.1690,  0.9178],\n",
      "        [ 1.5810,  1.3010,  1.2753, -0.2010],\n",
      "        [ 0.9624,  0.2492, -0.4845, -2.0929],\n",
      "        [-0.8199, -0.4210, -0.9620,  1.2825],\n",
      "        [-0.3430, -0.6821, -0.9887, -1.7018],\n",
      "        [-0.7498, -1.1285,  0.4135,  0.2892]], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.142753Z",
     "start_time": "2025-06-02T07:39:43.138619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = torch.tensor([2,3,5])\n",
    "token_embeddings = token_embedding_layer(input_ids)\n",
    "print(\"token_embeddings: \\n\", token_embeddings) # return row 2,3,5 of token_embedding_layer.weight"
   ],
   "id": "f5f091aea61d7bd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embeddings: \n",
      " tensor([[ 0.7671, -1.1925,  0.6984, -1.4097],\n",
      "        [ 0.1794,  1.8951,  0.4954,  0.2692],\n",
      "        [ 1.5810,  1.3010,  1.2753, -0.2010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- GPT-2 uses an embedding table of size (50257 tokens × 768 dimensions).",
   "id": "36d374a0ebebd213"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.575534Z",
     "start_time": "2025-06-02T07:39:43.167299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "vocab_size = 50527\n",
    "embed_dim = 768\n",
    "torch.manual_seed(123)\n",
    "token_embedding_layer_gpt2 = nn.Embedding(vocab_size, embed_dim)\n",
    "print(\"token_embedding_layer_gpt2 shape: \", token_embedding_layer_gpt2.weight.shape)\n",
    "print(\"token_embedding_layer_gpt2 weight: \", token_embedding_layer_gpt2.weight)"
   ],
   "id": "15155af19beba2db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding_layer_gpt2 shape:  torch.Size([50527, 768])\n",
      "token_embedding_layer_gpt2 weight:  Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035,  ..., -0.3181, -1.3936,  0.5226],\n",
      "        [ 0.2579,  0.3420, -0.8168,  ..., -0.4098,  0.4978, -0.3721],\n",
      "        [ 0.7957,  0.5350,  0.9427,  ..., -1.0749,  0.0955, -1.4138],\n",
      "        ...,\n",
      "        [-1.8239,  0.0192,  0.9472,  ..., -0.2287,  1.0394,  0.1882],\n",
      "        [-0.8952, -1.3001,  1.4985,  ..., -0.5879, -0.0340, -0.0092],\n",
      "        [-1.3114, -2.2304, -0.4247,  ...,  0.8176,  1.3480, -0.5107]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.587981Z",
     "start_time": "2025-06-02T07:39:43.584713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = torch.tensor([2,3,5])\n",
    "print(token_embedding_layer_gpt2(input_ids))"
   ],
   "id": "6d50972e328cf124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7957,  0.5350,  0.9427,  ..., -1.0749,  0.0955, -1.4138],\n",
      "        [-0.0312,  1.6913, -2.2380,  ...,  0.2379, -1.1839, -0.3179],\n",
      "        [-0.4334, -0.5095, -0.7118,  ...,  0.8329,  0.2992,  0.2496]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Position Embedding: From Positon to Vectors\n",
    "position embeddin is\n",
    "- a matrix with shape (context_size, embed_dim)\n",
    "- initialized with random values\n",
    "- a learnable parameter, updated during training"
   ],
   "id": "24b736817407ccd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.625421Z",
     "start_time": "2025-06-02T07:39:43.621707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "context_size = 5\n",
    "embed_dim = 4\n",
    "torch.manual_seed(123)\n",
    "position_embedding_layer = nn.Embedding(context_size, embed_dim)\n",
    "print(\"position_embedding_layer shape: \", position_embedding_layer.weight.shape)\n",
    "print(\"position_embedding_layer weight: \", position_embedding_layer.weight)"
   ],
   "id": "36341f55b89764f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_embedding_layer shape:  torch.Size([5, 4])\n",
      "position_embedding_layer weight:  Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880],\n",
      "        [ 1.5810,  1.3010,  1.2753, -0.2010],\n",
      "        [-0.1606, -0.4015,  0.6957, -1.8061],\n",
      "        [-1.1589,  0.3255, -0.6315, -2.8400],\n",
      "        [-0.7849, -1.4096, -0.4076,  0.7953]], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Position embedding has nothing to do with token values or IDs — it's purely based on their positions in the sequence.",
   "id": "1f5df6042d2eeeb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.648703Z",
     "start_time": "2025-06-02T07:39:43.644983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = torch.tensor([2,3,5])\n",
    "position_embeddings = position_embedding_layer(torch.arange(len(input_ids))) # use Position of input_ids, NOT values of it\n",
    "print(\"position_embeddings: \\n\", position_embeddings) # return row 0,1,2 of position_embedding_layer.weight"
   ],
   "id": "bf5e375b6a21e836",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_embeddings: \n",
      " tensor([[ 0.3374, -0.1778, -0.3035, -0.5880],\n",
      "        [ 1.5810,  1.3010,  1.2753, -0.2010],\n",
      "        [-0.1606, -0.4015,  0.6957, -1.8061]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "input_embeddings = token_embeddings + pos_embeddings",
   "id": "2bd67e53baac64d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.675551Z",
     "start_time": "2025-06-02T07:39:43.672298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_embeddings = token_embeddings + position_embeddings\n",
    "print(\"shape of input_embeddings : \",input_embeddings.shape)\n",
    "print(\"input_embeddings: \", input_embeddings)"
   ],
   "id": "e897996664d078f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input_embeddings :  torch.Size([3, 4])\n",
      "input_embeddings:  tensor([[ 1.1045, -1.3703,  0.3948, -1.9977],\n",
      "        [ 1.7603,  3.1962,  1.7707,  0.0682],\n",
      "        [ 1.4204,  0.8996,  1.9710, -2.0070]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "GPT-2 uses a position embedding table of size (1024 positions × 768 dimensions).\n",
    "\n"
   ],
   "id": "435218d5469365c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.711615Z",
     "start_time": "2025-06-02T07:39:43.699823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "context_size = 1024\n",
    "embed_dim = 768\n",
    "torch.manual_seed(123)\n",
    "position_embedding_layer_gpt2 = nn.Embedding(context_size, embed_dim)\n",
    "print(\"position_embedding_layer_gpt2 shape: \", position_embedding_layer_gpt2.weight.shape)\n",
    "print(\"position_embedding_layer_gpt2 weight: \", position_embedding_layer_gpt2.weight)"
   ],
   "id": "f57d754106b7c437",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_embedding_layer_gpt2 shape:  torch.Size([1024, 768])\n",
      "position_embedding_layer_gpt2 weight:  Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035,  ..., -0.3181, -1.3936,  0.5226],\n",
      "        [ 0.2579,  0.3420, -0.8168,  ..., -0.4098,  0.4978, -0.3721],\n",
      "        [ 0.7957,  0.5350,  0.9427,  ..., -1.0749,  0.0955, -1.4138],\n",
      "        ...,\n",
      "        [-1.2094,  0.6397,  0.6342,  ..., -0.4582,  1.4911,  1.2406],\n",
      "        [-0.2253, -0.1078,  0.0479,  ...,  0.2521, -0.2893, -0.5639],\n",
      "        [-0.5375, -1.1562,  2.2554,  ...,  1.4322,  1.2488,  0.1897]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.777860Z",
     "start_time": "2025-06-02T07:39:43.725256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"Peter_Rabbit.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = dataloader_v1(raw_text,batch_size=3, context_size=1024,stride=2)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"shape of input: \",inputs.shape)\n",
    "print(\"first batch, input: \\n\", inputs,\"\\n targets: \\n\", targets)"
   ],
   "id": "7c3c15857b6c3df8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input:  torch.Size([3, 1024])\n",
      "first batch, input: \n",
      " tensor([[ 7454,  2402,   257,  ...,   480,   517,   290],\n",
      "        [  257,   640,   612,  ...,   290,   517, 36907],\n",
      "        [  612,   547,  1440,  ..., 36907,    13,  1763]]) \n",
      " targets: \n",
      " tensor([[ 2402,   257,   640,  ...,   517,   290,   517],\n",
      "        [  640,   612,   547,  ...,   517, 36907,    13],\n",
      "        [  547,  1440,  1310,  ...,    13,  1763,  1473]])\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T07:39:43.801356Z",
     "start_time": "2025-06-02T07:39:43.795743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_embeddings = token_embedding_layer_gpt2(inputs)\n",
    "print(\"shape of token_embeddings: \",token_embeddings.shape)\n",
    "# print(\"token_embeddings: \", token_embeddings)\n",
    "\n",
    "position_embeddings = position_embedding_layer_gpt2(torch.arange(context_size))\n",
    "print(\"shape of position_embeddings: \",position_embeddings.shape)\n",
    "# print(\"position_embeddings: \", position_embeddings)\n",
    "\n",
    "# token_embeddings shape: [batch_size, seq_len, embedding_dim]\n",
    "# position_embeddings shape: [seq_len, embedding_dim]\n",
    "# PyTorch automatically broadcasts position_embeddings across batch dimension\n",
    "input_embeddings = token_embeddings + position_embeddings\n",
    "print(\"shape of input_embeddings : \",input_embeddings.shape)\n",
    "# print(\"input_embeddings: \", input_embeddings)"
   ],
   "id": "2db8b4f396044304",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of token_embeddings:  torch.Size([3, 1024, 768])\n",
      "shape of position_embeddings:  torch.Size([1024, 768])\n",
      "shape of input_embeddings :  torch.Size([3, 1024, 768])\n"
     ]
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
