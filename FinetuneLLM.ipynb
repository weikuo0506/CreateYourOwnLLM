{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-tune LLM to follow instructions\n",
   "id": "b6e02cf5201e2731"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load open weights",
   "id": "91e1b02b97bac08c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:25:45.454408Z",
     "start_time": "2025-06-15T08:25:44.675861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import GPT2Model, GPT_CONFIG_124M, complete_text, generate_text_simple, tensor_to_text, text_to_tensor\n",
    "\n",
    "GPT_CONFIG_124M.update({\"qkv_bias\": True})\n",
    "model = GPT2Model(GPT_CONFIG_124M)\n"
   ],
   "id": "423a8cab44a48e29",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:11:56.833964Z",
     "start_time": "2025-06-15T02:11:56.819066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ],
   "id": "891e084dd7eeb90a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768])\n",
      "pos_emb.weight torch.Size([1024, 768])\n",
      "blocks.0.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_Q.bias torch.Size([768])\n",
      "blocks.0.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_K.bias torch.Size([768])\n",
      "blocks.0.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_V.bias torch.Size([768])\n",
      "blocks.0.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.0.attn.out_proj.bias torch.Size([768])\n",
      "blocks.0.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.0.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.0.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.0.ff.layers.2.bias torch.Size([768])\n",
      "blocks.0.ln1.weight torch.Size([768])\n",
      "blocks.0.ln1.bias torch.Size([768])\n",
      "blocks.0.ln2.weight torch.Size([768])\n",
      "blocks.0.ln2.bias torch.Size([768])\n",
      "blocks.1.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_Q.bias torch.Size([768])\n",
      "blocks.1.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_K.bias torch.Size([768])\n",
      "blocks.1.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_V.bias torch.Size([768])\n",
      "blocks.1.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.1.attn.out_proj.bias torch.Size([768])\n",
      "blocks.1.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.1.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.1.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.1.ff.layers.2.bias torch.Size([768])\n",
      "blocks.1.ln1.weight torch.Size([768])\n",
      "blocks.1.ln1.bias torch.Size([768])\n",
      "blocks.1.ln2.weight torch.Size([768])\n",
      "blocks.1.ln2.bias torch.Size([768])\n",
      "blocks.2.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_Q.bias torch.Size([768])\n",
      "blocks.2.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_K.bias torch.Size([768])\n",
      "blocks.2.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_V.bias torch.Size([768])\n",
      "blocks.2.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.2.attn.out_proj.bias torch.Size([768])\n",
      "blocks.2.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.2.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.2.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.2.ff.layers.2.bias torch.Size([768])\n",
      "blocks.2.ln1.weight torch.Size([768])\n",
      "blocks.2.ln1.bias torch.Size([768])\n",
      "blocks.2.ln2.weight torch.Size([768])\n",
      "blocks.2.ln2.bias torch.Size([768])\n",
      "blocks.3.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_Q.bias torch.Size([768])\n",
      "blocks.3.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_K.bias torch.Size([768])\n",
      "blocks.3.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_V.bias torch.Size([768])\n",
      "blocks.3.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.3.attn.out_proj.bias torch.Size([768])\n",
      "blocks.3.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.3.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.3.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.3.ff.layers.2.bias torch.Size([768])\n",
      "blocks.3.ln1.weight torch.Size([768])\n",
      "blocks.3.ln1.bias torch.Size([768])\n",
      "blocks.3.ln2.weight torch.Size([768])\n",
      "blocks.3.ln2.bias torch.Size([768])\n",
      "blocks.4.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_Q.bias torch.Size([768])\n",
      "blocks.4.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_K.bias torch.Size([768])\n",
      "blocks.4.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_V.bias torch.Size([768])\n",
      "blocks.4.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.4.attn.out_proj.bias torch.Size([768])\n",
      "blocks.4.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.4.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.4.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.4.ff.layers.2.bias torch.Size([768])\n",
      "blocks.4.ln1.weight torch.Size([768])\n",
      "blocks.4.ln1.bias torch.Size([768])\n",
      "blocks.4.ln2.weight torch.Size([768])\n",
      "blocks.4.ln2.bias torch.Size([768])\n",
      "blocks.5.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_Q.bias torch.Size([768])\n",
      "blocks.5.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_K.bias torch.Size([768])\n",
      "blocks.5.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_V.bias torch.Size([768])\n",
      "blocks.5.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.5.attn.out_proj.bias torch.Size([768])\n",
      "blocks.5.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.5.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.5.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.5.ff.layers.2.bias torch.Size([768])\n",
      "blocks.5.ln1.weight torch.Size([768])\n",
      "blocks.5.ln1.bias torch.Size([768])\n",
      "blocks.5.ln2.weight torch.Size([768])\n",
      "blocks.5.ln2.bias torch.Size([768])\n",
      "blocks.6.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_Q.bias torch.Size([768])\n",
      "blocks.6.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_K.bias torch.Size([768])\n",
      "blocks.6.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_V.bias torch.Size([768])\n",
      "blocks.6.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.6.attn.out_proj.bias torch.Size([768])\n",
      "blocks.6.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.6.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.6.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.6.ff.layers.2.bias torch.Size([768])\n",
      "blocks.6.ln1.weight torch.Size([768])\n",
      "blocks.6.ln1.bias torch.Size([768])\n",
      "blocks.6.ln2.weight torch.Size([768])\n",
      "blocks.6.ln2.bias torch.Size([768])\n",
      "blocks.7.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_Q.bias torch.Size([768])\n",
      "blocks.7.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_K.bias torch.Size([768])\n",
      "blocks.7.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_V.bias torch.Size([768])\n",
      "blocks.7.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.7.attn.out_proj.bias torch.Size([768])\n",
      "blocks.7.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.7.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.7.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.7.ff.layers.2.bias torch.Size([768])\n",
      "blocks.7.ln1.weight torch.Size([768])\n",
      "blocks.7.ln1.bias torch.Size([768])\n",
      "blocks.7.ln2.weight torch.Size([768])\n",
      "blocks.7.ln2.bias torch.Size([768])\n",
      "blocks.8.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_Q.bias torch.Size([768])\n",
      "blocks.8.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_K.bias torch.Size([768])\n",
      "blocks.8.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_V.bias torch.Size([768])\n",
      "blocks.8.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.8.attn.out_proj.bias torch.Size([768])\n",
      "blocks.8.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.8.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.8.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.8.ff.layers.2.bias torch.Size([768])\n",
      "blocks.8.ln1.weight torch.Size([768])\n",
      "blocks.8.ln1.bias torch.Size([768])\n",
      "blocks.8.ln2.weight torch.Size([768])\n",
      "blocks.8.ln2.bias torch.Size([768])\n",
      "blocks.9.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_Q.bias torch.Size([768])\n",
      "blocks.9.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_K.bias torch.Size([768])\n",
      "blocks.9.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_V.bias torch.Size([768])\n",
      "blocks.9.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.9.attn.out_proj.bias torch.Size([768])\n",
      "blocks.9.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.9.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.9.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.9.ff.layers.2.bias torch.Size([768])\n",
      "blocks.9.ln1.weight torch.Size([768])\n",
      "blocks.9.ln1.bias torch.Size([768])\n",
      "blocks.9.ln2.weight torch.Size([768])\n",
      "blocks.9.ln2.bias torch.Size([768])\n",
      "blocks.10.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_Q.bias torch.Size([768])\n",
      "blocks.10.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_K.bias torch.Size([768])\n",
      "blocks.10.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_V.bias torch.Size([768])\n",
      "blocks.10.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.10.attn.out_proj.bias torch.Size([768])\n",
      "blocks.10.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.10.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.10.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.10.ff.layers.2.bias torch.Size([768])\n",
      "blocks.10.ln1.weight torch.Size([768])\n",
      "blocks.10.ln1.bias torch.Size([768])\n",
      "blocks.10.ln2.weight torch.Size([768])\n",
      "blocks.10.ln2.bias torch.Size([768])\n",
      "blocks.11.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_Q.bias torch.Size([768])\n",
      "blocks.11.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_K.bias torch.Size([768])\n",
      "blocks.11.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_V.bias torch.Size([768])\n",
      "blocks.11.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.11.attn.out_proj.bias torch.Size([768])\n",
      "blocks.11.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.11.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.11.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.11.ff.layers.2.bias torch.Size([768])\n",
      "blocks.11.ln1.weight torch.Size([768])\n",
      "blocks.11.ln1.bias torch.Size([768])\n",
      "blocks.11.ln2.weight torch.Size([768])\n",
      "blocks.11.ln2.bias torch.Size([768])\n",
      "final_norm.weight torch.Size([768])\n",
      "final_norm.bias torch.Size([768])\n",
      "out_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:25:50.948739Z",
     "start_time": "2025-06-15T08:25:50.277821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)"
   ],
   "id": "49648386cc30dc35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start ofucc Matth Names sankleg sprayimize inflicting ShallUTC ))) spill Main insanely mph\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download GPT2 from OpenAI",
   "id": "2ba0d473bfd76ff9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:15:17.049393Z",
     "start_time": "2025-06-15T02:15:17.036015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def download_file(url, destination, backup_url=None):\n",
    "    def _attempt_download(download_url):\n",
    "        with urllib.request.urlopen(download_url) as response:\n",
    "            total_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "            if os.path.exists(destination) and os.path.getsize(destination) == total_size:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return True\n",
    "\n",
    "            with tqdm(total=total_size, unit=\"iB\", unit_scale=True, desc=os.path.basename(download_url)) as pbar, \\\n",
    "                 open(destination, \"wb\") as f:\n",
    "                for chunk in iter(lambda: response.read(1024), b\"\"):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "            return True\n",
    "\n",
    "    try:\n",
    "        if _attempt_download(url):\n",
    "            return\n",
    "    except (urllib.error.HTTPError, urllib.error.URLError):\n",
    "        if backup_url:\n",
    "            print(f\"Primary URL failed. Trying backup URL: {backup_url}\")\n",
    "            try:\n",
    "                if _attempt_download(backup_url):\n",
    "                    return\n",
    "            except (urllib.error.HTTPError, urllib.error.URLError):\n",
    "                pass\n",
    "        print(f\"Failed to download from primary URL ({url})\"\n",
    "              + (f\" and backup URL ({backup_url})\" if backup_url else \"\") + \".\\n\"\n",
    "              \"Check your internet connection or the file availability.\\n\"\n",
    "              \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    allowed_sizes = {\"124M\", \"355M\", \"774M\", \"1558M\"}\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size must be one of {allowed_sizes}\")\n",
    "\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    base_url = f\"https://openaipublic.blob.core.windows.net/gpt-2/models/{model_size}/\"\n",
    "    backup_url = f\"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2/{model_size}/\"\n",
    "\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    for fname in filenames:\n",
    "        dst = os.path.join(model_dir, fname)\n",
    "        if os.path.exists(dst):\n",
    "            print(f\"Already exists: {fname}, skipping download.\")\n",
    "            continue\n",
    "        primary = urljoin(base_url, fname)\n",
    "        backup = urljoin(backup_url, fname)\n",
    "        print(f\"Downloading {fname} ...\")\n",
    "        download_file(primary, dst, backup)\n",
    "\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    with open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        settings = json.load(f)\n",
    "\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "    return settings, params\n"
   ],
   "id": "f29d5d3c3d0402a0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:25:59.855760Z",
     "start_time": "2025-06-15T08:25:59.332644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Params:\", params.keys())"
   ],
   "id": "28d0828f52dfce6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: checkpoint, skipping download.\n",
      "Already exists: encoder.json, skipping download.\n",
      "Already exists: hparams.json, skipping download.\n",
      "Already exists: model.ckpt.data-00000-of-00001, skipping download.\n",
      "Already exists: model.ckpt.index, skipping download.\n",
      "Already exists: model.ckpt.meta, skipping download.\n",
      "Already exists: vocab.bpe, skipping download.\n",
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Params: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:26:02.734869Z",
     "start_time": "2025-06-15T08:26:02.688666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def assign_(left, right):\n",
    "    if right is None:\n",
    "        raise ValueError(\"'right' cannot be None\")\n",
    "    right_tensor = torch.as_tensor(right, dtype=left.dtype, device=left.device)\n",
    "    if right_tensor.numel() == 0:\n",
    "        raise ValueError(\"'right' cannot be Empty\")\n",
    "    if left.shape != right_tensor.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {left.shape} vs {right_tensor.shape}\")\n",
    "    with torch.no_grad():\n",
    "        left.copy_(right_tensor)\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    assign_(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    assign_(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b, (block, pblock) in enumerate(zip(gpt.blocks, params[\"blocks\"])):\n",
    "        # Attention QKV\n",
    "        qw, kw, vw = np.split(pblock[\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
    "        qb, kb, vb = np.split(pblock[\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
    "        assign_(block.attn.W_Q.weight, qw.T)\n",
    "        assign_(block.attn.W_K.weight, kw.T)\n",
    "        assign_(block.attn.W_V.weight, vw.T)\n",
    "        assign_(block.attn.W_Q.bias, qb)\n",
    "        assign_(block.attn.W_K.bias, kb)\n",
    "        assign_(block.attn.W_V.bias, vb)\n",
    "\n",
    "        # Attention output projection\n",
    "        assign_(block.attn.out_proj.weight, pblock[\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        assign_(block.attn.out_proj.bias,   pblock[\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # Feedforward\n",
    "        assign_(block.ff.layers[0].weight, pblock[\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        assign_(block.ff.layers[0].bias,   pblock[\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        assign_(block.ff.layers[2].weight, pblock[\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        assign_(block.ff.layers[2].bias,   pblock[\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # LayerNorms\n",
    "        assign_(block.ln1.weight, pblock[\"ln_1\"][\"g\"])\n",
    "        assign_(block.ln1.bias, pblock[\"ln_1\"][\"b\"])\n",
    "        assign_(block.ln2.weight, pblock[\"ln_2\"][\"g\"])\n",
    "        assign_(block.ln2.bias, pblock[\"ln_2\"][\"b\"])\n",
    "\n",
    "    assign_(gpt.final_norm.weight, params[\"g\"])\n",
    "    assign_(gpt.final_norm.bias, params[\"b\"])\n",
    "    assign_(gpt.out_head.weight,  params[\"wte\"])\n"
   ],
   "id": "e83e9805c9622997",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:26:06.417612Z",
     "start_time": "2025-06-15T08:26:06.272488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_weights_into_gpt(model, params)\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n"
   ],
   "id": "7696090694c7c0c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:23:59.896307Z",
     "start_time": "2025-06-15T02:23:59.824877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.mean().item(), param.std().item())\n"
   ],
   "id": "6ae4be9bb4b68198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768]) 0.00037981756031513214 0.14369554817676544\n",
      "pos_emb.weight torch.Size([1024, 768]) -0.0006787165184505284 0.1226913258433342\n",
      "blocks.0.attn.W_Q.weight torch.Size([768, 768]) 0.00015374351642094553 0.2386905699968338\n",
      "blocks.0.attn.W_Q.bias torch.Size([768]) -0.007821076549589634 0.3427544832229614\n",
      "blocks.0.attn.W_K.weight torch.Size([768, 768]) 1.2351122677500825e-05 0.2432965785264969\n",
      "blocks.0.attn.W_K.bias torch.Size([768]) 0.0048723239451646805 0.18297071754932404\n",
      "blocks.0.attn.W_V.weight torch.Size([768, 768]) -5.968316145299468e-06 0.05811797454953194\n",
      "blocks.0.attn.W_V.bias torch.Size([768]) 0.0008267878438346088 0.04772818833589554\n",
      "blocks.0.attn.out_proj.weight torch.Size([768, 768]) -0.0001613790518604219 0.1474614143371582\n",
      "blocks.0.attn.out_proj.bias torch.Size([768]) -0.00691022165119648 0.2589662969112396\n",
      "blocks.0.ff.layers.0.weight torch.Size([3072, 768]) -0.0007485305541194975 0.14116929471492767\n",
      "blocks.0.ff.layers.0.bias torch.Size([3072]) -0.0931621864438057 0.13235801458358765\n",
      "blocks.0.ff.layers.2.weight torch.Size([768, 3072]) 8.009047633095179e-06 0.0879654809832573\n",
      "blocks.0.ff.layers.2.bias torch.Size([768]) -0.0004230523481965065 0.10169976204633713\n",
      "blocks.0.ln1.weight torch.Size([768]) 0.18035894632339478 0.04131494462490082\n",
      "blocks.0.ln1.bias torch.Size([768]) -0.006593453232198954 0.03580174222588539\n",
      "blocks.0.ln2.weight torch.Size([768]) 0.8678296208381653 0.48494789004325867\n",
      "blocks.0.ln2.bias torch.Size([768]) 0.00920353177934885 0.07009701430797577\n",
      "blocks.1.attn.W_Q.weight torch.Size([768, 768]) -0.00015265395632013679 0.15103811025619507\n",
      "blocks.1.attn.W_Q.bias torch.Size([768]) 0.001711765886284411 0.3485022187232971\n",
      "blocks.1.attn.W_K.weight torch.Size([768, 768]) 0.00023460258671548218 0.15902258455753326\n",
      "blocks.1.attn.W_K.bias torch.Size([768]) 0.0008940294501371682 0.08274678885936737\n",
      "blocks.1.attn.W_V.weight torch.Size([768, 768]) 2.3152324502007104e-06 0.10367371886968613\n",
      "blocks.1.attn.W_V.bias torch.Size([768]) -0.00020504526037257165 0.07381219416856766\n",
      "blocks.1.attn.out_proj.weight torch.Size([768, 768]) -8.275873551610857e-05 0.10191785544157028\n",
      "blocks.1.attn.out_proj.bias torch.Size([768]) -0.0010727141052484512 0.104808010160923\n",
      "blocks.1.ff.layers.0.weight torch.Size([3072, 768]) 0.0006420306744985282 0.1307205706834793\n",
      "blocks.1.ff.layers.0.bias torch.Size([3072]) -0.07219841331243515 0.09491100162267685\n",
      "blocks.1.ff.layers.2.weight torch.Size([768, 3072]) 9.794025390874594e-05 0.08719106763601303\n",
      "blocks.1.ff.layers.2.bias torch.Size([768]) 0.0002505724842194468 0.10042908787727356\n",
      "blocks.1.ln1.weight torch.Size([768]) 0.22284089028835297 0.05130758509039879\n",
      "blocks.1.ln1.bias torch.Size([768]) -0.0050234864465892315 0.052436452358961105\n",
      "blocks.1.ln2.weight torch.Size([768]) 0.24269406497478485 0.03164536505937576\n",
      "blocks.1.ln2.bias torch.Size([768]) -0.004073990508913994 0.03919508308172226\n",
      "blocks.2.attn.W_Q.weight torch.Size([768, 768]) 0.0001179432583739981 0.18932299315929413\n",
      "blocks.2.attn.W_Q.bias torch.Size([768]) -0.006064689252525568 0.2589069902896881\n",
      "blocks.2.attn.W_K.weight torch.Size([768, 768]) -4.986231942893937e-05 0.15181933343410492\n",
      "blocks.2.attn.W_K.bias torch.Size([768]) -0.003441582666710019 0.10127473622560501\n",
      "blocks.2.attn.W_V.weight torch.Size([768, 768]) 0.00017838712665252388 0.10502831637859344\n",
      "blocks.2.attn.W_V.bias torch.Size([768]) -0.0021237407345324755 0.0655340775847435\n",
      "blocks.2.attn.out_proj.weight torch.Size([768, 768]) -3.299467061879113e-05 0.08103547245264053\n",
      "blocks.2.attn.out_proj.bias torch.Size([768]) 0.003375591477379203 0.1451117843389511\n",
      "blocks.2.ff.layers.0.weight torch.Size([3072, 768]) -0.005061259493231773 0.13352689146995544\n",
      "blocks.2.ff.layers.0.bias torch.Size([3072]) -0.0928223729133606 0.1066708043217659\n",
      "blocks.2.ff.layers.2.weight torch.Size([768, 3072]) 0.00019714866357389838 0.09308750927448273\n",
      "blocks.2.ff.layers.2.bias torch.Size([768]) 0.002819357207044959 0.11242914199829102\n",
      "blocks.2.ln1.weight torch.Size([768]) 0.24077002704143524 0.07527267932891846\n",
      "blocks.2.ln1.bias torch.Size([768]) -0.00036007841117680073 0.07055915892124176\n",
      "blocks.2.ln2.weight torch.Size([768]) 0.2925874888896942 0.04540559649467468\n",
      "blocks.2.ln2.bias torch.Size([768]) 0.006347864866256714 0.043932873755693436\n",
      "blocks.3.attn.W_Q.weight torch.Size([768, 768]) -0.00021896824182476848 0.16474692523479462\n",
      "blocks.3.attn.W_Q.bias torch.Size([768]) -0.0038416890893131495 0.2126476913690567\n",
      "blocks.3.attn.W_K.weight torch.Size([768, 768]) -5.696068546967581e-05 0.1537768691778183\n",
      "blocks.3.attn.W_K.bias torch.Size([768]) -0.0004758095892611891 0.10474759340286255\n",
      "blocks.3.attn.W_V.weight torch.Size([768, 768]) 0.00020247689099051058 0.09766855090856552\n",
      "blocks.3.attn.W_V.bias torch.Size([768]) 0.0016517918556928635 0.0643523558974266\n",
      "blocks.3.attn.out_proj.weight torch.Size([768, 768]) 3.374144580448046e-05 0.0841251015663147\n",
      "blocks.3.attn.out_proj.bias torch.Size([768]) -0.0015612887218594551 0.10791037976741791\n",
      "blocks.3.ff.layers.0.weight torch.Size([3072, 768]) -0.00595002481713891 0.12953029572963715\n",
      "blocks.3.ff.layers.0.bias torch.Size([3072]) -0.09253177046775818 0.08565898984670639\n",
      "blocks.3.ff.layers.2.weight torch.Size([768, 3072]) 0.00017645648040343076 0.09180603921413422\n",
      "blocks.3.ff.layers.2.bias torch.Size([768]) 0.0021020257845520973 0.11511888355016708\n",
      "blocks.3.ln1.weight torch.Size([768]) 0.3010978400707245 0.05351252108812332\n",
      "blocks.3.ln1.bias torch.Size([768]) 0.0054475064389407635 0.07015811651945114\n",
      "blocks.3.ln2.weight torch.Size([768]) 0.30650773644447327 0.05265355482697487\n",
      "blocks.3.ln2.bias torch.Size([768]) 0.00996524840593338 0.04357629269361496\n",
      "blocks.4.attn.W_Q.weight torch.Size([768, 768]) 0.0002935132652055472 0.1704353392124176\n",
      "blocks.4.attn.W_Q.bias torch.Size([768]) 0.012315389700233936 0.20646966993808746\n",
      "blocks.4.attn.W_K.weight torch.Size([768, 768]) 5.515472003025934e-05 0.15752458572387695\n",
      "blocks.4.attn.W_K.bias torch.Size([768]) 0.004667005501687527 0.3554832935333252\n",
      "blocks.4.attn.W_V.weight torch.Size([768, 768]) 0.00010751151421573013 0.10232267528772354\n",
      "blocks.4.attn.W_V.bias torch.Size([768]) -0.0013783341273665428 0.05650889128446579\n",
      "blocks.4.attn.out_proj.weight torch.Size([768, 768]) -1.1396015906939283e-05 0.09297914057970047\n",
      "blocks.4.attn.out_proj.bias torch.Size([768]) -0.0009013282251544297 0.10049892961978912\n",
      "blocks.4.ff.layers.0.weight torch.Size([3072, 768]) -0.0032636644318699837 0.12971320748329163\n",
      "blocks.4.ff.layers.0.bias torch.Size([3072]) -0.08612614870071411 0.09320200234651566\n",
      "blocks.4.ff.layers.2.weight torch.Size([768, 3072]) 0.0001799796591512859 0.09099913388490677\n",
      "blocks.4.ff.layers.2.bias torch.Size([768]) 0.0016059394693002105 0.13685975968837738\n",
      "blocks.4.ln1.weight torch.Size([768]) 0.31934547424316406 0.04739116132259369\n",
      "blocks.4.ln1.bias torch.Size([768]) 0.007916287519037724 0.06749274581670761\n",
      "blocks.4.ln2.weight torch.Size([768]) 0.2725818455219269 0.043896984308958054\n",
      "blocks.4.ln2.bias torch.Size([768]) 0.0009596580639481544 0.02688399702310562\n",
      "blocks.5.attn.W_Q.weight torch.Size([768, 768]) -0.00026773728313855827 0.14125582575798035\n",
      "blocks.5.attn.W_Q.bias torch.Size([768]) -0.0016259821131825447 0.12543262541294098\n",
      "blocks.5.attn.W_K.weight torch.Size([768, 768]) 4.6552144340239465e-05 0.13614720106124878\n",
      "blocks.5.attn.W_K.bias torch.Size([768]) 0.006209613289684057 0.10714870691299438\n",
      "blocks.5.attn.W_V.weight torch.Size([768, 768]) -0.00010422924242448062 0.10330777615308762\n",
      "blocks.5.attn.W_V.bias torch.Size([768]) -0.001447124988771975 0.04811704158782959\n",
      "blocks.5.attn.out_proj.weight torch.Size([768, 768]) 1.0988791473209858e-05 0.09377486258745193\n",
      "blocks.5.attn.out_proj.bias torch.Size([768]) -0.0011684768833220005 0.11080432683229446\n",
      "blocks.5.ff.layers.0.weight torch.Size([3072, 768]) -0.004193877335637808 0.1267070472240448\n",
      "blocks.5.ff.layers.0.bias torch.Size([3072]) -0.08502542972564697 0.08900804817676544\n",
      "blocks.5.ff.layers.2.weight torch.Size([768, 3072]) 0.00011598864512052387 0.09735694527626038\n",
      "blocks.5.ff.layers.2.bias torch.Size([768]) 0.0009532846161164343 0.1064532995223999\n",
      "blocks.5.ln1.weight torch.Size([768]) 0.3731194734573364 0.04502682015299797\n",
      "blocks.5.ln1.bias torch.Size([768]) 0.011876348406076431 0.04906607046723366\n",
      "blocks.5.ln2.weight torch.Size([768]) 0.27900201082229614 0.05147692933678627\n",
      "blocks.5.ln2.bias torch.Size([768]) 0.00815197080373764 0.03279997780919075\n",
      "blocks.6.attn.W_Q.weight torch.Size([768, 768]) 0.00019029114628210664 0.13397741317749023\n",
      "blocks.6.attn.W_Q.bias torch.Size([768]) 0.004683490376919508 0.18410006165504456\n",
      "blocks.6.attn.W_K.weight torch.Size([768, 768]) -7.406622171401978e-05 0.12758222222328186\n",
      "blocks.6.attn.W_K.bias torch.Size([768]) 0.0015144060598686337 0.1012103334069252\n",
      "blocks.6.attn.W_V.weight torch.Size([768, 768]) 0.00021866592578589916 0.11854671686887741\n",
      "blocks.6.attn.W_V.bias torch.Size([768]) -0.0007340701413340867 0.035576995462179184\n",
      "blocks.6.attn.out_proj.weight torch.Size([768, 768]) 3.6974248359911144e-05 0.11368992179632187\n",
      "blocks.6.attn.out_proj.bias torch.Size([768]) -0.0004363872576504946 0.10605379194021225\n",
      "blocks.6.ff.layers.0.weight torch.Size([3072, 768]) -0.0028191537130624056 0.12635649740695953\n",
      "blocks.6.ff.layers.0.bias torch.Size([3072]) -0.08570227026939392 0.09056127071380615\n",
      "blocks.6.ff.layers.2.weight torch.Size([768, 3072]) 9.54796705627814e-05 0.10733181238174438\n",
      "blocks.6.ff.layers.2.bias torch.Size([768]) 0.0015628753462806344 0.12105625122785568\n",
      "blocks.6.ln1.weight torch.Size([768]) 0.3455983102321625 0.0441710501909256\n",
      "blocks.6.ln1.bias torch.Size([768]) 0.011822459287941456 0.0662064254283905\n",
      "blocks.6.ln2.weight torch.Size([768]) 0.2594689726829529 0.047400206327438354\n",
      "blocks.6.ln2.bias torch.Size([768]) 0.004331209696829319 0.03382179141044617\n",
      "blocks.7.attn.W_Q.weight torch.Size([768, 768]) -0.00045816207421012223 0.1364603042602539\n",
      "blocks.7.attn.W_Q.bias torch.Size([768]) -0.008822117000818253 0.2147248536348343\n",
      "blocks.7.attn.W_K.weight torch.Size([768, 768]) 0.00013387270155362785 0.13045501708984375\n",
      "blocks.7.attn.W_K.bias torch.Size([768]) -0.0036485891323536634 0.09873808920383453\n",
      "blocks.7.attn.W_V.weight torch.Size([768, 768]) 5.1211449317634106e-05 0.1194656491279602\n",
      "blocks.7.attn.W_V.bias torch.Size([768]) -0.00037803445593453944 0.036869797855615616\n",
      "blocks.7.attn.out_proj.weight torch.Size([768, 768]) 2.3895683625596575e-05 0.11391738802194595\n",
      "blocks.7.attn.out_proj.bias torch.Size([768]) -0.00013363065954763442 0.14512065052986145\n",
      "blocks.7.ff.layers.0.weight torch.Size([3072, 768]) -0.00352298840880394 0.12642338871955872\n",
      "blocks.7.ff.layers.0.bias torch.Size([3072]) -0.08847203105688095 0.09064304083585739\n",
      "blocks.7.ff.layers.2.weight torch.Size([768, 3072]) 8.648945367895067e-05 0.1187349334359169\n",
      "blocks.7.ff.layers.2.bias torch.Size([768]) 0.0011925119906663895 0.12881210446357727\n",
      "blocks.7.ln1.weight torch.Size([768]) 0.3565715253353119 0.04378596320748329\n",
      "blocks.7.ln1.bias torch.Size([768]) 0.01434354204684496 0.05914687365293503\n",
      "blocks.7.ln2.weight torch.Size([768]) 0.2560140788555145 0.04705559089779854\n",
      "blocks.7.ln2.bias torch.Size([768]) 0.009183691814541817 0.04571011662483215\n",
      "blocks.8.attn.W_Q.weight torch.Size([768, 768]) -0.0003004284226335585 0.13009199500083923\n",
      "blocks.8.attn.W_Q.bias torch.Size([768]) -0.013181586749851704 0.20185819268226624\n",
      "blocks.8.attn.W_K.weight torch.Size([768, 768]) 0.00017389189451932907 0.12435027956962585\n",
      "blocks.8.attn.W_K.bias torch.Size([768]) -0.0026047879364341497 0.09828340262174606\n",
      "blocks.8.attn.W_V.weight torch.Size([768, 768]) -0.00038428150583058596 0.12628209590911865\n",
      "blocks.8.attn.W_V.bias torch.Size([768]) -0.0008028498850762844 0.036681145429611206\n",
      "blocks.8.attn.out_proj.weight torch.Size([768, 768]) 8.123623047140427e-06 0.12236364185810089\n",
      "blocks.8.attn.out_proj.bias torch.Size([768]) 0.0010857944143936038 0.14007924497127533\n",
      "blocks.8.ff.layers.0.weight torch.Size([3072, 768]) -0.0020665344782173634 0.12728072702884674\n",
      "blocks.8.ff.layers.0.bias torch.Size([3072]) -0.08505804091691971 0.0935441330075264\n",
      "blocks.8.ff.layers.2.weight torch.Size([768, 3072]) 4.6921471948735416e-05 0.13540396094322205\n",
      "blocks.8.ff.layers.2.bias torch.Size([768]) 0.0011560862185433507 0.12735813856124878\n",
      "blocks.8.ln1.weight torch.Size([768]) 0.3352259397506714 0.044541217386722565\n",
      "blocks.8.ln1.bias torch.Size([768]) 0.013251811265945435 0.06870879977941513\n",
      "blocks.8.ln2.weight torch.Size([768]) 0.2566564977169037 0.04127686098217964\n",
      "blocks.8.ln2.bias torch.Size([768]) 0.000384395505534485 0.049368783831596375\n",
      "blocks.9.attn.W_Q.weight torch.Size([768, 768]) 0.00044967501889914274 0.12296558916568756\n",
      "blocks.9.attn.W_Q.bias torch.Size([768]) 0.008131361566483974 0.22075878083705902\n",
      "blocks.9.attn.W_K.weight torch.Size([768, 768]) -0.0003476667625363916 0.11892230808734894\n",
      "blocks.9.attn.W_K.bias torch.Size([768]) -0.007281634956598282 0.09623975306749344\n",
      "blocks.9.attn.W_V.weight torch.Size([768, 768]) -0.00031157408375293016 0.13612067699432373\n",
      "blocks.9.attn.W_V.bias torch.Size([768]) 6.7658256739377975e-06 0.03432562202215195\n",
      "blocks.9.attn.out_proj.weight torch.Size([768, 768]) -2.7884121664101258e-05 0.13681966066360474\n",
      "blocks.9.attn.out_proj.bias torch.Size([768]) 0.0021340707316994667 0.20949006080627441\n",
      "blocks.9.ff.layers.0.weight torch.Size([3072, 768]) -0.0027408814057707787 0.12761937081813812\n",
      "blocks.9.ff.layers.0.bias torch.Size([3072]) -0.08366744965314865 0.09236326813697815\n",
      "blocks.9.ff.layers.2.weight torch.Size([768, 3072]) 3.4792548831319436e-05 0.1558738499879837\n",
      "blocks.9.ff.layers.2.bias torch.Size([768]) 0.0007137329666875303 0.15918298065662384\n",
      "blocks.9.ln1.weight torch.Size([768]) 0.3575561046600342 0.04693679139018059\n",
      "blocks.9.ln1.bias torch.Size([768]) 0.01590331830084324 0.06386822462081909\n",
      "blocks.9.ln2.weight torch.Size([768]) 0.26497504115104675 0.041519638150930405\n",
      "blocks.9.ln2.bias torch.Size([768]) 0.006400738377124071 0.04571041837334633\n",
      "blocks.10.attn.W_Q.weight torch.Size([768, 768]) 0.00031771004432812333 0.11865982413291931\n",
      "blocks.10.attn.W_Q.bias torch.Size([768]) 0.008911840617656708 0.2265850007534027\n",
      "blocks.10.attn.W_K.weight torch.Size([768, 768]) -0.00012976815924048424 0.11467549949884415\n",
      "blocks.10.attn.W_K.bias torch.Size([768]) -0.002668120665475726 0.09832144528627396\n",
      "blocks.10.attn.W_V.weight torch.Size([768, 768]) 8.956639067037031e-05 0.14459584653377533\n",
      "blocks.10.attn.W_V.bias torch.Size([768]) -0.0014112890930846334 0.04762532189488411\n",
      "blocks.10.attn.out_proj.weight torch.Size([768, 768]) -8.798572821433481e-07 0.14662745594978333\n",
      "blocks.10.attn.out_proj.bias torch.Size([768]) 0.0020241064485162497 0.2322089970111847\n",
      "blocks.10.ff.layers.0.weight torch.Size([3072, 768]) -0.0032080465462058783 0.12764813005924225\n",
      "blocks.10.ff.layers.0.bias torch.Size([3072]) -0.07652498036623001 0.09122857451438904\n",
      "blocks.10.ff.layers.2.weight torch.Size([768, 3072]) 6.105272404965945e-06 0.17814528942108154\n",
      "blocks.10.ff.layers.2.bias torch.Size([768]) 0.0016591990133747458 0.19404050707817078\n",
      "blocks.10.ln1.weight torch.Size([768]) 0.37820783257484436 0.055699631571769714\n",
      "blocks.10.ln1.bias torch.Size([768]) 0.018612800166010857 0.05506131052970886\n",
      "blocks.10.ln2.weight torch.Size([768]) 0.2896941602230072 0.05117756500840187\n",
      "blocks.10.ln2.bias torch.Size([768]) 0.021159043535590172 0.04487457126379013\n",
      "blocks.11.attn.W_Q.weight torch.Size([768, 768]) -1.1366326361894608e-05 0.10982047021389008\n",
      "blocks.11.attn.W_Q.bias torch.Size([768]) 0.0005513962241820991 0.18319359421730042\n",
      "blocks.11.attn.W_K.weight torch.Size([768, 768]) -4.2440758988959715e-05 0.10549236834049225\n",
      "blocks.11.attn.W_K.bias torch.Size([768]) 0.0015315081691369414 0.08636081963777542\n",
      "blocks.11.attn.W_V.weight torch.Size([768, 768]) 0.0002158462448278442 0.16225944459438324\n",
      "blocks.11.attn.W_V.bias torch.Size([768]) 0.00011266752699157223 0.05450312793254852\n",
      "blocks.11.attn.out_proj.weight torch.Size([768, 768]) -5.3778097935719416e-05 0.1819266527891159\n",
      "blocks.11.attn.out_proj.bias torch.Size([768]) -0.021505361422896385 0.46894726157188416\n",
      "blocks.11.ff.layers.0.weight torch.Size([3072, 768]) -0.001846416387706995 0.13000451028347015\n",
      "blocks.11.ff.layers.0.bias torch.Size([3072]) -0.06411425024271011 0.0930408164858818\n",
      "blocks.11.ff.layers.2.weight torch.Size([768, 3072]) -0.00043532054405659437 0.19821906089782715\n",
      "blocks.11.ff.layers.2.bias torch.Size([768]) 0.000971626432146877 0.10824692994356155\n",
      "blocks.11.ln1.weight torch.Size([768]) 0.4786931276321411 0.06516604125499725\n",
      "blocks.11.ln1.bias torch.Size([768]) 0.023284194990992546 0.05674212425947189\n",
      "blocks.11.ln2.weight torch.Size([768]) 0.5041061043739319 0.09001091122627258\n",
      "blocks.11.ln2.bias torch.Size([768]) 0.009192791767418385 0.03916310518980026\n",
      "final_norm.weight torch.Size([768]) 1.5078086853027344 1.3910776376724243\n",
      "final_norm.bias torch.Size([768]) -0.003138466738164425 0.4196469485759735\n",
      "out_head.weight torch.Size([50257, 768]) 0.00037981756031513214 0.14369554817676544\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:49:36.416965Z",
     "start_time": "2025-06-15T03:49:35.406847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import complete_text\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)"
   ],
   "id": "a6c89718f5289428",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of the game, and then the game ends.\n",
      "\n",
      "The game is a\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:26:12.152899Z",
     "start_time": "2025-06-15T08:26:11.323346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "torch.manual_seed(123)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(\"at the start of\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", tensor_to_text(token_ids, tokenizer))"
   ],
   "id": "84204d1eb2f82b26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of an international series of events. You don't have to worry about who has\n"
     ]
    }
   ],
   "execution_count": 177
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Instruction Finetuning",
   "id": "d988216ca3645214"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:16:10.339298Z",
     "start_time": "2025-06-15T07:16:06.573976Z"
    }
   },
   "cell_type": "code",
   "source": "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
   "id": "3a43e589337beead",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-15 15:16:07--  https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 30.100.0.25, 30.100.0.26, 30.100.0.23, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|30.100.0.25|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 22773992 (22M) [text/plain]\r\n",
      "Saving to: alpaca_data.json\r\n",
      "\r\n",
      "alpaca_data.json    100%[===================>]  21.72M  21.9MB/s    in 1.0s    \r\n",
      "\r\n",
      "2025-06-15 15:16:10 (21.9 MB/s) - alpaca_data.json saved [22773992/22773992]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:12:01.601813Z",
     "start_time": "2025-06-15T10:12:01.495906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"alpaca_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "random.seed(123)\n",
    "data = random.sample(data, 1000)\n"
   ],
   "id": "2b051862bc4b6205",
   "outputs": [],
   "execution_count": 242
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:12:08.474168Z",
     "start_time": "2025-06-15T10:12:08.427517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_input(entry):\n",
    "    instruction = entry.get(\"instruction\", \"\").strip()\n",
    "    input_section = entry.get(\"input\", \"\").strip()\n",
    "\n",
    "    parts = [\n",
    "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n",
    "        \"\\n\\n### Instruction:\\n\" + instruction,\n",
    "    ]\n",
    "\n",
    "    if input_section:\n",
    "        parts.append(\"\\n\\n### Input:\\n\" + input_section)\n",
    "\n",
    "    return \"\".join(parts)\n"
   ],
   "id": "d3baaa33104e8709",
   "outputs": [],
   "execution_count": 243
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:12:15.730295Z",
     "start_time": "2025-06-15T10:12:15.681349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ],
   "id": "8e2ea8dda1848a37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a general statement about the importance of empathy.\n",
      "\n",
      "### Response:\n",
      "Empathy is essential for fostering meaningful relationships, deepening understanding between people, and building a more compassionate world.\n"
     ]
    }
   ],
   "execution_count": 245
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:13:17.887566Z",
     "start_time": "2025-06-15T10:13:17.846763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = len(data)\n",
    "train_data = data[:int(n * 0.80)]\n",
    "test_data = data[int(n * 0.80):int(n * 0.90)]\n",
    "val_data = data[int(n * 0.90):]"
   ],
   "id": "2866573989336046",
   "outputs": [],
   "execution_count": 248
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:13:19.772505Z",
     "start_time": "2025-06-15T10:13:19.726844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ],
   "id": "1790ba80fbfdd587",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 800\n",
      "Validation set length: 100\n",
      "Test set length: 100\n"
     ]
    }
   ],
   "execution_count": 249
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:13:23.818324Z",
     "start_time": "2025-06-15T10:13:23.747082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(train_dataset))\n",
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ],
   "id": "dd4c34cccb5194d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n",
      "Train loader:\n",
      "torch.Size([8, 110]) torch.Size([8, 110])\n",
      "torch.Size([8, 146]) torch.Size([8, 146])\n",
      "torch.Size([8, 141]) torch.Size([8, 141])\n",
      "torch.Size([8, 147]) torch.Size([8, 147])\n",
      "torch.Size([8, 107]) torch.Size([8, 107])\n",
      "torch.Size([8, 249]) torch.Size([8, 249])\n",
      "torch.Size([8, 231]) torch.Size([8, 231])\n",
      "torch.Size([8, 199]) torch.Size([8, 199])\n",
      "torch.Size([8, 185]) torch.Size([8, 185])\n",
      "torch.Size([8, 130]) torch.Size([8, 130])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 147]) torch.Size([8, 147])\n",
      "torch.Size([8, 259]) torch.Size([8, 259])\n",
      "torch.Size([8, 109]) torch.Size([8, 109])\n",
      "torch.Size([8, 201]) torch.Size([8, 201])\n",
      "torch.Size([8, 199]) torch.Size([8, 199])\n",
      "torch.Size([8, 201]) torch.Size([8, 201])\n",
      "torch.Size([8, 307]) torch.Size([8, 307])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 191]) torch.Size([8, 191])\n",
      "torch.Size([8, 155]) torch.Size([8, 155])\n",
      "torch.Size([8, 292]) torch.Size([8, 292])\n",
      "torch.Size([8, 111]) torch.Size([8, 111])\n",
      "torch.Size([8, 122]) torch.Size([8, 122])\n",
      "torch.Size([8, 273]) torch.Size([8, 273])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 208]) torch.Size([8, 208])\n",
      "torch.Size([8, 160]) torch.Size([8, 160])\n",
      "torch.Size([8, 326]) torch.Size([8, 326])\n",
      "torch.Size([8, 135]) torch.Size([8, 135])\n",
      "torch.Size([8, 220]) torch.Size([8, 220])\n",
      "torch.Size([8, 200]) torch.Size([8, 200])\n",
      "torch.Size([8, 182]) torch.Size([8, 182])\n",
      "torch.Size([8, 172]) torch.Size([8, 172])\n",
      "torch.Size([8, 131]) torch.Size([8, 131])\n",
      "torch.Size([8, 166]) torch.Size([8, 166])\n",
      "torch.Size([8, 175]) torch.Size([8, 175])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 280]) torch.Size([8, 280])\n",
      "torch.Size([8, 145]) torch.Size([8, 145])\n",
      "torch.Size([8, 113]) torch.Size([8, 113])\n",
      "torch.Size([8, 223]) torch.Size([8, 223])\n",
      "torch.Size([8, 229]) torch.Size([8, 229])\n",
      "torch.Size([8, 310]) torch.Size([8, 310])\n",
      "torch.Size([8, 164]) torch.Size([8, 164])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 203]) torch.Size([8, 203])\n",
      "torch.Size([8, 209]) torch.Size([8, 209])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 184]) torch.Size([8, 184])\n",
      "torch.Size([8, 133]) torch.Size([8, 133])\n",
      "torch.Size([8, 197]) torch.Size([8, 197])\n",
      "torch.Size([8, 213]) torch.Size([8, 213])\n",
      "torch.Size([8, 192]) torch.Size([8, 192])\n",
      "torch.Size([8, 118]) torch.Size([8, 118])\n",
      "torch.Size([8, 177]) torch.Size([8, 177])\n",
      "torch.Size([8, 281]) torch.Size([8, 281])\n",
      "torch.Size([8, 106]) torch.Size([8, 106])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 427]) torch.Size([8, 427])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 115]) torch.Size([8, 115])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 114]) torch.Size([8, 114])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 126]) torch.Size([8, 126])\n",
      "torch.Size([8, 225]) torch.Size([8, 225])\n",
      "torch.Size([8, 164]) torch.Size([8, 164])\n",
      "torch.Size([8, 138]) torch.Size([8, 138])\n",
      "torch.Size([8, 257]) torch.Size([8, 257])\n",
      "torch.Size([8, 342]) torch.Size([8, 342])\n",
      "torch.Size([8, 137]) torch.Size([8, 137])\n",
      "torch.Size([8, 185]) torch.Size([8, 185])\n",
      "torch.Size([8, 265]) torch.Size([8, 265])\n",
      "torch.Size([8, 279]) torch.Size([8, 279])\n",
      "torch.Size([8, 114]) torch.Size([8, 114])\n",
      "torch.Size([8, 168]) torch.Size([8, 168])\n",
      "torch.Size([8, 274]) torch.Size([8, 274])\n",
      "torch.Size([8, 127]) torch.Size([8, 127])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 160]) torch.Size([8, 160])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 132]) torch.Size([8, 132])\n",
      "torch.Size([8, 168]) torch.Size([8, 168])\n",
      "torch.Size([8, 198]) torch.Size([8, 198])\n",
      "torch.Size([8, 233]) torch.Size([8, 233])\n",
      "torch.Size([8, 178]) torch.Size([8, 178])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 182]) torch.Size([8, 182])\n",
      "torch.Size([8, 230]) torch.Size([8, 230])\n",
      "torch.Size([8, 169]) torch.Size([8, 169])\n",
      "torch.Size([8, 426]) torch.Size([8, 426])\n",
      "torch.Size([8, 179]) torch.Size([8, 179])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 190]) torch.Size([8, 190])\n",
      "torch.Size([8, 269]) torch.Size([8, 269])\n",
      "torch.Size([8, 271]) torch.Size([8, 271])\n",
      "torch.Size([8, 206]) torch.Size([8, 206])\n",
      "torch.Size([8, 124]) torch.Size([8, 124])\n",
      "torch.Size([8, 344]) torch.Size([8, 344])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 231]) torch.Size([8, 231])\n",
      "torch.Size([8, 119]) torch.Size([8, 119])\n",
      "torch.Size([8, 126]) torch.Size([8, 126])\n"
     ]
    }
   ],
   "execution_count": 250
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:59:50.829037Z",
     "start_time": "2025-06-15T07:59:50.730241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from functools import partial\n",
    "\n",
    "device = \"cpu\"  # or \"cuda\" if available\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(\n",
    "                format_input(entry) + f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            )\n",
    "            for entry in data\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_texts[idx]\n",
    "\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    max_len = min(\n",
    "        max(len(seq) + 1 for seq in batch),\n",
    "        allowed_max_length or float('inf')\n",
    "    )\n",
    "\n",
    "    input_tensors, label_tensors = [], []\n",
    "\n",
    "    for seq in batch:\n",
    "        seq = seq + [pad_token_id]\n",
    "        padded = seq + [pad_token_id] * (max_len - len(seq))\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1], dtype=torch.long)\n",
    "        labels = torch.tensor(padded[1:], dtype=torch.long)\n",
    "\n",
    "        # Mask padding in labels except the first one\n",
    "        pad_mask = (labels == pad_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(pad_mask) > 1:\n",
    "            labels[pad_mask[1:]] = ignore_index\n",
    "\n",
    "        input_tensors.append(inputs)\n",
    "        label_tensors.append(labels)\n",
    "\n",
    "    return (\n",
    "        torch.stack(input_tensors).to(device),\n",
    "        torch.stack(label_tensors).to(device)\n",
    "    )\n",
    "\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")\n"
   ],
   "id": "7370d205bf9e7f88",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:13:30.892233Z",
     "start_time": "2025-06-15T10:13:30.816075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=True,drop_last=True,num_workers=0)\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=False,drop_last=False,num_workers=0)\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=False,drop_last=False,num_workers=0)"
   ],
   "id": "75e168056c043dbb",
   "outputs": [],
   "execution_count": 251
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:13:35.831101Z",
     "start_time": "2025-06-15T10:13:35.760669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)\n",
    "print(\"inputs: \",inputs[0])\n",
    "print(\"targets: \",targets[0])"
   ],
   "id": "fe4bac319d2bb9a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader:\n",
      "torch.Size([8, 218]) torch.Size([8, 218])\n",
      "torch.Size([8, 110]) torch.Size([8, 110])\n",
      "torch.Size([8, 166]) torch.Size([8, 166])\n",
      "torch.Size([8, 174]) torch.Size([8, 174])\n",
      "torch.Size([8, 135]) torch.Size([8, 135])\n",
      "torch.Size([8, 457]) torch.Size([8, 457])\n",
      "torch.Size([8, 125]) torch.Size([8, 125])\n",
      "torch.Size([8, 119]) torch.Size([8, 119])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 167]) torch.Size([8, 167])\n",
      "torch.Size([8, 169]) torch.Size([8, 169])\n",
      "torch.Size([8, 96]) torch.Size([8, 96])\n",
      "torch.Size([8, 191]) torch.Size([8, 191])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 196]) torch.Size([8, 196])\n",
      "torch.Size([8, 239]) torch.Size([8, 239])\n",
      "torch.Size([8, 147]) torch.Size([8, 147])\n",
      "torch.Size([8, 116]) torch.Size([8, 116])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 210]) torch.Size([8, 210])\n",
      "torch.Size([8, 200]) torch.Size([8, 200])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 219]) torch.Size([8, 219])\n",
      "torch.Size([8, 264]) torch.Size([8, 264])\n",
      "torch.Size([8, 152]) torch.Size([8, 152])\n",
      "torch.Size([8, 242]) torch.Size([8, 242])\n",
      "torch.Size([8, 214]) torch.Size([8, 214])\n",
      "torch.Size([8, 194]) torch.Size([8, 194])\n",
      "torch.Size([8, 177]) torch.Size([8, 177])\n",
      "torch.Size([8, 210]) torch.Size([8, 210])\n",
      "torch.Size([8, 187]) torch.Size([8, 187])\n",
      "torch.Size([8, 302]) torch.Size([8, 302])\n",
      "torch.Size([8, 170]) torch.Size([8, 170])\n",
      "torch.Size([8, 174]) torch.Size([8, 174])\n",
      "torch.Size([8, 242]) torch.Size([8, 242])\n",
      "torch.Size([8, 234]) torch.Size([8, 234])\n",
      "torch.Size([8, 235]) torch.Size([8, 235])\n",
      "torch.Size([8, 145]) torch.Size([8, 145])\n",
      "torch.Size([8, 122]) torch.Size([8, 122])\n",
      "torch.Size([8, 129]) torch.Size([8, 129])\n",
      "torch.Size([8, 228]) torch.Size([8, 228])\n",
      "torch.Size([8, 561]) torch.Size([8, 561])\n",
      "torch.Size([8, 219]) torch.Size([8, 219])\n",
      "torch.Size([8, 151]) torch.Size([8, 151])\n",
      "torch.Size([8, 344]) torch.Size([8, 344])\n",
      "torch.Size([8, 375]) torch.Size([8, 375])\n",
      "torch.Size([8, 326]) torch.Size([8, 326])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 156]) torch.Size([8, 156])\n",
      "torch.Size([8, 149]) torch.Size([8, 149])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 239]) torch.Size([8, 239])\n",
      "torch.Size([8, 172]) torch.Size([8, 172])\n",
      "torch.Size([8, 144]) torch.Size([8, 144])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 262]) torch.Size([8, 262])\n",
      "torch.Size([8, 240]) torch.Size([8, 240])\n",
      "torch.Size([8, 218]) torch.Size([8, 218])\n",
      "torch.Size([8, 159]) torch.Size([8, 159])\n",
      "torch.Size([8, 153]) torch.Size([8, 153])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 152]) torch.Size([8, 152])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 200]) torch.Size([8, 200])\n",
      "torch.Size([8, 243]) torch.Size([8, 243])\n",
      "torch.Size([8, 160]) torch.Size([8, 160])\n",
      "torch.Size([8, 182]) torch.Size([8, 182])\n",
      "torch.Size([8, 167]) torch.Size([8, 167])\n",
      "torch.Size([8, 252]) torch.Size([8, 252])\n",
      "torch.Size([8, 146]) torch.Size([8, 146])\n",
      "torch.Size([8, 155]) torch.Size([8, 155])\n",
      "torch.Size([8, 129]) torch.Size([8, 129])\n",
      "torch.Size([8, 153]) torch.Size([8, 153])\n",
      "torch.Size([8, 143]) torch.Size([8, 143])\n",
      "torch.Size([8, 338]) torch.Size([8, 338])\n",
      "torch.Size([8, 146]) torch.Size([8, 146])\n",
      "torch.Size([8, 210]) torch.Size([8, 210])\n",
      "torch.Size([8, 220]) torch.Size([8, 220])\n",
      "torch.Size([8, 161]) torch.Size([8, 161])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 196]) torch.Size([8, 196])\n",
      "torch.Size([8, 122]) torch.Size([8, 122])\n",
      "torch.Size([8, 205]) torch.Size([8, 205])\n",
      "torch.Size([8, 162]) torch.Size([8, 162])\n",
      "torch.Size([8, 418]) torch.Size([8, 418])\n",
      "torch.Size([8, 144]) torch.Size([8, 144])\n",
      "torch.Size([8, 206]) torch.Size([8, 206])\n",
      "torch.Size([8, 289]) torch.Size([8, 289])\n",
      "torch.Size([8, 254]) torch.Size([8, 254])\n",
      "torch.Size([8, 174]) torch.Size([8, 174])\n",
      "torch.Size([8, 310]) torch.Size([8, 310])\n",
      "torch.Size([8, 420]) torch.Size([8, 420])\n",
      "torch.Size([8, 141]) torch.Size([8, 141])\n",
      "torch.Size([8, 251]) torch.Size([8, 251])\n",
      "torch.Size([8, 150]) torch.Size([8, 150])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 221]) torch.Size([8, 221])\n",
      "torch.Size([8, 223]) torch.Size([8, 223])\n",
      "torch.Size([8, 132]) torch.Size([8, 132])\n",
      "torch.Size([8, 224]) torch.Size([8, 224])\n",
      "inputs:  tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 24564,  4892,   703, 15713,  2568,   460,\n",
      "          307,   973,   284,  1176,  5682,    13,   198,   198, 21017, 18261,\n",
      "           25,   198, 26764,   413,   540,  2568,   318,   257,  3665,   290,\n",
      "        13347,  3038,   329, 43067,  5682,    13,   632, 34547,  3288,  4133,\n",
      "          588, 34488,    11,  2344,    11,   290,  1660,   284,  4439,  8744,\n",
      "          290,   460,   307,   973,   287,   257,  4996,   286,  2842,    13,\n",
      "        12347, 13043,   389,   262,   749,  6768,   973,  3038,   329,  1588,\n",
      "           12,  9888,  1363,  2568,  8136,    11,   290,   389,  6007,   286,\n",
      "        16930,   257,  1363,   290,  9194,  8744,    13,  3086, 35658,   460,\n",
      "         2148,  3424,  2568,  3227,   290,   389,  6481,   973,   287, 12420,\n",
      "         3006,    13, 32116, 31067,  2568, 34547,   262,  3288,  1176,   286,\n",
      "        18180,   284,  7716,  8744,   290,   318,  2968,   287,  3006,   351,\n",
      "        23933,  1660,  4237,    13,  9461,    11,  4903, 49723,  2568,   635,\n",
      "         3544,   262,  4894,   422,   262,  3668,   284,  7716,  2568,   290,\n",
      "          318,  5033,   517,   290,   517,  2968,   287, 12420,  5479,    13,\n",
      "        24199,    11, 15713,  2568,  4394,   257,  3424,   290,  6942,   835,\n",
      "          284,  2222,  1176,   284,  5682,    13, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256])\n",
      "targets:  tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 24564,  4892,   703, 15713,  2568,   460,   307,\n",
      "          973,   284,  1176,  5682,    13,   198,   198, 21017, 18261,    25,\n",
      "          198, 26764,   413,   540,  2568,   318,   257,  3665,   290, 13347,\n",
      "         3038,   329, 43067,  5682,    13,   632, 34547,  3288,  4133,   588,\n",
      "        34488,    11,  2344,    11,   290,  1660,   284,  4439,  8744,   290,\n",
      "          460,   307,   973,   287,   257,  4996,   286,  2842,    13, 12347,\n",
      "        13043,   389,   262,   749,  6768,   973,  3038,   329,  1588,    12,\n",
      "         9888,  1363,  2568,  8136,    11,   290,   389,  6007,   286, 16930,\n",
      "          257,  1363,   290,  9194,  8744,    13,  3086, 35658,   460,  2148,\n",
      "         3424,  2568,  3227,   290,   389,  6481,   973,   287, 12420,  3006,\n",
      "           13, 32116, 31067,  2568, 34547,   262,  3288,  1176,   286, 18180,\n",
      "          284,  7716,  8744,   290,   318,  2968,   287,  3006,   351, 23933,\n",
      "         1660,  4237,    13,  9461,    11,  4903, 49723,  2568,   635,  3544,\n",
      "          262,  4894,   422,   262,  3668,   284,  7716,  2568,   290,   318,\n",
      "         5033,   517,   290,   517,  2968,   287, 12420,  5479,    13, 24199,\n",
      "           11, 15713,  2568,  4394,   257,  3424,   290,  6942,   835,   284,\n",
      "         2222,  1176,   284,  5682,    13, 50256,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "execution_count": 252
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:13:43.190570Z",
     "start_time": "2025-06-15T10:13:41.853872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=1024,\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = tensor_to_text(token_ids, tokenizer)\n",
    "print(generated_text)"
   ],
   "id": "3520f02e1e624290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Analyze the text below, and interpret the message it implies.\n",
      "\n",
      "### Input:\n",
      "The world is increasingly becoming a dangerous place to live.\n",
      "\n",
      "### Response:\n",
      "The text below suggests that the world is increasingly becoming a dangerous place to live.\n"
     ]
    }
   ],
   "execution_count": 253
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:14:03.049337Z",
     "start_time": "2025-06-15T10:13:58.981654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import loss_loader\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "id": "bda02c08bc5dcfa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.8374699354171753\n",
      "Validation loss: 1.7696917057037354\n"
     ]
    }
   ],
   "execution_count": 254
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train as normal",
   "id": "dccf9e269d59d3f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:27:25.701938Z",
     "start_time": "2025-06-15T10:16:21.144031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time\n",
    "from gpt2_v2 import train_model_simple, build_tokenizer\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_num_threads(12)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# FineTune the model\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=format_input(val_data[0]),\n",
    "    tokenizer=build_tokenizer()\n",
    ")\n",
    "\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f\"Training completed in {elapsed:.2f} minutes.\")\n"
   ],
   "id": "a297eb6b0a9efce3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000005): Train loss 1.881, Val loss 1.806, Tokens seen: 6424\n",
      "Ep 1 (Step 000010): Train loss 1.843, Val loss 1.783, Tokens seen: 14624\n",
      "Ep 1 (Step 000015): Train loss 1.842, Val loss 1.784, Tokens seen: 21104\n",
      "Ep 1 (Step 000020): Train loss 1.870, Val loss 1.774, Tokens seen: 28240\n",
      "Ep 1 (Step 000025): Train loss 1.803, Val loss 1.773, Tokens seen: 36288\n",
      "Ep 1 (Step 000030): Train loss 1.593, Val loss 1.763, Tokens seen: 44584\n",
      "Ep 1 (Step 000035): Train loss 1.754, Val loss 1.755, Tokens seen: 53184\n",
      "Ep 1 (Step 000040): Train loss 1.704, Val loss 1.769, Tokens seen: 60104\n",
      "Ep 1 (Step 000045): Train loss 1.851, Val loss 1.754, Tokens seen: 72128\n",
      "Ep 1 (Step 000050): Train loss 1.683, Val loss 1.744, Tokens seen: 81440\n",
      "Ep 1 (Step 000055): Train loss 1.643, Val loss 1.757, Tokens seen: 88864\n",
      "Ep 1 (Step 000060): Train loss 1.712, Val loss 1.750, Tokens seen: 97120\n",
      "Ep 1 (Step 000065): Train loss 1.679, Val loss 1.739, Tokens seen: 104936\n",
      "Ep 1 (Step 000070): Train loss 1.693, Val loss 1.742, Tokens seen: 112192\n",
      "Ep 1 (Step 000075): Train loss 1.722, Val loss 1.735, Tokens seen: 119536\n",
      "Ep 1 (Step 000080): Train loss 1.714, Val loss 1.738, Tokens seen: 126736\n",
      "Ep 1 (Step 000085): Train loss 1.661, Val loss 1.734, Tokens seen: 135560\n",
      "Ep 1 (Step 000090): Train loss 1.568, Val loss 1.732, Tokens seen: 144096\n",
      "Ep 1 (Step 000095): Train loss 1.584, Val loss 1.731, Tokens seen: 154272\n",
      "Ep 1 (Step 000100): Train loss 1.600, Val loss 1.725, Tokens seen: 162040\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Analyze the text below, and interpret the message it implies.\n",
      "\n",
      "### Input:\n",
      "The world is increasingly becoming a dangerous place to live.\n",
      "\n",
      "### Response:\n",
      "The text implies that the world is becoming increasingly dangerous to live in.\n",
      " Checkpoint saved: checkpoints/checkpoint_epoch1.pth\n",
      "Ep 2 (Step 000105): Train loss 1.707, Val loss 1.737, Tokens seen: 168504\n",
      "Ep 2 (Step 000110): Train loss 1.428, Val loss 1.736, Tokens seen: 174832\n",
      "Ep 2 (Step 000115): Train loss 1.494, Val loss 1.758, Tokens seen: 182232\n",
      "Ep 2 (Step 000120): Train loss 1.515, Val loss 1.750, Tokens seen: 188792\n",
      "Ep 2 (Step 000125): Train loss 1.446, Val loss 1.732, Tokens seen: 197440\n",
      "Ep 2 (Step 000130): Train loss 1.576, Val loss 1.750, Tokens seen: 205488\n",
      "Ep 2 (Step 000135): Train loss 1.559, Val loss 1.730, Tokens seen: 213760\n",
      "Ep 2 (Step 000140): Train loss 1.468, Val loss 1.739, Tokens seen: 222032\n",
      "Ep 2 (Step 000145): Train loss 1.548, Val loss 1.740, Tokens seen: 230560\n",
      "Ep 2 (Step 000150): Train loss 1.471, Val loss 1.741, Tokens seen: 240576\n",
      "Ep 2 (Step 000155): Train loss 1.384, Val loss 1.744, Tokens seen: 248984\n",
      "Ep 2 (Step 000160): Train loss 1.529, Val loss 1.725, Tokens seen: 256312\n",
      "Ep 2 (Step 000165): Train loss 1.485, Val loss 1.742, Tokens seen: 264560\n",
      "Ep 2 (Step 000170): Train loss 1.542, Val loss 1.732, Tokens seen: 271744\n",
      "Ep 2 (Step 000175): Train loss 1.402, Val loss 1.727, Tokens seen: 280280\n",
      "Ep 2 (Step 000180): Train loss 1.469, Val loss 1.736, Tokens seen: 286704\n",
      "Ep 2 (Step 000185): Train loss 1.410, Val loss 1.734, Tokens seen: 297816\n",
      "Ep 2 (Step 000190): Train loss 1.373, Val loss 1.722, Tokens seen: 303968\n",
      "Ep 2 (Step 000195): Train loss 1.442, Val loss 1.732, Tokens seen: 313272\n",
      "Ep 2 (Step 000200): Train loss 1.443, Val loss 1.732, Tokens seen: 320096\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Analyze the text below, and interpret the message it implies.\n",
      "\n",
      "### Input:\n",
      "The world is increasingly becoming a dangerous place to live.\n",
      "\n",
      "### Response:\n",
      "The text implies that the world is becoming increasingly dangerous due to the increasing\n",
      " Checkpoint saved: checkpoints/checkpoint_epoch2.pth\n",
      " Training complete. Final model saved: checkpoints/final_model.pth\n",
      "Training completed in 11.07 minutes.\n"
     ]
    }
   ],
   "execution_count": 256
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:52:16.905800Z",
     "start_time": "2025-06-15T10:52:16.685363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ],
   "id": "fe49e1158904c239",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 257
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:26:48.056964Z",
     "start_time": "2025-06-15T09:26:46.378604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[1])\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=1024,\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = tensor_to_text(token_ids, tokenizer)\n",
    "print(generated_text)"
   ],
   "id": "1818eb1a3d9ca2c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Propose a suitable title for the below article\n",
      "\n",
      "### Input:\n",
      "This article discusses the current state of the music industry and how technology has had an impact on its evolution.\n",
      "\n",
      "### Response:\n",
      "The current state of the music industry and how technology has had an impact on its evolution is described in the article.\n"
     ]
    }
   ],
   "execution_count": 221
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save model",
   "id": "ffe50f3c0d3864f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:34:40.857986Z",
     "start_time": "2025-06-15T09:32:36.904855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def generate_response(entry, model):\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=text_to_tensor(input_text, tokenizer),\n",
    "        max_new_tokens=35,\n",
    "        context_size=1024,\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    generated_text = tensor_to_text(token_ids, tokenizer)\n",
    "    response = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "    return response\n",
    "\n",
    "# Generate and attach responses\n",
    "for entry in tqdm(test_data, desc=\"Generating responses\"):\n",
    "    entry[\"model_response\"] = generate_response(entry, model)\n",
    "\n",
    "# Save to file\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as f:\n",
    "    json.dump(test_data, f, indent=4)\n"
   ],
   "id": "97b67f7b32b44749",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|| 100/100 [02:03<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:38:54.211213Z",
     "start_time": "2025-06-15T09:38:54.163078Z"
    }
   },
   "cell_type": "code",
   "source": "print(test_data[0])",
   "id": "3f9cb2c8c0a22d6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Design a logo for a child care center.', 'input': '', 'output': 'The logo should be warm, playful and inviting. It should utilize a font that is easy to read and appealing to children. I would suggest a combination of vibrant colors, such as yellow, blue and green. It could include an image of a happy kid or an illustration of a sun and clouds. The logo should also convey a sense of security and safety, as the center is aiming to provide a safe, nurturing environment for kids.', 'model_response': 'The logo should be simple and simple, with a simple design and a simple message. It should be simple and simple, with a simple message and'}\n"
     ]
    }
   ],
   "execution_count": 228
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:53:11.423681Z",
     "start_time": "2025-06-15T10:53:09.558519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_name = \"gpt2-124M-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ],
   "id": "2e5f0b4a2aae83e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-124M-sft.pth\n"
     ]
    }
   ],
   "execution_count": 258
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate the model",
   "id": "2d16eb445d3fd5da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:53:25.104083Z",
     "start_time": "2025-06-15T10:53:25.047853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import psutil\n",
    "\n",
    "def is_process_running(name_substr: str) -> bool:\n",
    "    \"\"\"Check if any running process contains the given substring in its name.\"\"\"\n",
    "    return any(name_substr.lower() in (proc.info[\"name\"] or \"\").lower()\n",
    "               for proc in psutil.process_iter([\"name\"]))\n",
    "\n",
    "if not is_process_running(\"ollama\"):\n",
    "    raise RuntimeError(\" Ollama not running. Please launch it before proceeding.\")\n",
    "\n",
    "print(\" Ollama is running.\")\n"
   ],
   "id": "56c837af437f0965",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ollama is running.\n"
     ]
    }
   ],
   "execution_count": 262
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:49:32.572991Z",
     "start_time": "2025-06-15T09:49:32.480140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "    prompt: str,\n",
    "    model: str = \"llama3\",\n",
    "    url: str = \"http://localhost:11434/api/chat\",\n",
    "    seed: int = 123,\n",
    "    temperature: float = 0.0,\n",
    "    num_ctx: int = 2048\n",
    ") -> str:\n",
    "    \"\"\"Send a prompt to a local chat model and return the generated response.\"\"\"\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"options\": {\n",
    "            \"seed\": seed,\n",
    "            \"temperature\": temperature,\n",
    "            \"num_ctx\": num_ctx\n",
    "        }\n",
    "    }\n",
    "\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=json.dumps(data).encode(\"utf-8\"),\n",
    "        method=\"POST\",\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    response_text = []\n",
    "\n",
    "    try:\n",
    "        with urllib.request.urlopen(request) as response:\n",
    "            for line in response:\n",
    "                line = line.decode(\"utf-8\").strip()\n",
    "                if line:\n",
    "                    message_chunk = json.loads(line)\n",
    "                    content = message_chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    response_text.append(content)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to query model: {e}\")\n",
    "\n",
    "    return \"\".join(response_text)\n"
   ],
   "id": "576103fbe032757",
   "outputs": [],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:50:17.141779Z",
     "start_time": "2025-06-15T09:49:59.651359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = query_model(\"What do Llamas eat?\", \"llama3\")\n",
    "print(result)"
   ],
   "id": "8034456625eb72ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:55:33.008610Z",
     "start_time": "2025-06-15T09:55:32.912473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_model_scores(data, response_key=\"model_response\", model=\"llama3\"):\n",
    "    \"\"\"Generate integer scores (0100) for model responses using LLM evaluation.\"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for entry in tqdm(data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            \"Given the input below, the correct output, and the model's response, \"\n",
    "            \"score the model's response on a scale from 0 to 100, where 100 is the best.\\n\\n\"\n",
    "            f\"### Input:\\n{format_input(entry)}\\n\\n\"\n",
    "            f\"### Expected Output:\\n{entry['output']}\\n\\n\"\n",
    "            f\"### Model Response:\\n{entry.get(response_key, '').strip()}\\n\\n\"\n",
    "            \"### Respond with the integer number only.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score_str = query_model(prompt, model=model).strip()\n",
    "            score = int(score_str)\n",
    "            scores.append(score)\n",
    "        except ValueError:\n",
    "            print(f\"[Warning] Invalid score format: {score_str!r}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Scoring failed for entry: {e}\")\n",
    "\n",
    "    return scores\n"
   ],
   "id": "6ef00a7087af6e6c",
   "outputs": [],
   "execution_count": 238
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T10:54:55.889631Z",
     "start_time": "2025-06-15T10:53:40.967497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scores = generate_model_scores(test_data)\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average: {sum(scores)/len(scores):.2f}, Max: {max(scores)}, Min: {min(scores)}\")"
   ],
   "id": "a437d382509a0058",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:   6%|         | 6/100 [00:12<02:31,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: 'I\\'d score the model\\'s response as 0 out of 100. The reason is that the model did not provide a complete and accurate response. It simply left a blank space without providing any information about the United Nations or its establishment date.\\n\\nA good response would have included the correct answer, which is \"The United Nations was established in 1945, 75 years ago.\"'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:   8%|         | 8/100 [00:14<01:49,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0 out of 100, since it didn't provide any output at all!\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  10%|         | 10/100 [00:16<01:58,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response a 0 out of 100.\\n\\nThe model didn't provide any output at all, which is not acceptable for this task. The expected output includes specific categories and amounts that are relevant to the input expenses, but the model failed to generate anything resembling the expected output.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  15%|        | 15/100 [00:21<02:02,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0 out of 100. Here's why:\\n\\n* The instruction asks for a specific answer, but the model simply leaves the response blank.\\n* The expected output provides a clear and concise answer to the question, which is not reflected in the model's response.\\n\\nTo achieve a higher score, the model should provide an accurate and relevant response that completes the request. In this case, the model failed to do so, resulting in a score of 0.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  20%|        | 20/100 [00:25<01:23,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0/100. The reason is that the model did not provide any response at all, whereas the task was to explain why the output is not accurate (which implies that there should be some kind of analysis or critique).\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  21%|        | 21/100 [00:27<01:50,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: 'I\\'d score the model\\'s response as 0/100.\\n\\nThe reason is that the model didn\\'t provide any response at all, which means it failed to complete the task described in the instruction. The expected output was clearly specified as \"7, 9\", but the model didn\\'t even attempt to generate a response.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  26%|       | 26/100 [00:30<01:10,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0/100 because it didn't provide any output at all! A complete response would be expected to include the type of quadrilateral formed by the given points, which is a rectangle in this case.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  27%|       | 27/100 [00:32<01:33,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0 out of 100.\\n\\nThe reason is that the model didn't provide any output at all, whereas the expected output was a specific area code (51) for the given ZIP code (74321). The model failed to complete the task and provide an answer.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  33%|      | 33/100 [00:36<01:02,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0/100, since it doesn't provide a data visualization at all. The expected output is a simple bar chart showing the temperature of three cities, but the model simply left the space blank.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  40%|      | 40/100 [00:41<01:11,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I would score the model's response as 20 out of 100.\\n\\nThe model failed to provide a chart or any relevant information about the employment rate in the US from 2019 to 2020, which is the main requirement of the task. The expected output clearly shows a table with the employment rates for each year, but the model's response does not meet this expectation at all.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  67%|   | 67/100 [00:53<00:29,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: 'I\\'d score the model\\'s response a 0 out of 100 because it doesn\\'t provide any information about the BPM of the song \"Fading Away\" by Jaws of Love, which is the expected output. The model simply leaves the response blank, failing to complete the task as instructed.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  76%|  | 76/100 [01:00<00:27,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0 out of 100. The reason is that the model did not provide any response at all, which means it failed to complete the task as instructed. A correct response would have calculated the area of the circle using the formula A = r and provided the result, which is 12.57m2 in this case.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  83%| | 83/100 [01:04<00:15,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0 out of 100. The expected output is a clear and concise answer that provides both possibilities for generating random numbers using a computer, whereas the model's response is blank, indicating no attempt to provide an answer.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  86%| | 86/100 [01:08<00:19,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0 out of 100. The reason is that the model simply copied the current trading price, which doesn't take into account any potential changes or fluctuations in the stock market throughout the day. A good predictive model should consider various factors and make an educated estimate based on historical data and trends. In this case, the model's response lacks any meaningful analysis or prediction, making it a poor attempt at completing the task.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  87%| | 87/100 [01:09<00:16,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0 out of 100, since it didn't provide any output at all!\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|| 100/100 [01:14<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 85 of 100\n",
      "Average: 56.62, Max: 85, Min: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 263
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
