{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-tune LLM to follow instructions\n",
   "id": "b6e02cf5201e2731"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load open weights",
   "id": "91e1b02b97bac08c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:18:04.305648Z",
     "start_time": "2025-06-15T02:18:03.546261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import GPT2Model, GPT_CONFIG_124M, complete_text, generate_text_simple, tensor_to_text, text_to_tensor\n",
    "\n",
    "GPT_CONFIG_124M.update({\"qkv_bias\": True})\n",
    "model = GPT2Model(GPT_CONFIG_124M)\n"
   ],
   "id": "423a8cab44a48e29",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:11:56.833964Z",
     "start_time": "2025-06-15T02:11:56.819066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ],
   "id": "891e084dd7eeb90a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768])\n",
      "pos_emb.weight torch.Size([1024, 768])\n",
      "blocks.0.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_Q.bias torch.Size([768])\n",
      "blocks.0.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_K.bias torch.Size([768])\n",
      "blocks.0.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_V.bias torch.Size([768])\n",
      "blocks.0.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.0.attn.out_proj.bias torch.Size([768])\n",
      "blocks.0.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.0.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.0.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.0.ff.layers.2.bias torch.Size([768])\n",
      "blocks.0.ln1.weight torch.Size([768])\n",
      "blocks.0.ln1.bias torch.Size([768])\n",
      "blocks.0.ln2.weight torch.Size([768])\n",
      "blocks.0.ln2.bias torch.Size([768])\n",
      "blocks.1.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_Q.bias torch.Size([768])\n",
      "blocks.1.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_K.bias torch.Size([768])\n",
      "blocks.1.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_V.bias torch.Size([768])\n",
      "blocks.1.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.1.attn.out_proj.bias torch.Size([768])\n",
      "blocks.1.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.1.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.1.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.1.ff.layers.2.bias torch.Size([768])\n",
      "blocks.1.ln1.weight torch.Size([768])\n",
      "blocks.1.ln1.bias torch.Size([768])\n",
      "blocks.1.ln2.weight torch.Size([768])\n",
      "blocks.1.ln2.bias torch.Size([768])\n",
      "blocks.2.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_Q.bias torch.Size([768])\n",
      "blocks.2.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_K.bias torch.Size([768])\n",
      "blocks.2.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_V.bias torch.Size([768])\n",
      "blocks.2.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.2.attn.out_proj.bias torch.Size([768])\n",
      "blocks.2.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.2.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.2.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.2.ff.layers.2.bias torch.Size([768])\n",
      "blocks.2.ln1.weight torch.Size([768])\n",
      "blocks.2.ln1.bias torch.Size([768])\n",
      "blocks.2.ln2.weight torch.Size([768])\n",
      "blocks.2.ln2.bias torch.Size([768])\n",
      "blocks.3.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_Q.bias torch.Size([768])\n",
      "blocks.3.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_K.bias torch.Size([768])\n",
      "blocks.3.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_V.bias torch.Size([768])\n",
      "blocks.3.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.3.attn.out_proj.bias torch.Size([768])\n",
      "blocks.3.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.3.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.3.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.3.ff.layers.2.bias torch.Size([768])\n",
      "blocks.3.ln1.weight torch.Size([768])\n",
      "blocks.3.ln1.bias torch.Size([768])\n",
      "blocks.3.ln2.weight torch.Size([768])\n",
      "blocks.3.ln2.bias torch.Size([768])\n",
      "blocks.4.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_Q.bias torch.Size([768])\n",
      "blocks.4.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_K.bias torch.Size([768])\n",
      "blocks.4.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_V.bias torch.Size([768])\n",
      "blocks.4.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.4.attn.out_proj.bias torch.Size([768])\n",
      "blocks.4.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.4.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.4.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.4.ff.layers.2.bias torch.Size([768])\n",
      "blocks.4.ln1.weight torch.Size([768])\n",
      "blocks.4.ln1.bias torch.Size([768])\n",
      "blocks.4.ln2.weight torch.Size([768])\n",
      "blocks.4.ln2.bias torch.Size([768])\n",
      "blocks.5.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_Q.bias torch.Size([768])\n",
      "blocks.5.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_K.bias torch.Size([768])\n",
      "blocks.5.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_V.bias torch.Size([768])\n",
      "blocks.5.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.5.attn.out_proj.bias torch.Size([768])\n",
      "blocks.5.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.5.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.5.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.5.ff.layers.2.bias torch.Size([768])\n",
      "blocks.5.ln1.weight torch.Size([768])\n",
      "blocks.5.ln1.bias torch.Size([768])\n",
      "blocks.5.ln2.weight torch.Size([768])\n",
      "blocks.5.ln2.bias torch.Size([768])\n",
      "blocks.6.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_Q.bias torch.Size([768])\n",
      "blocks.6.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_K.bias torch.Size([768])\n",
      "blocks.6.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_V.bias torch.Size([768])\n",
      "blocks.6.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.6.attn.out_proj.bias torch.Size([768])\n",
      "blocks.6.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.6.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.6.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.6.ff.layers.2.bias torch.Size([768])\n",
      "blocks.6.ln1.weight torch.Size([768])\n",
      "blocks.6.ln1.bias torch.Size([768])\n",
      "blocks.6.ln2.weight torch.Size([768])\n",
      "blocks.6.ln2.bias torch.Size([768])\n",
      "blocks.7.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_Q.bias torch.Size([768])\n",
      "blocks.7.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_K.bias torch.Size([768])\n",
      "blocks.7.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_V.bias torch.Size([768])\n",
      "blocks.7.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.7.attn.out_proj.bias torch.Size([768])\n",
      "blocks.7.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.7.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.7.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.7.ff.layers.2.bias torch.Size([768])\n",
      "blocks.7.ln1.weight torch.Size([768])\n",
      "blocks.7.ln1.bias torch.Size([768])\n",
      "blocks.7.ln2.weight torch.Size([768])\n",
      "blocks.7.ln2.bias torch.Size([768])\n",
      "blocks.8.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_Q.bias torch.Size([768])\n",
      "blocks.8.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_K.bias torch.Size([768])\n",
      "blocks.8.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_V.bias torch.Size([768])\n",
      "blocks.8.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.8.attn.out_proj.bias torch.Size([768])\n",
      "blocks.8.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.8.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.8.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.8.ff.layers.2.bias torch.Size([768])\n",
      "blocks.8.ln1.weight torch.Size([768])\n",
      "blocks.8.ln1.bias torch.Size([768])\n",
      "blocks.8.ln2.weight torch.Size([768])\n",
      "blocks.8.ln2.bias torch.Size([768])\n",
      "blocks.9.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_Q.bias torch.Size([768])\n",
      "blocks.9.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_K.bias torch.Size([768])\n",
      "blocks.9.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_V.bias torch.Size([768])\n",
      "blocks.9.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.9.attn.out_proj.bias torch.Size([768])\n",
      "blocks.9.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.9.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.9.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.9.ff.layers.2.bias torch.Size([768])\n",
      "blocks.9.ln1.weight torch.Size([768])\n",
      "blocks.9.ln1.bias torch.Size([768])\n",
      "blocks.9.ln2.weight torch.Size([768])\n",
      "blocks.9.ln2.bias torch.Size([768])\n",
      "blocks.10.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_Q.bias torch.Size([768])\n",
      "blocks.10.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_K.bias torch.Size([768])\n",
      "blocks.10.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_V.bias torch.Size([768])\n",
      "blocks.10.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.10.attn.out_proj.bias torch.Size([768])\n",
      "blocks.10.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.10.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.10.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.10.ff.layers.2.bias torch.Size([768])\n",
      "blocks.10.ln1.weight torch.Size([768])\n",
      "blocks.10.ln1.bias torch.Size([768])\n",
      "blocks.10.ln2.weight torch.Size([768])\n",
      "blocks.10.ln2.bias torch.Size([768])\n",
      "blocks.11.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_Q.bias torch.Size([768])\n",
      "blocks.11.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_K.bias torch.Size([768])\n",
      "blocks.11.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_V.bias torch.Size([768])\n",
      "blocks.11.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.11.attn.out_proj.bias torch.Size([768])\n",
      "blocks.11.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.11.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.11.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.11.ff.layers.2.bias torch.Size([768])\n",
      "blocks.11.ln1.weight torch.Size([768])\n",
      "blocks.11.ln1.bias torch.Size([768])\n",
      "blocks.11.ln2.weight torch.Size([768])\n",
      "blocks.11.ln2.bias torch.Size([768])\n",
      "final_norm.weight torch.Size([768])\n",
      "final_norm.bias torch.Size([768])\n",
      "out_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:18:11.032464Z",
     "start_time": "2025-06-15T02:18:10.287023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)"
   ],
   "id": "49648386cc30dc35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start ofYears Gadget bookuggisheffectsdozenispers POW sang Nikolonic Springfield morbid signallingug\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download GPT2 from OpenAI",
   "id": "2ba0d473bfd76ff9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:15:17.049393Z",
     "start_time": "2025-06-15T02:15:17.036015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def download_file(url, destination, backup_url=None):\n",
    "    def _attempt_download(download_url):\n",
    "        with urllib.request.urlopen(download_url) as response:\n",
    "            total_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "            if os.path.exists(destination) and os.path.getsize(destination) == total_size:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return True\n",
    "\n",
    "            with tqdm(total=total_size, unit=\"iB\", unit_scale=True, desc=os.path.basename(download_url)) as pbar, \\\n",
    "                 open(destination, \"wb\") as f:\n",
    "                for chunk in iter(lambda: response.read(1024), b\"\"):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "            return True\n",
    "\n",
    "    try:\n",
    "        if _attempt_download(url):\n",
    "            return\n",
    "    except (urllib.error.HTTPError, urllib.error.URLError):\n",
    "        if backup_url:\n",
    "            print(f\"Primary URL failed. Trying backup URL: {backup_url}\")\n",
    "            try:\n",
    "                if _attempt_download(backup_url):\n",
    "                    return\n",
    "            except (urllib.error.HTTPError, urllib.error.URLError):\n",
    "                pass\n",
    "        print(f\"Failed to download from primary URL ({url})\"\n",
    "              + (f\" and backup URL ({backup_url})\" if backup_url else \"\") + \".\\n\"\n",
    "              \"Check your internet connection or the file availability.\\n\"\n",
    "              \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    allowed_sizes = {\"124M\", \"355M\", \"774M\", \"1558M\"}\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size must be one of {allowed_sizes}\")\n",
    "\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    base_url = f\"https://openaipublic.blob.core.windows.net/gpt-2/models/{model_size}/\"\n",
    "    backup_url = f\"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2/{model_size}/\"\n",
    "\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    for fname in filenames:\n",
    "        dst = os.path.join(model_dir, fname)\n",
    "        if os.path.exists(dst):\n",
    "            print(f\"Already exists: {fname}, skipping download.\")\n",
    "            continue\n",
    "        primary = urljoin(base_url, fname)\n",
    "        backup = urljoin(backup_url, fname)\n",
    "        print(f\"Downloading {fname} ...\")\n",
    "        download_file(primary, dst, backup)\n",
    "\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    with open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        settings = json.load(f)\n",
    "\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "    return settings, params\n"
   ],
   "id": "f29d5d3c3d0402a0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:18:19.601402Z",
     "start_time": "2025-06-15T02:18:19.176612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Params:\", params.keys())"
   ],
   "id": "28d0828f52dfce6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: checkpoint, skipping download.\n",
      "Already exists: encoder.json, skipping download.\n",
      "Already exists: hparams.json, skipping download.\n",
      "Already exists: model.ckpt.data-00000-of-00001, skipping download.\n",
      "Already exists: model.ckpt.index, skipping download.\n",
      "Already exists: model.ckpt.meta, skipping download.\n",
      "Already exists: vocab.bpe, skipping download.\n",
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Params: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:18:51.810618Z",
     "start_time": "2025-06-15T02:18:51.784421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def assign_(left, right):\n",
    "    if right is None:\n",
    "        raise ValueError(\"'right' cannot be None\")\n",
    "    right_tensor = torch.as_tensor(right, dtype=left.dtype, device=left.device)\n",
    "    if right_tensor.numel() == 0:\n",
    "        raise ValueError(\"'right' cannot be Empty\")\n",
    "    if left.shape != right_tensor.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {left.shape} vs {right_tensor.shape}\")\n",
    "    with torch.no_grad():\n",
    "        left.copy_(right_tensor)\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    assign_(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    assign_(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b, (block, pblock) in enumerate(zip(gpt.blocks, params[\"blocks\"])):\n",
    "        # Attention QKV\n",
    "        qw, kw, vw = np.split(pblock[\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
    "        qb, kb, vb = np.split(pblock[\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
    "        assign_(block.attn.W_Q.weight, qw.T)\n",
    "        assign_(block.attn.W_K.weight, kw.T)\n",
    "        assign_(block.attn.W_V.weight, vw.T)\n",
    "        assign_(block.attn.W_Q.bias, qb)\n",
    "        assign_(block.attn.W_K.bias, kb)\n",
    "        assign_(block.attn.W_V.bias, vb)\n",
    "\n",
    "        # Attention output projection\n",
    "        assign_(block.attn.out_proj.weight, pblock[\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        assign_(block.attn.out_proj.bias,   pblock[\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # Feedforward\n",
    "        assign_(block.ff.layers[0].weight, pblock[\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        assign_(block.ff.layers[0].bias,   pblock[\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        assign_(block.ff.layers[2].weight, pblock[\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        assign_(block.ff.layers[2].bias,   pblock[\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # LayerNorms\n",
    "        assign_(block.ln1.weight, pblock[\"ln_1\"][\"g\"])\n",
    "        assign_(block.ln1.bias, pblock[\"ln_1\"][\"b\"])\n",
    "        assign_(block.ln2.weight, pblock[\"ln_2\"][\"g\"])\n",
    "        assign_(block.ln2.bias, pblock[\"ln_2\"][\"b\"])\n",
    "\n",
    "    assign_(gpt.final_norm.weight, params[\"g\"])\n",
    "    assign_(gpt.final_norm.bias, params[\"b\"])\n",
    "    assign_(gpt.out_head.weight,  params[\"wte\"])\n"
   ],
   "id": "e83e9805c9622997",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:23:55.329952Z",
     "start_time": "2025-06-15T02:23:54.964856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_weights_into_gpt(model, params)\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n"
   ],
   "id": "7696090694c7c0c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:23:59.896307Z",
     "start_time": "2025-06-15T02:23:59.824877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.mean().item(), param.std().item())\n"
   ],
   "id": "6ae4be9bb4b68198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768]) 0.00037981756031513214 0.14369554817676544\n",
      "pos_emb.weight torch.Size([1024, 768]) -0.0006787165184505284 0.1226913258433342\n",
      "blocks.0.attn.W_Q.weight torch.Size([768, 768]) 0.00015374351642094553 0.2386905699968338\n",
      "blocks.0.attn.W_Q.bias torch.Size([768]) -0.007821076549589634 0.3427544832229614\n",
      "blocks.0.attn.W_K.weight torch.Size([768, 768]) 1.2351122677500825e-05 0.2432965785264969\n",
      "blocks.0.attn.W_K.bias torch.Size([768]) 0.0048723239451646805 0.18297071754932404\n",
      "blocks.0.attn.W_V.weight torch.Size([768, 768]) -5.968316145299468e-06 0.05811797454953194\n",
      "blocks.0.attn.W_V.bias torch.Size([768]) 0.0008267878438346088 0.04772818833589554\n",
      "blocks.0.attn.out_proj.weight torch.Size([768, 768]) -0.0001613790518604219 0.1474614143371582\n",
      "blocks.0.attn.out_proj.bias torch.Size([768]) -0.00691022165119648 0.2589662969112396\n",
      "blocks.0.ff.layers.0.weight torch.Size([3072, 768]) -0.0007485305541194975 0.14116929471492767\n",
      "blocks.0.ff.layers.0.bias torch.Size([3072]) -0.0931621864438057 0.13235801458358765\n",
      "blocks.0.ff.layers.2.weight torch.Size([768, 3072]) 8.009047633095179e-06 0.0879654809832573\n",
      "blocks.0.ff.layers.2.bias torch.Size([768]) -0.0004230523481965065 0.10169976204633713\n",
      "blocks.0.ln1.weight torch.Size([768]) 0.18035894632339478 0.04131494462490082\n",
      "blocks.0.ln1.bias torch.Size([768]) -0.006593453232198954 0.03580174222588539\n",
      "blocks.0.ln2.weight torch.Size([768]) 0.8678296208381653 0.48494789004325867\n",
      "blocks.0.ln2.bias torch.Size([768]) 0.00920353177934885 0.07009701430797577\n",
      "blocks.1.attn.W_Q.weight torch.Size([768, 768]) -0.00015265395632013679 0.15103811025619507\n",
      "blocks.1.attn.W_Q.bias torch.Size([768]) 0.001711765886284411 0.3485022187232971\n",
      "blocks.1.attn.W_K.weight torch.Size([768, 768]) 0.00023460258671548218 0.15902258455753326\n",
      "blocks.1.attn.W_K.bias torch.Size([768]) 0.0008940294501371682 0.08274678885936737\n",
      "blocks.1.attn.W_V.weight torch.Size([768, 768]) 2.3152324502007104e-06 0.10367371886968613\n",
      "blocks.1.attn.W_V.bias torch.Size([768]) -0.00020504526037257165 0.07381219416856766\n",
      "blocks.1.attn.out_proj.weight torch.Size([768, 768]) -8.275873551610857e-05 0.10191785544157028\n",
      "blocks.1.attn.out_proj.bias torch.Size([768]) -0.0010727141052484512 0.104808010160923\n",
      "blocks.1.ff.layers.0.weight torch.Size([3072, 768]) 0.0006420306744985282 0.1307205706834793\n",
      "blocks.1.ff.layers.0.bias torch.Size([3072]) -0.07219841331243515 0.09491100162267685\n",
      "blocks.1.ff.layers.2.weight torch.Size([768, 3072]) 9.794025390874594e-05 0.08719106763601303\n",
      "blocks.1.ff.layers.2.bias torch.Size([768]) 0.0002505724842194468 0.10042908787727356\n",
      "blocks.1.ln1.weight torch.Size([768]) 0.22284089028835297 0.05130758509039879\n",
      "blocks.1.ln1.bias torch.Size([768]) -0.0050234864465892315 0.052436452358961105\n",
      "blocks.1.ln2.weight torch.Size([768]) 0.24269406497478485 0.03164536505937576\n",
      "blocks.1.ln2.bias torch.Size([768]) -0.004073990508913994 0.03919508308172226\n",
      "blocks.2.attn.W_Q.weight torch.Size([768, 768]) 0.0001179432583739981 0.18932299315929413\n",
      "blocks.2.attn.W_Q.bias torch.Size([768]) -0.006064689252525568 0.2589069902896881\n",
      "blocks.2.attn.W_K.weight torch.Size([768, 768]) -4.986231942893937e-05 0.15181933343410492\n",
      "blocks.2.attn.W_K.bias torch.Size([768]) -0.003441582666710019 0.10127473622560501\n",
      "blocks.2.attn.W_V.weight torch.Size([768, 768]) 0.00017838712665252388 0.10502831637859344\n",
      "blocks.2.attn.W_V.bias torch.Size([768]) -0.0021237407345324755 0.0655340775847435\n",
      "blocks.2.attn.out_proj.weight torch.Size([768, 768]) -3.299467061879113e-05 0.08103547245264053\n",
      "blocks.2.attn.out_proj.bias torch.Size([768]) 0.003375591477379203 0.1451117843389511\n",
      "blocks.2.ff.layers.0.weight torch.Size([3072, 768]) -0.005061259493231773 0.13352689146995544\n",
      "blocks.2.ff.layers.0.bias torch.Size([3072]) -0.0928223729133606 0.1066708043217659\n",
      "blocks.2.ff.layers.2.weight torch.Size([768, 3072]) 0.00019714866357389838 0.09308750927448273\n",
      "blocks.2.ff.layers.2.bias torch.Size([768]) 0.002819357207044959 0.11242914199829102\n",
      "blocks.2.ln1.weight torch.Size([768]) 0.24077002704143524 0.07527267932891846\n",
      "blocks.2.ln1.bias torch.Size([768]) -0.00036007841117680073 0.07055915892124176\n",
      "blocks.2.ln2.weight torch.Size([768]) 0.2925874888896942 0.04540559649467468\n",
      "blocks.2.ln2.bias torch.Size([768]) 0.006347864866256714 0.043932873755693436\n",
      "blocks.3.attn.W_Q.weight torch.Size([768, 768]) -0.00021896824182476848 0.16474692523479462\n",
      "blocks.3.attn.W_Q.bias torch.Size([768]) -0.0038416890893131495 0.2126476913690567\n",
      "blocks.3.attn.W_K.weight torch.Size([768, 768]) -5.696068546967581e-05 0.1537768691778183\n",
      "blocks.3.attn.W_K.bias torch.Size([768]) -0.0004758095892611891 0.10474759340286255\n",
      "blocks.3.attn.W_V.weight torch.Size([768, 768]) 0.00020247689099051058 0.09766855090856552\n",
      "blocks.3.attn.W_V.bias torch.Size([768]) 0.0016517918556928635 0.0643523558974266\n",
      "blocks.3.attn.out_proj.weight torch.Size([768, 768]) 3.374144580448046e-05 0.0841251015663147\n",
      "blocks.3.attn.out_proj.bias torch.Size([768]) -0.0015612887218594551 0.10791037976741791\n",
      "blocks.3.ff.layers.0.weight torch.Size([3072, 768]) -0.00595002481713891 0.12953029572963715\n",
      "blocks.3.ff.layers.0.bias torch.Size([3072]) -0.09253177046775818 0.08565898984670639\n",
      "blocks.3.ff.layers.2.weight torch.Size([768, 3072]) 0.00017645648040343076 0.09180603921413422\n",
      "blocks.3.ff.layers.2.bias torch.Size([768]) 0.0021020257845520973 0.11511888355016708\n",
      "blocks.3.ln1.weight torch.Size([768]) 0.3010978400707245 0.05351252108812332\n",
      "blocks.3.ln1.bias torch.Size([768]) 0.0054475064389407635 0.07015811651945114\n",
      "blocks.3.ln2.weight torch.Size([768]) 0.30650773644447327 0.05265355482697487\n",
      "blocks.3.ln2.bias torch.Size([768]) 0.00996524840593338 0.04357629269361496\n",
      "blocks.4.attn.W_Q.weight torch.Size([768, 768]) 0.0002935132652055472 0.1704353392124176\n",
      "blocks.4.attn.W_Q.bias torch.Size([768]) 0.012315389700233936 0.20646966993808746\n",
      "blocks.4.attn.W_K.weight torch.Size([768, 768]) 5.515472003025934e-05 0.15752458572387695\n",
      "blocks.4.attn.W_K.bias torch.Size([768]) 0.004667005501687527 0.3554832935333252\n",
      "blocks.4.attn.W_V.weight torch.Size([768, 768]) 0.00010751151421573013 0.10232267528772354\n",
      "blocks.4.attn.W_V.bias torch.Size([768]) -0.0013783341273665428 0.05650889128446579\n",
      "blocks.4.attn.out_proj.weight torch.Size([768, 768]) -1.1396015906939283e-05 0.09297914057970047\n",
      "blocks.4.attn.out_proj.bias torch.Size([768]) -0.0009013282251544297 0.10049892961978912\n",
      "blocks.4.ff.layers.0.weight torch.Size([3072, 768]) -0.0032636644318699837 0.12971320748329163\n",
      "blocks.4.ff.layers.0.bias torch.Size([3072]) -0.08612614870071411 0.09320200234651566\n",
      "blocks.4.ff.layers.2.weight torch.Size([768, 3072]) 0.0001799796591512859 0.09099913388490677\n",
      "blocks.4.ff.layers.2.bias torch.Size([768]) 0.0016059394693002105 0.13685975968837738\n",
      "blocks.4.ln1.weight torch.Size([768]) 0.31934547424316406 0.04739116132259369\n",
      "blocks.4.ln1.bias torch.Size([768]) 0.007916287519037724 0.06749274581670761\n",
      "blocks.4.ln2.weight torch.Size([768]) 0.2725818455219269 0.043896984308958054\n",
      "blocks.4.ln2.bias torch.Size([768]) 0.0009596580639481544 0.02688399702310562\n",
      "blocks.5.attn.W_Q.weight torch.Size([768, 768]) -0.00026773728313855827 0.14125582575798035\n",
      "blocks.5.attn.W_Q.bias torch.Size([768]) -0.0016259821131825447 0.12543262541294098\n",
      "blocks.5.attn.W_K.weight torch.Size([768, 768]) 4.6552144340239465e-05 0.13614720106124878\n",
      "blocks.5.attn.W_K.bias torch.Size([768]) 0.006209613289684057 0.10714870691299438\n",
      "blocks.5.attn.W_V.weight torch.Size([768, 768]) -0.00010422924242448062 0.10330777615308762\n",
      "blocks.5.attn.W_V.bias torch.Size([768]) -0.001447124988771975 0.04811704158782959\n",
      "blocks.5.attn.out_proj.weight torch.Size([768, 768]) 1.0988791473209858e-05 0.09377486258745193\n",
      "blocks.5.attn.out_proj.bias torch.Size([768]) -0.0011684768833220005 0.11080432683229446\n",
      "blocks.5.ff.layers.0.weight torch.Size([3072, 768]) -0.004193877335637808 0.1267070472240448\n",
      "blocks.5.ff.layers.0.bias torch.Size([3072]) -0.08502542972564697 0.08900804817676544\n",
      "blocks.5.ff.layers.2.weight torch.Size([768, 3072]) 0.00011598864512052387 0.09735694527626038\n",
      "blocks.5.ff.layers.2.bias torch.Size([768]) 0.0009532846161164343 0.1064532995223999\n",
      "blocks.5.ln1.weight torch.Size([768]) 0.3731194734573364 0.04502682015299797\n",
      "blocks.5.ln1.bias torch.Size([768]) 0.011876348406076431 0.04906607046723366\n",
      "blocks.5.ln2.weight torch.Size([768]) 0.27900201082229614 0.05147692933678627\n",
      "blocks.5.ln2.bias torch.Size([768]) 0.00815197080373764 0.03279997780919075\n",
      "blocks.6.attn.W_Q.weight torch.Size([768, 768]) 0.00019029114628210664 0.13397741317749023\n",
      "blocks.6.attn.W_Q.bias torch.Size([768]) 0.004683490376919508 0.18410006165504456\n",
      "blocks.6.attn.W_K.weight torch.Size([768, 768]) -7.406622171401978e-05 0.12758222222328186\n",
      "blocks.6.attn.W_K.bias torch.Size([768]) 0.0015144060598686337 0.1012103334069252\n",
      "blocks.6.attn.W_V.weight torch.Size([768, 768]) 0.00021866592578589916 0.11854671686887741\n",
      "blocks.6.attn.W_V.bias torch.Size([768]) -0.0007340701413340867 0.035576995462179184\n",
      "blocks.6.attn.out_proj.weight torch.Size([768, 768]) 3.6974248359911144e-05 0.11368992179632187\n",
      "blocks.6.attn.out_proj.bias torch.Size([768]) -0.0004363872576504946 0.10605379194021225\n",
      "blocks.6.ff.layers.0.weight torch.Size([3072, 768]) -0.0028191537130624056 0.12635649740695953\n",
      "blocks.6.ff.layers.0.bias torch.Size([3072]) -0.08570227026939392 0.09056127071380615\n",
      "blocks.6.ff.layers.2.weight torch.Size([768, 3072]) 9.54796705627814e-05 0.10733181238174438\n",
      "blocks.6.ff.layers.2.bias torch.Size([768]) 0.0015628753462806344 0.12105625122785568\n",
      "blocks.6.ln1.weight torch.Size([768]) 0.3455983102321625 0.0441710501909256\n",
      "blocks.6.ln1.bias torch.Size([768]) 0.011822459287941456 0.0662064254283905\n",
      "blocks.6.ln2.weight torch.Size([768]) 0.2594689726829529 0.047400206327438354\n",
      "blocks.6.ln2.bias torch.Size([768]) 0.004331209696829319 0.03382179141044617\n",
      "blocks.7.attn.W_Q.weight torch.Size([768, 768]) -0.00045816207421012223 0.1364603042602539\n",
      "blocks.7.attn.W_Q.bias torch.Size([768]) -0.008822117000818253 0.2147248536348343\n",
      "blocks.7.attn.W_K.weight torch.Size([768, 768]) 0.00013387270155362785 0.13045501708984375\n",
      "blocks.7.attn.W_K.bias torch.Size([768]) -0.0036485891323536634 0.09873808920383453\n",
      "blocks.7.attn.W_V.weight torch.Size([768, 768]) 5.1211449317634106e-05 0.1194656491279602\n",
      "blocks.7.attn.W_V.bias torch.Size([768]) -0.00037803445593453944 0.036869797855615616\n",
      "blocks.7.attn.out_proj.weight torch.Size([768, 768]) 2.3895683625596575e-05 0.11391738802194595\n",
      "blocks.7.attn.out_proj.bias torch.Size([768]) -0.00013363065954763442 0.14512065052986145\n",
      "blocks.7.ff.layers.0.weight torch.Size([3072, 768]) -0.00352298840880394 0.12642338871955872\n",
      "blocks.7.ff.layers.0.bias torch.Size([3072]) -0.08847203105688095 0.09064304083585739\n",
      "blocks.7.ff.layers.2.weight torch.Size([768, 3072]) 8.648945367895067e-05 0.1187349334359169\n",
      "blocks.7.ff.layers.2.bias torch.Size([768]) 0.0011925119906663895 0.12881210446357727\n",
      "blocks.7.ln1.weight torch.Size([768]) 0.3565715253353119 0.04378596320748329\n",
      "blocks.7.ln1.bias torch.Size([768]) 0.01434354204684496 0.05914687365293503\n",
      "blocks.7.ln2.weight torch.Size([768]) 0.2560140788555145 0.04705559089779854\n",
      "blocks.7.ln2.bias torch.Size([768]) 0.009183691814541817 0.04571011662483215\n",
      "blocks.8.attn.W_Q.weight torch.Size([768, 768]) -0.0003004284226335585 0.13009199500083923\n",
      "blocks.8.attn.W_Q.bias torch.Size([768]) -0.013181586749851704 0.20185819268226624\n",
      "blocks.8.attn.W_K.weight torch.Size([768, 768]) 0.00017389189451932907 0.12435027956962585\n",
      "blocks.8.attn.W_K.bias torch.Size([768]) -0.0026047879364341497 0.09828340262174606\n",
      "blocks.8.attn.W_V.weight torch.Size([768, 768]) -0.00038428150583058596 0.12628209590911865\n",
      "blocks.8.attn.W_V.bias torch.Size([768]) -0.0008028498850762844 0.036681145429611206\n",
      "blocks.8.attn.out_proj.weight torch.Size([768, 768]) 8.123623047140427e-06 0.12236364185810089\n",
      "blocks.8.attn.out_proj.bias torch.Size([768]) 0.0010857944143936038 0.14007924497127533\n",
      "blocks.8.ff.layers.0.weight torch.Size([3072, 768]) -0.0020665344782173634 0.12728072702884674\n",
      "blocks.8.ff.layers.0.bias torch.Size([3072]) -0.08505804091691971 0.0935441330075264\n",
      "blocks.8.ff.layers.2.weight torch.Size([768, 3072]) 4.6921471948735416e-05 0.13540396094322205\n",
      "blocks.8.ff.layers.2.bias torch.Size([768]) 0.0011560862185433507 0.12735813856124878\n",
      "blocks.8.ln1.weight torch.Size([768]) 0.3352259397506714 0.044541217386722565\n",
      "blocks.8.ln1.bias torch.Size([768]) 0.013251811265945435 0.06870879977941513\n",
      "blocks.8.ln2.weight torch.Size([768]) 0.2566564977169037 0.04127686098217964\n",
      "blocks.8.ln2.bias torch.Size([768]) 0.000384395505534485 0.049368783831596375\n",
      "blocks.9.attn.W_Q.weight torch.Size([768, 768]) 0.00044967501889914274 0.12296558916568756\n",
      "blocks.9.attn.W_Q.bias torch.Size([768]) 0.008131361566483974 0.22075878083705902\n",
      "blocks.9.attn.W_K.weight torch.Size([768, 768]) -0.0003476667625363916 0.11892230808734894\n",
      "blocks.9.attn.W_K.bias torch.Size([768]) -0.007281634956598282 0.09623975306749344\n",
      "blocks.9.attn.W_V.weight torch.Size([768, 768]) -0.00031157408375293016 0.13612067699432373\n",
      "blocks.9.attn.W_V.bias torch.Size([768]) 6.7658256739377975e-06 0.03432562202215195\n",
      "blocks.9.attn.out_proj.weight torch.Size([768, 768]) -2.7884121664101258e-05 0.13681966066360474\n",
      "blocks.9.attn.out_proj.bias torch.Size([768]) 0.0021340707316994667 0.20949006080627441\n",
      "blocks.9.ff.layers.0.weight torch.Size([3072, 768]) -0.0027408814057707787 0.12761937081813812\n",
      "blocks.9.ff.layers.0.bias torch.Size([3072]) -0.08366744965314865 0.09236326813697815\n",
      "blocks.9.ff.layers.2.weight torch.Size([768, 3072]) 3.4792548831319436e-05 0.1558738499879837\n",
      "blocks.9.ff.layers.2.bias torch.Size([768]) 0.0007137329666875303 0.15918298065662384\n",
      "blocks.9.ln1.weight torch.Size([768]) 0.3575561046600342 0.04693679139018059\n",
      "blocks.9.ln1.bias torch.Size([768]) 0.01590331830084324 0.06386822462081909\n",
      "blocks.9.ln2.weight torch.Size([768]) 0.26497504115104675 0.041519638150930405\n",
      "blocks.9.ln2.bias torch.Size([768]) 0.006400738377124071 0.04571041837334633\n",
      "blocks.10.attn.W_Q.weight torch.Size([768, 768]) 0.00031771004432812333 0.11865982413291931\n",
      "blocks.10.attn.W_Q.bias torch.Size([768]) 0.008911840617656708 0.2265850007534027\n",
      "blocks.10.attn.W_K.weight torch.Size([768, 768]) -0.00012976815924048424 0.11467549949884415\n",
      "blocks.10.attn.W_K.bias torch.Size([768]) -0.002668120665475726 0.09832144528627396\n",
      "blocks.10.attn.W_V.weight torch.Size([768, 768]) 8.956639067037031e-05 0.14459584653377533\n",
      "blocks.10.attn.W_V.bias torch.Size([768]) -0.0014112890930846334 0.04762532189488411\n",
      "blocks.10.attn.out_proj.weight torch.Size([768, 768]) -8.798572821433481e-07 0.14662745594978333\n",
      "blocks.10.attn.out_proj.bias torch.Size([768]) 0.0020241064485162497 0.2322089970111847\n",
      "blocks.10.ff.layers.0.weight torch.Size([3072, 768]) -0.0032080465462058783 0.12764813005924225\n",
      "blocks.10.ff.layers.0.bias torch.Size([3072]) -0.07652498036623001 0.09122857451438904\n",
      "blocks.10.ff.layers.2.weight torch.Size([768, 3072]) 6.105272404965945e-06 0.17814528942108154\n",
      "blocks.10.ff.layers.2.bias torch.Size([768]) 0.0016591990133747458 0.19404050707817078\n",
      "blocks.10.ln1.weight torch.Size([768]) 0.37820783257484436 0.055699631571769714\n",
      "blocks.10.ln1.bias torch.Size([768]) 0.018612800166010857 0.05506131052970886\n",
      "blocks.10.ln2.weight torch.Size([768]) 0.2896941602230072 0.05117756500840187\n",
      "blocks.10.ln2.bias torch.Size([768]) 0.021159043535590172 0.04487457126379013\n",
      "blocks.11.attn.W_Q.weight torch.Size([768, 768]) -1.1366326361894608e-05 0.10982047021389008\n",
      "blocks.11.attn.W_Q.bias torch.Size([768]) 0.0005513962241820991 0.18319359421730042\n",
      "blocks.11.attn.W_K.weight torch.Size([768, 768]) -4.2440758988959715e-05 0.10549236834049225\n",
      "blocks.11.attn.W_K.bias torch.Size([768]) 0.0015315081691369414 0.08636081963777542\n",
      "blocks.11.attn.W_V.weight torch.Size([768, 768]) 0.0002158462448278442 0.16225944459438324\n",
      "blocks.11.attn.W_V.bias torch.Size([768]) 0.00011266752699157223 0.05450312793254852\n",
      "blocks.11.attn.out_proj.weight torch.Size([768, 768]) -5.3778097935719416e-05 0.1819266527891159\n",
      "blocks.11.attn.out_proj.bias torch.Size([768]) -0.021505361422896385 0.46894726157188416\n",
      "blocks.11.ff.layers.0.weight torch.Size([3072, 768]) -0.001846416387706995 0.13000451028347015\n",
      "blocks.11.ff.layers.0.bias torch.Size([3072]) -0.06411425024271011 0.0930408164858818\n",
      "blocks.11.ff.layers.2.weight torch.Size([768, 3072]) -0.00043532054405659437 0.19821906089782715\n",
      "blocks.11.ff.layers.2.bias torch.Size([768]) 0.000971626432146877 0.10824692994356155\n",
      "blocks.11.ln1.weight torch.Size([768]) 0.4786931276321411 0.06516604125499725\n",
      "blocks.11.ln1.bias torch.Size([768]) 0.023284194990992546 0.05674212425947189\n",
      "blocks.11.ln2.weight torch.Size([768]) 0.5041061043739319 0.09001091122627258\n",
      "blocks.11.ln2.bias torch.Size([768]) 0.009192791767418385 0.03916310518980026\n",
      "final_norm.weight torch.Size([768]) 1.5078086853027344 1.3910776376724243\n",
      "final_norm.bias torch.Size([768]) -0.003138466738164425 0.4196469485759735\n",
      "out_head.weight torch.Size([50257, 768]) 0.00037981756031513214 0.14369554817676544\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:49:36.416965Z",
     "start_time": "2025-06-15T03:49:35.406847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import complete_text\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)"
   ],
   "id": "a6c89718f5289428",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of the game, and then the game ends.\n",
      "\n",
      "The game is a\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:51:43.852438Z",
     "start_time": "2025-06-15T03:51:42.920123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(\"at the start of\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", tensor_to_text(token_ids, tokenizer))"
   ],
   "id": "84204d1eb2f82b26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of an international series of events. You don't have to worry about who has\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Instruction Finetuning",
   "id": "d988216ca3645214"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3a43e589337beead"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
