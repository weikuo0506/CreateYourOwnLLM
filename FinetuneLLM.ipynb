{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-tune LLM to follow instructions\n",
   "id": "b6e02cf5201e2731"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load open weights",
   "id": "91e1b02b97bac08c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:48.447602Z",
     "start_time": "2025-06-27T02:16:46.267268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import GPT2Model, GPT_CONFIG_124M, complete_text, generate_text_simple, tensor_to_text, text_to_tensor\n",
    "\n",
    "GPT_CONFIG_124M.update({\"qkv_bias\": True})\n",
    "model = GPT2Model(GPT_CONFIG_124M)\n"
   ],
   "id": "423a8cab44a48e29",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:48.456674Z",
     "start_time": "2025-06-27T02:16:48.453307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ],
   "id": "891e084dd7eeb90a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768])\n",
      "pos_emb.weight torch.Size([1024, 768])\n",
      "blocks.0.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_Q.bias torch.Size([768])\n",
      "blocks.0.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_K.bias torch.Size([768])\n",
      "blocks.0.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_V.bias torch.Size([768])\n",
      "blocks.0.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.0.attn.out_proj.bias torch.Size([768])\n",
      "blocks.0.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.0.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.0.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.0.ff.layers.2.bias torch.Size([768])\n",
      "blocks.0.ln1.weight torch.Size([768])\n",
      "blocks.0.ln1.bias torch.Size([768])\n",
      "blocks.0.ln2.weight torch.Size([768])\n",
      "blocks.0.ln2.bias torch.Size([768])\n",
      "blocks.1.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_Q.bias torch.Size([768])\n",
      "blocks.1.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_K.bias torch.Size([768])\n",
      "blocks.1.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_V.bias torch.Size([768])\n",
      "blocks.1.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.1.attn.out_proj.bias torch.Size([768])\n",
      "blocks.1.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.1.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.1.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.1.ff.layers.2.bias torch.Size([768])\n",
      "blocks.1.ln1.weight torch.Size([768])\n",
      "blocks.1.ln1.bias torch.Size([768])\n",
      "blocks.1.ln2.weight torch.Size([768])\n",
      "blocks.1.ln2.bias torch.Size([768])\n",
      "blocks.2.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_Q.bias torch.Size([768])\n",
      "blocks.2.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_K.bias torch.Size([768])\n",
      "blocks.2.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_V.bias torch.Size([768])\n",
      "blocks.2.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.2.attn.out_proj.bias torch.Size([768])\n",
      "blocks.2.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.2.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.2.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.2.ff.layers.2.bias torch.Size([768])\n",
      "blocks.2.ln1.weight torch.Size([768])\n",
      "blocks.2.ln1.bias torch.Size([768])\n",
      "blocks.2.ln2.weight torch.Size([768])\n",
      "blocks.2.ln2.bias torch.Size([768])\n",
      "blocks.3.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_Q.bias torch.Size([768])\n",
      "blocks.3.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_K.bias torch.Size([768])\n",
      "blocks.3.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_V.bias torch.Size([768])\n",
      "blocks.3.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.3.attn.out_proj.bias torch.Size([768])\n",
      "blocks.3.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.3.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.3.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.3.ff.layers.2.bias torch.Size([768])\n",
      "blocks.3.ln1.weight torch.Size([768])\n",
      "blocks.3.ln1.bias torch.Size([768])\n",
      "blocks.3.ln2.weight torch.Size([768])\n",
      "blocks.3.ln2.bias torch.Size([768])\n",
      "blocks.4.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_Q.bias torch.Size([768])\n",
      "blocks.4.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_K.bias torch.Size([768])\n",
      "blocks.4.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_V.bias torch.Size([768])\n",
      "blocks.4.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.4.attn.out_proj.bias torch.Size([768])\n",
      "blocks.4.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.4.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.4.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.4.ff.layers.2.bias torch.Size([768])\n",
      "blocks.4.ln1.weight torch.Size([768])\n",
      "blocks.4.ln1.bias torch.Size([768])\n",
      "blocks.4.ln2.weight torch.Size([768])\n",
      "blocks.4.ln2.bias torch.Size([768])\n",
      "blocks.5.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_Q.bias torch.Size([768])\n",
      "blocks.5.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_K.bias torch.Size([768])\n",
      "blocks.5.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_V.bias torch.Size([768])\n",
      "blocks.5.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.5.attn.out_proj.bias torch.Size([768])\n",
      "blocks.5.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.5.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.5.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.5.ff.layers.2.bias torch.Size([768])\n",
      "blocks.5.ln1.weight torch.Size([768])\n",
      "blocks.5.ln1.bias torch.Size([768])\n",
      "blocks.5.ln2.weight torch.Size([768])\n",
      "blocks.5.ln2.bias torch.Size([768])\n",
      "blocks.6.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_Q.bias torch.Size([768])\n",
      "blocks.6.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_K.bias torch.Size([768])\n",
      "blocks.6.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_V.bias torch.Size([768])\n",
      "blocks.6.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.6.attn.out_proj.bias torch.Size([768])\n",
      "blocks.6.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.6.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.6.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.6.ff.layers.2.bias torch.Size([768])\n",
      "blocks.6.ln1.weight torch.Size([768])\n",
      "blocks.6.ln1.bias torch.Size([768])\n",
      "blocks.6.ln2.weight torch.Size([768])\n",
      "blocks.6.ln2.bias torch.Size([768])\n",
      "blocks.7.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_Q.bias torch.Size([768])\n",
      "blocks.7.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_K.bias torch.Size([768])\n",
      "blocks.7.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_V.bias torch.Size([768])\n",
      "blocks.7.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.7.attn.out_proj.bias torch.Size([768])\n",
      "blocks.7.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.7.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.7.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.7.ff.layers.2.bias torch.Size([768])\n",
      "blocks.7.ln1.weight torch.Size([768])\n",
      "blocks.7.ln1.bias torch.Size([768])\n",
      "blocks.7.ln2.weight torch.Size([768])\n",
      "blocks.7.ln2.bias torch.Size([768])\n",
      "blocks.8.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_Q.bias torch.Size([768])\n",
      "blocks.8.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_K.bias torch.Size([768])\n",
      "blocks.8.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_V.bias torch.Size([768])\n",
      "blocks.8.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.8.attn.out_proj.bias torch.Size([768])\n",
      "blocks.8.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.8.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.8.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.8.ff.layers.2.bias torch.Size([768])\n",
      "blocks.8.ln1.weight torch.Size([768])\n",
      "blocks.8.ln1.bias torch.Size([768])\n",
      "blocks.8.ln2.weight torch.Size([768])\n",
      "blocks.8.ln2.bias torch.Size([768])\n",
      "blocks.9.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_Q.bias torch.Size([768])\n",
      "blocks.9.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_K.bias torch.Size([768])\n",
      "blocks.9.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_V.bias torch.Size([768])\n",
      "blocks.9.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.9.attn.out_proj.bias torch.Size([768])\n",
      "blocks.9.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.9.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.9.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.9.ff.layers.2.bias torch.Size([768])\n",
      "blocks.9.ln1.weight torch.Size([768])\n",
      "blocks.9.ln1.bias torch.Size([768])\n",
      "blocks.9.ln2.weight torch.Size([768])\n",
      "blocks.9.ln2.bias torch.Size([768])\n",
      "blocks.10.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_Q.bias torch.Size([768])\n",
      "blocks.10.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_K.bias torch.Size([768])\n",
      "blocks.10.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_V.bias torch.Size([768])\n",
      "blocks.10.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.10.attn.out_proj.bias torch.Size([768])\n",
      "blocks.10.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.10.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.10.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.10.ff.layers.2.bias torch.Size([768])\n",
      "blocks.10.ln1.weight torch.Size([768])\n",
      "blocks.10.ln1.bias torch.Size([768])\n",
      "blocks.10.ln2.weight torch.Size([768])\n",
      "blocks.10.ln2.bias torch.Size([768])\n",
      "blocks.11.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_Q.bias torch.Size([768])\n",
      "blocks.11.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_K.bias torch.Size([768])\n",
      "blocks.11.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_V.bias torch.Size([768])\n",
      "blocks.11.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.11.attn.out_proj.bias torch.Size([768])\n",
      "blocks.11.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.11.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.11.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.11.ff.layers.2.bias torch.Size([768])\n",
      "blocks.11.ln1.weight torch.Size([768])\n",
      "blocks.11.ln1.bias torch.Size([768])\n",
      "blocks.11.ln2.weight torch.Size([768])\n",
      "blocks.11.ln2.bias torch.Size([768])\n",
      "final_norm.weight torch.Size([768])\n",
      "final_norm.bias torch.Size([768])\n",
      "out_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:49.262084Z",
     "start_time": "2025-06-27T02:16:48.464339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)"
   ],
   "id": "49648386cc30dc35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start ofooming Kaiserasma Upgrade Fiji Depthrowth introduce Scourge gamers inv ResearchersaxySearch commitments\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download GPT2 from OpenAI",
   "id": "2ba0d473bfd76ff9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:52.579273Z",
     "start_time": "2025-06-27T02:16:49.267548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def download_file(url, destination):\n",
    "    def _attempt_download(download_url):\n",
    "        with urllib.request.urlopen(download_url) as response:\n",
    "            total_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "            if os.path.exists(destination) and os.path.getsize(destination) == total_size:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return True\n",
    "\n",
    "            with tqdm(total=total_size, unit=\"iB\", unit_scale=True, desc=os.path.basename(download_url)) as pbar, \\\n",
    "                 open(destination, \"wb\") as f:\n",
    "                for chunk in iter(lambda: response.read(1024), b\"\"):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "            return True\n",
    "\n",
    "    try:\n",
    "        if _attempt_download(url):\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    allowed_sizes = {\"124M\", \"355M\", \"774M\", \"1558M\"}\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size must be one of {allowed_sizes}\")\n",
    "\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    base_url = f\"https://openaipublic.blob.core.windows.net/gpt-2/models/{model_size}/\"\n",
    "\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    for fname in filenames:\n",
    "        dst = os.path.join(model_dir, fname)\n",
    "        if os.path.exists(dst):\n",
    "            print(f\"Already exists: {fname}, skipping download.\")\n",
    "            continue\n",
    "        primary = urljoin(base_url, fname)\n",
    "        print(f\"Downloading {fname} ...\")\n",
    "        download_file(primary, dst)\n",
    "\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    with open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        settings = json.load(f)\n",
    "\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "    return settings, params\n"
   ],
   "id": "f29d5d3c3d0402a0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/PycharmProjects/CreateYourOwnLLM/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:52.927806Z",
     "start_time": "2025-06-27T02:16:52.584554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Params:\", params.keys())"
   ],
   "id": "28d0828f52dfce6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: checkpoint, skipping download.\n",
      "Already exists: encoder.json, skipping download.\n",
      "Already exists: hparams.json, skipping download.\n",
      "Already exists: model.ckpt.data-00000-of-00001, skipping download.\n",
      "Already exists: model.ckpt.index, skipping download.\n",
      "Already exists: model.ckpt.meta, skipping download.\n",
      "Already exists: vocab.bpe, skipping download.\n",
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Params: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:52.938257Z",
     "start_time": "2025-06-27T02:16:52.931604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def assign_(left, right):\n",
    "    if right is None:\n",
    "        raise ValueError(\"'right' cannot be None\")\n",
    "    right_tensor = torch.as_tensor(right, dtype=left.dtype, device=left.device)\n",
    "    if right_tensor.numel() == 0:\n",
    "        raise ValueError(\"'right' cannot be Empty\")\n",
    "    if left.shape != right_tensor.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {left.shape} vs {right_tensor.shape}\")\n",
    "    with torch.no_grad():\n",
    "        left.copy_(right_tensor)\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    assign_(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    assign_(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b, (block, pblock) in enumerate(zip(gpt.blocks, params[\"blocks\"])):\n",
    "        # Attention QKV\n",
    "        qw, kw, vw = np.split(pblock[\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
    "        qb, kb, vb = np.split(pblock[\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
    "        assign_(block.attn.W_Q.weight, qw.T)\n",
    "        assign_(block.attn.W_K.weight, kw.T)\n",
    "        assign_(block.attn.W_V.weight, vw.T)\n",
    "        assign_(block.attn.W_Q.bias, qb)\n",
    "        assign_(block.attn.W_K.bias, kb)\n",
    "        assign_(block.attn.W_V.bias, vb)\n",
    "\n",
    "        # Attention output projection\n",
    "        assign_(block.attn.out_proj.weight, pblock[\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        assign_(block.attn.out_proj.bias,   pblock[\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # Feedforward\n",
    "        assign_(block.ff.layers[0].weight, pblock[\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        assign_(block.ff.layers[0].bias,   pblock[\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        assign_(block.ff.layers[2].weight, pblock[\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        assign_(block.ff.layers[2].bias,   pblock[\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # LayerNorms\n",
    "        assign_(block.ln1.weight, pblock[\"ln_1\"][\"g\"])\n",
    "        assign_(block.ln1.bias, pblock[\"ln_1\"][\"b\"])\n",
    "        assign_(block.ln2.weight, pblock[\"ln_2\"][\"g\"])\n",
    "        assign_(block.ln2.bias, pblock[\"ln_2\"][\"b\"])\n",
    "\n",
    "    assign_(gpt.final_norm.weight, params[\"g\"])\n",
    "    assign_(gpt.final_norm.bias, params[\"b\"])\n",
    "    assign_(gpt.out_head.weight,  params[\"wte\"])\n"
   ],
   "id": "e83e9805c9622997",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:53.040183Z",
     "start_time": "2025-06-27T02:16:52.947800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_weights_into_gpt(model, params)\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n"
   ],
   "id": "7696090694c7c0c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:53.108804Z",
     "start_time": "2025-06-27T02:16:53.050443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.mean().item(), param.std().item())\n"
   ],
   "id": "6ae4be9bb4b68198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768]) 0.00037981756031513214 0.14369554817676544\n",
      "pos_emb.weight torch.Size([1024, 768]) -0.0006787165184505284 0.1226913258433342\n",
      "blocks.0.attn.W_Q.weight torch.Size([768, 768]) 0.00015374351642094553 0.2386905699968338\n",
      "blocks.0.attn.W_Q.bias torch.Size([768]) -0.007821076549589634 0.3427544832229614\n",
      "blocks.0.attn.W_K.weight torch.Size([768, 768]) 1.2351122677500825e-05 0.2432965785264969\n",
      "blocks.0.attn.W_K.bias torch.Size([768]) 0.0048723239451646805 0.18297071754932404\n",
      "blocks.0.attn.W_V.weight torch.Size([768, 768]) -5.968316145299468e-06 0.05811797454953194\n",
      "blocks.0.attn.W_V.bias torch.Size([768]) 0.0008267878438346088 0.04772818833589554\n",
      "blocks.0.attn.out_proj.weight torch.Size([768, 768]) -0.0001613790518604219 0.1474614143371582\n",
      "blocks.0.attn.out_proj.bias torch.Size([768]) -0.00691022165119648 0.2589662969112396\n",
      "blocks.0.ff.layers.0.weight torch.Size([3072, 768]) -0.0007485305541194975 0.14116929471492767\n",
      "blocks.0.ff.layers.0.bias torch.Size([3072]) -0.0931621864438057 0.13235801458358765\n",
      "blocks.0.ff.layers.2.weight torch.Size([768, 3072]) 8.009047633095179e-06 0.0879654809832573\n",
      "blocks.0.ff.layers.2.bias torch.Size([768]) -0.0004230523481965065 0.10169976204633713\n",
      "blocks.0.ln1.weight torch.Size([768]) 0.18035894632339478 0.04131494462490082\n",
      "blocks.0.ln1.bias torch.Size([768]) -0.006593453232198954 0.03580174222588539\n",
      "blocks.0.ln2.weight torch.Size([768]) 0.8678296208381653 0.48494789004325867\n",
      "blocks.0.ln2.bias torch.Size([768]) 0.00920353177934885 0.07009701430797577\n",
      "blocks.1.attn.W_Q.weight torch.Size([768, 768]) -0.00015265395632013679 0.15103811025619507\n",
      "blocks.1.attn.W_Q.bias torch.Size([768]) 0.001711765886284411 0.3485022187232971\n",
      "blocks.1.attn.W_K.weight torch.Size([768, 768]) 0.00023460258671548218 0.15902258455753326\n",
      "blocks.1.attn.W_K.bias torch.Size([768]) 0.0008940294501371682 0.08274678885936737\n",
      "blocks.1.attn.W_V.weight torch.Size([768, 768]) 2.3152324502007104e-06 0.10367371886968613\n",
      "blocks.1.attn.W_V.bias torch.Size([768]) -0.00020504526037257165 0.07381219416856766\n",
      "blocks.1.attn.out_proj.weight torch.Size([768, 768]) -8.275873551610857e-05 0.10191785544157028\n",
      "blocks.1.attn.out_proj.bias torch.Size([768]) -0.0010727141052484512 0.104808010160923\n",
      "blocks.1.ff.layers.0.weight torch.Size([3072, 768]) 0.0006420306744985282 0.1307205706834793\n",
      "blocks.1.ff.layers.0.bias torch.Size([3072]) -0.07219841331243515 0.09491100162267685\n",
      "blocks.1.ff.layers.2.weight torch.Size([768, 3072]) 9.794025390874594e-05 0.08719106763601303\n",
      "blocks.1.ff.layers.2.bias torch.Size([768]) 0.0002505724842194468 0.10042908787727356\n",
      "blocks.1.ln1.weight torch.Size([768]) 0.22284089028835297 0.05130758509039879\n",
      "blocks.1.ln1.bias torch.Size([768]) -0.0050234864465892315 0.052436452358961105\n",
      "blocks.1.ln2.weight torch.Size([768]) 0.24269406497478485 0.03164536505937576\n",
      "blocks.1.ln2.bias torch.Size([768]) -0.004073990508913994 0.03919508308172226\n",
      "blocks.2.attn.W_Q.weight torch.Size([768, 768]) 0.0001179432583739981 0.18932299315929413\n",
      "blocks.2.attn.W_Q.bias torch.Size([768]) -0.006064689252525568 0.2589069902896881\n",
      "blocks.2.attn.W_K.weight torch.Size([768, 768]) -4.986231942893937e-05 0.15181933343410492\n",
      "blocks.2.attn.W_K.bias torch.Size([768]) -0.003441582666710019 0.10127473622560501\n",
      "blocks.2.attn.W_V.weight torch.Size([768, 768]) 0.00017838712665252388 0.10502831637859344\n",
      "blocks.2.attn.W_V.bias torch.Size([768]) -0.0021237407345324755 0.0655340775847435\n",
      "blocks.2.attn.out_proj.weight torch.Size([768, 768]) -3.299467061879113e-05 0.08103547245264053\n",
      "blocks.2.attn.out_proj.bias torch.Size([768]) 0.003375591477379203 0.1451117843389511\n",
      "blocks.2.ff.layers.0.weight torch.Size([3072, 768]) -0.005061259493231773 0.13352689146995544\n",
      "blocks.2.ff.layers.0.bias torch.Size([3072]) -0.0928223729133606 0.1066708043217659\n",
      "blocks.2.ff.layers.2.weight torch.Size([768, 3072]) 0.00019714866357389838 0.09308750927448273\n",
      "blocks.2.ff.layers.2.bias torch.Size([768]) 0.002819357207044959 0.11242914199829102\n",
      "blocks.2.ln1.weight torch.Size([768]) 0.24077002704143524 0.07527267932891846\n",
      "blocks.2.ln1.bias torch.Size([768]) -0.00036007841117680073 0.07055915892124176\n",
      "blocks.2.ln2.weight torch.Size([768]) 0.2925874888896942 0.04540559649467468\n",
      "blocks.2.ln2.bias torch.Size([768]) 0.006347864866256714 0.043932873755693436\n",
      "blocks.3.attn.W_Q.weight torch.Size([768, 768]) -0.00021896824182476848 0.16474692523479462\n",
      "blocks.3.attn.W_Q.bias torch.Size([768]) -0.0038416890893131495 0.2126476913690567\n",
      "blocks.3.attn.W_K.weight torch.Size([768, 768]) -5.696068546967581e-05 0.1537768691778183\n",
      "blocks.3.attn.W_K.bias torch.Size([768]) -0.0004758095892611891 0.10474759340286255\n",
      "blocks.3.attn.W_V.weight torch.Size([768, 768]) 0.00020247689099051058 0.09766855090856552\n",
      "blocks.3.attn.W_V.bias torch.Size([768]) 0.0016517918556928635 0.0643523558974266\n",
      "blocks.3.attn.out_proj.weight torch.Size([768, 768]) 3.374144580448046e-05 0.0841251015663147\n",
      "blocks.3.attn.out_proj.bias torch.Size([768]) -0.0015612887218594551 0.10791037976741791\n",
      "blocks.3.ff.layers.0.weight torch.Size([3072, 768]) -0.00595002481713891 0.12953029572963715\n",
      "blocks.3.ff.layers.0.bias torch.Size([3072]) -0.09253177046775818 0.08565898984670639\n",
      "blocks.3.ff.layers.2.weight torch.Size([768, 3072]) 0.00017645648040343076 0.09180603921413422\n",
      "blocks.3.ff.layers.2.bias torch.Size([768]) 0.0021020257845520973 0.11511888355016708\n",
      "blocks.3.ln1.weight torch.Size([768]) 0.3010978400707245 0.05351252108812332\n",
      "blocks.3.ln1.bias torch.Size([768]) 0.0054475064389407635 0.07015811651945114\n",
      "blocks.3.ln2.weight torch.Size([768]) 0.30650773644447327 0.05265355482697487\n",
      "blocks.3.ln2.bias torch.Size([768]) 0.00996524840593338 0.04357629269361496\n",
      "blocks.4.attn.W_Q.weight torch.Size([768, 768]) 0.0002935132652055472 0.1704353392124176\n",
      "blocks.4.attn.W_Q.bias torch.Size([768]) 0.012315389700233936 0.20646966993808746\n",
      "blocks.4.attn.W_K.weight torch.Size([768, 768]) 5.515472003025934e-05 0.15752458572387695\n",
      "blocks.4.attn.W_K.bias torch.Size([768]) 0.004667005501687527 0.3554832935333252\n",
      "blocks.4.attn.W_V.weight torch.Size([768, 768]) 0.00010751151421573013 0.10232267528772354\n",
      "blocks.4.attn.W_V.bias torch.Size([768]) -0.0013783341273665428 0.05650889128446579\n",
      "blocks.4.attn.out_proj.weight torch.Size([768, 768]) -1.1396015906939283e-05 0.09297914057970047\n",
      "blocks.4.attn.out_proj.bias torch.Size([768]) -0.0009013282251544297 0.10049892961978912\n",
      "blocks.4.ff.layers.0.weight torch.Size([3072, 768]) -0.0032636644318699837 0.12971320748329163\n",
      "blocks.4.ff.layers.0.bias torch.Size([3072]) -0.08612614870071411 0.09320200234651566\n",
      "blocks.4.ff.layers.2.weight torch.Size([768, 3072]) 0.0001799796591512859 0.09099913388490677\n",
      "blocks.4.ff.layers.2.bias torch.Size([768]) 0.0016059394693002105 0.13685975968837738\n",
      "blocks.4.ln1.weight torch.Size([768]) 0.31934547424316406 0.04739116132259369\n",
      "blocks.4.ln1.bias torch.Size([768]) 0.007916287519037724 0.06749274581670761\n",
      "blocks.4.ln2.weight torch.Size([768]) 0.2725818455219269 0.043896984308958054\n",
      "blocks.4.ln2.bias torch.Size([768]) 0.0009596580639481544 0.02688399702310562\n",
      "blocks.5.attn.W_Q.weight torch.Size([768, 768]) -0.00026773728313855827 0.14125582575798035\n",
      "blocks.5.attn.W_Q.bias torch.Size([768]) -0.0016259821131825447 0.12543262541294098\n",
      "blocks.5.attn.W_K.weight torch.Size([768, 768]) 4.6552144340239465e-05 0.13614720106124878\n",
      "blocks.5.attn.W_K.bias torch.Size([768]) 0.006209613289684057 0.10714870691299438\n",
      "blocks.5.attn.W_V.weight torch.Size([768, 768]) -0.00010422924242448062 0.10330777615308762\n",
      "blocks.5.attn.W_V.bias torch.Size([768]) -0.001447124988771975 0.04811704158782959\n",
      "blocks.5.attn.out_proj.weight torch.Size([768, 768]) 1.0988791473209858e-05 0.09377486258745193\n",
      "blocks.5.attn.out_proj.bias torch.Size([768]) -0.0011684768833220005 0.11080432683229446\n",
      "blocks.5.ff.layers.0.weight torch.Size([3072, 768]) -0.004193877335637808 0.1267070472240448\n",
      "blocks.5.ff.layers.0.bias torch.Size([3072]) -0.08502542972564697 0.08900804817676544\n",
      "blocks.5.ff.layers.2.weight torch.Size([768, 3072]) 0.00011598864512052387 0.09735694527626038\n",
      "blocks.5.ff.layers.2.bias torch.Size([768]) 0.0009532846161164343 0.1064532995223999\n",
      "blocks.5.ln1.weight torch.Size([768]) 0.3731194734573364 0.04502682015299797\n",
      "blocks.5.ln1.bias torch.Size([768]) 0.011876348406076431 0.04906607046723366\n",
      "blocks.5.ln2.weight torch.Size([768]) 0.27900201082229614 0.05147692933678627\n",
      "blocks.5.ln2.bias torch.Size([768]) 0.00815197080373764 0.03279997780919075\n",
      "blocks.6.attn.W_Q.weight torch.Size([768, 768]) 0.00019029114628210664 0.13397741317749023\n",
      "blocks.6.attn.W_Q.bias torch.Size([768]) 0.004683490376919508 0.18410006165504456\n",
      "blocks.6.attn.W_K.weight torch.Size([768, 768]) -7.406622171401978e-05 0.12758222222328186\n",
      "blocks.6.attn.W_K.bias torch.Size([768]) 0.0015144060598686337 0.1012103334069252\n",
      "blocks.6.attn.W_V.weight torch.Size([768, 768]) 0.00021866592578589916 0.11854671686887741\n",
      "blocks.6.attn.W_V.bias torch.Size([768]) -0.0007340701413340867 0.035576995462179184\n",
      "blocks.6.attn.out_proj.weight torch.Size([768, 768]) 3.6974248359911144e-05 0.11368992179632187\n",
      "blocks.6.attn.out_proj.bias torch.Size([768]) -0.0004363872576504946 0.10605379194021225\n",
      "blocks.6.ff.layers.0.weight torch.Size([3072, 768]) -0.0028191537130624056 0.12635649740695953\n",
      "blocks.6.ff.layers.0.bias torch.Size([3072]) -0.08570227026939392 0.09056127071380615\n",
      "blocks.6.ff.layers.2.weight torch.Size([768, 3072]) 9.54796705627814e-05 0.10733181238174438\n",
      "blocks.6.ff.layers.2.bias torch.Size([768]) 0.0015628753462806344 0.12105625122785568\n",
      "blocks.6.ln1.weight torch.Size([768]) 0.3455983102321625 0.0441710501909256\n",
      "blocks.6.ln1.bias torch.Size([768]) 0.011822459287941456 0.0662064254283905\n",
      "blocks.6.ln2.weight torch.Size([768]) 0.2594689726829529 0.047400206327438354\n",
      "blocks.6.ln2.bias torch.Size([768]) 0.004331209696829319 0.03382179141044617\n",
      "blocks.7.attn.W_Q.weight torch.Size([768, 768]) -0.00045816207421012223 0.1364603042602539\n",
      "blocks.7.attn.W_Q.bias torch.Size([768]) -0.008822117000818253 0.2147248536348343\n",
      "blocks.7.attn.W_K.weight torch.Size([768, 768]) 0.00013387270155362785 0.13045501708984375\n",
      "blocks.7.attn.W_K.bias torch.Size([768]) -0.0036485891323536634 0.09873808920383453\n",
      "blocks.7.attn.W_V.weight torch.Size([768, 768]) 5.1211449317634106e-05 0.1194656491279602\n",
      "blocks.7.attn.W_V.bias torch.Size([768]) -0.00037803445593453944 0.036869797855615616\n",
      "blocks.7.attn.out_proj.weight torch.Size([768, 768]) 2.3895683625596575e-05 0.11391738802194595\n",
      "blocks.7.attn.out_proj.bias torch.Size([768]) -0.00013363065954763442 0.14512065052986145\n",
      "blocks.7.ff.layers.0.weight torch.Size([3072, 768]) -0.00352298840880394 0.12642338871955872\n",
      "blocks.7.ff.layers.0.bias torch.Size([3072]) -0.08847203105688095 0.09064304083585739\n",
      "blocks.7.ff.layers.2.weight torch.Size([768, 3072]) 8.648945367895067e-05 0.1187349334359169\n",
      "blocks.7.ff.layers.2.bias torch.Size([768]) 0.0011925119906663895 0.12881210446357727\n",
      "blocks.7.ln1.weight torch.Size([768]) 0.3565715253353119 0.04378596320748329\n",
      "blocks.7.ln1.bias torch.Size([768]) 0.01434354204684496 0.05914687365293503\n",
      "blocks.7.ln2.weight torch.Size([768]) 0.2560140788555145 0.04705559089779854\n",
      "blocks.7.ln2.bias torch.Size([768]) 0.009183691814541817 0.04571011662483215\n",
      "blocks.8.attn.W_Q.weight torch.Size([768, 768]) -0.0003004284226335585 0.13009199500083923\n",
      "blocks.8.attn.W_Q.bias torch.Size([768]) -0.013181586749851704 0.20185819268226624\n",
      "blocks.8.attn.W_K.weight torch.Size([768, 768]) 0.00017389189451932907 0.12435027956962585\n",
      "blocks.8.attn.W_K.bias torch.Size([768]) -0.0026047879364341497 0.09828340262174606\n",
      "blocks.8.attn.W_V.weight torch.Size([768, 768]) -0.00038428150583058596 0.12628209590911865\n",
      "blocks.8.attn.W_V.bias torch.Size([768]) -0.0008028498850762844 0.036681145429611206\n",
      "blocks.8.attn.out_proj.weight torch.Size([768, 768]) 8.123623047140427e-06 0.12236364185810089\n",
      "blocks.8.attn.out_proj.bias torch.Size([768]) 0.0010857944143936038 0.14007924497127533\n",
      "blocks.8.ff.layers.0.weight torch.Size([3072, 768]) -0.0020665344782173634 0.12728072702884674\n",
      "blocks.8.ff.layers.0.bias torch.Size([3072]) -0.08505804091691971 0.0935441330075264\n",
      "blocks.8.ff.layers.2.weight torch.Size([768, 3072]) 4.6921471948735416e-05 0.13540396094322205\n",
      "blocks.8.ff.layers.2.bias torch.Size([768]) 0.0011560862185433507 0.12735813856124878\n",
      "blocks.8.ln1.weight torch.Size([768]) 0.3352259397506714 0.044541217386722565\n",
      "blocks.8.ln1.bias torch.Size([768]) 0.013251811265945435 0.06870879977941513\n",
      "blocks.8.ln2.weight torch.Size([768]) 0.2566564977169037 0.04127686098217964\n",
      "blocks.8.ln2.bias torch.Size([768]) 0.000384395505534485 0.049368783831596375\n",
      "blocks.9.attn.W_Q.weight torch.Size([768, 768]) 0.00044967501889914274 0.12296558916568756\n",
      "blocks.9.attn.W_Q.bias torch.Size([768]) 0.008131361566483974 0.22075878083705902\n",
      "blocks.9.attn.W_K.weight torch.Size([768, 768]) -0.0003476667625363916 0.11892230808734894\n",
      "blocks.9.attn.W_K.bias torch.Size([768]) -0.007281634956598282 0.09623975306749344\n",
      "blocks.9.attn.W_V.weight torch.Size([768, 768]) -0.00031157408375293016 0.13612067699432373\n",
      "blocks.9.attn.W_V.bias torch.Size([768]) 6.7658256739377975e-06 0.03432562202215195\n",
      "blocks.9.attn.out_proj.weight torch.Size([768, 768]) -2.7884121664101258e-05 0.13681966066360474\n",
      "blocks.9.attn.out_proj.bias torch.Size([768]) 0.0021340707316994667 0.20949006080627441\n",
      "blocks.9.ff.layers.0.weight torch.Size([3072, 768]) -0.0027408814057707787 0.12761937081813812\n",
      "blocks.9.ff.layers.0.bias torch.Size([3072]) -0.08366744965314865 0.09236326813697815\n",
      "blocks.9.ff.layers.2.weight torch.Size([768, 3072]) 3.4792548831319436e-05 0.1558738499879837\n",
      "blocks.9.ff.layers.2.bias torch.Size([768]) 0.0007137329666875303 0.15918298065662384\n",
      "blocks.9.ln1.weight torch.Size([768]) 0.3575561046600342 0.04693679139018059\n",
      "blocks.9.ln1.bias torch.Size([768]) 0.01590331830084324 0.06386822462081909\n",
      "blocks.9.ln2.weight torch.Size([768]) 0.26497504115104675 0.041519638150930405\n",
      "blocks.9.ln2.bias torch.Size([768]) 0.006400738377124071 0.04571041837334633\n",
      "blocks.10.attn.W_Q.weight torch.Size([768, 768]) 0.00031771004432812333 0.11865982413291931\n",
      "blocks.10.attn.W_Q.bias torch.Size([768]) 0.008911840617656708 0.2265850007534027\n",
      "blocks.10.attn.W_K.weight torch.Size([768, 768]) -0.00012976815924048424 0.11467549949884415\n",
      "blocks.10.attn.W_K.bias torch.Size([768]) -0.002668120665475726 0.09832144528627396\n",
      "blocks.10.attn.W_V.weight torch.Size([768, 768]) 8.956639067037031e-05 0.14459584653377533\n",
      "blocks.10.attn.W_V.bias torch.Size([768]) -0.0014112890930846334 0.04762532189488411\n",
      "blocks.10.attn.out_proj.weight torch.Size([768, 768]) -8.798572821433481e-07 0.14662745594978333\n",
      "blocks.10.attn.out_proj.bias torch.Size([768]) 0.0020241064485162497 0.2322089970111847\n",
      "blocks.10.ff.layers.0.weight torch.Size([3072, 768]) -0.0032080465462058783 0.12764813005924225\n",
      "blocks.10.ff.layers.0.bias torch.Size([3072]) -0.07652498036623001 0.09122857451438904\n",
      "blocks.10.ff.layers.2.weight torch.Size([768, 3072]) 6.105272404965945e-06 0.17814528942108154\n",
      "blocks.10.ff.layers.2.bias torch.Size([768]) 0.0016591990133747458 0.19404050707817078\n",
      "blocks.10.ln1.weight torch.Size([768]) 0.37820783257484436 0.055699631571769714\n",
      "blocks.10.ln1.bias torch.Size([768]) 0.018612800166010857 0.05506131052970886\n",
      "blocks.10.ln2.weight torch.Size([768]) 0.2896941602230072 0.05117756500840187\n",
      "blocks.10.ln2.bias torch.Size([768]) 0.021159043535590172 0.04487457126379013\n",
      "blocks.11.attn.W_Q.weight torch.Size([768, 768]) -1.1366326361894608e-05 0.10982047021389008\n",
      "blocks.11.attn.W_Q.bias torch.Size([768]) 0.0005513962241820991 0.18319359421730042\n",
      "blocks.11.attn.W_K.weight torch.Size([768, 768]) -4.2440758988959715e-05 0.10549236834049225\n",
      "blocks.11.attn.W_K.bias torch.Size([768]) 0.0015315081691369414 0.08636081963777542\n",
      "blocks.11.attn.W_V.weight torch.Size([768, 768]) 0.0002158462448278442 0.16225944459438324\n",
      "blocks.11.attn.W_V.bias torch.Size([768]) 0.00011266752699157223 0.05450312793254852\n",
      "blocks.11.attn.out_proj.weight torch.Size([768, 768]) -5.3778097935719416e-05 0.1819266527891159\n",
      "blocks.11.attn.out_proj.bias torch.Size([768]) -0.021505361422896385 0.46894726157188416\n",
      "blocks.11.ff.layers.0.weight torch.Size([3072, 768]) -0.001846416387706995 0.13000451028347015\n",
      "blocks.11.ff.layers.0.bias torch.Size([3072]) -0.06411425024271011 0.0930408164858818\n",
      "blocks.11.ff.layers.2.weight torch.Size([768, 3072]) -0.00043532054405659437 0.19821906089782715\n",
      "blocks.11.ff.layers.2.bias torch.Size([768]) 0.000971626432146877 0.10824692994356155\n",
      "blocks.11.ln1.weight torch.Size([768]) 0.4786931276321411 0.06516604125499725\n",
      "blocks.11.ln1.bias torch.Size([768]) 0.023284194990992546 0.05674212425947189\n",
      "blocks.11.ln2.weight torch.Size([768]) 0.5041061043739319 0.09001091122627258\n",
      "blocks.11.ln2.bias torch.Size([768]) 0.009192791767418385 0.03916310518980026\n",
      "final_norm.weight torch.Size([768]) 1.5078086853027344 1.3910776376724243\n",
      "final_norm.bias torch.Size([768]) -0.003138466738164425 0.4196469485759735\n",
      "out_head.weight torch.Size([50257, 768]) 0.00037981756031513214 0.14369554817676544\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:53.624073Z",
     "start_time": "2025-06-27T02:16:53.113402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import complete_text\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)"
   ],
   "id": "a6c89718f5289428",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of the game, and then the game ends.\n",
      "\n",
      "The game is a\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:54.183413Z",
     "start_time": "2025-06-27T02:16:53.629503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "torch.manual_seed(123)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(\"at the start of\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", tensor_to_text(token_ids, tokenizer))"
   ],
   "id": "84204d1eb2f82b26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of an international series of events. You don't have to worry about who has\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Instruction Finetuning",
   "id": "d988216ca3645214"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:54.197809Z",
     "start_time": "2025-06-27T02:16:54.196210Z"
    }
   },
   "cell_type": "code",
   "source": "# !wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
   "id": "3a43e589337beead",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:54.268229Z",
     "start_time": "2025-06-27T02:16:54.207087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"alpaca_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "random.seed(123)\n",
    "data = random.sample(data, 1000)\n"
   ],
   "id": "2b051862bc4b6205",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:54.274563Z",
     "start_time": "2025-06-27T02:16:54.272110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_input(entry):\n",
    "    instruction = entry.get(\"instruction\", \"\").strip()\n",
    "    input_section = entry.get(\"input\", \"\").strip()\n",
    "\n",
    "    parts = [\n",
    "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n",
    "        \"\\n\\n### Instruction:\\n\" + instruction,\n",
    "    ]\n",
    "\n",
    "    if input_section:\n",
    "        parts.append(\"\\n\\n### Input:\\n\" + input_section)\n",
    "\n",
    "    return \"\".join(parts)\n"
   ],
   "id": "d3baaa33104e8709",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:54.283272Z",
     "start_time": "2025-06-27T02:16:54.281376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ],
   "id": "8e2ea8dda1848a37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a general statement about the importance of empathy.\n",
      "\n",
      "### Response:\n",
      "Empathy is essential for fostering meaningful relationships, deepening understanding between people, and building a more compassionate world.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:54.292411Z",
     "start_time": "2025-06-27T02:16:54.290355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = len(data)\n",
    "train_data = data[:int(n * 0.80)]\n",
    "test_data = data[int(n * 0.80):int(n * 0.90)]\n",
    "val_data = data[int(n * 0.90):]"
   ],
   "id": "2866573989336046",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:54.301722Z",
     "start_time": "2025-06-27T02:16:54.299692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ],
   "id": "1790ba80fbfdd587",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 800\n",
      "Validation set length: 100\n",
      "Test set length: 100\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:54.312524Z",
     "start_time": "2025-06-27T02:16:54.308346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from functools import partial\n",
    "\n",
    "device = \"cpu\"  # or \"cuda\" if available\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(\n",
    "                format_input(entry) + f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            )\n",
    "            for entry in data\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_texts[idx]\n",
    "\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    max_len = min(\n",
    "        max(len(seq) + 1 for seq in batch),\n",
    "        allowed_max_length or float('inf')\n",
    "    )\n",
    "\n",
    "    input_tensors, label_tensors = [], []\n",
    "\n",
    "    for seq in batch:\n",
    "        seq = seq + [pad_token_id]\n",
    "        padded = seq + [pad_token_id] * (max_len - len(seq))\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1], dtype=torch.long)\n",
    "        labels = torch.tensor(padded[1:], dtype=torch.long)\n",
    "\n",
    "        # Mask padding in labels except the first one\n",
    "        pad_mask = (labels == pad_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(pad_mask) > 1:\n",
    "            labels[pad_mask[1:]] = ignore_index\n",
    "\n",
    "        input_tensors.append(inputs)\n",
    "        label_tensors.append(labels)\n",
    "\n",
    "    return (\n",
    "        torch.stack(input_tensors).to(device),\n",
    "        torch.stack(label_tensors).to(device)\n",
    "    )\n",
    "\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")\n"
   ],
   "id": "7370d205bf9e7f88",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:54.349818Z",
     "start_time": "2025-06-27T02:16:54.319536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=True,drop_last=True,num_workers=0)\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=False,drop_last=False,num_workers=0)\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=False,drop_last=False,num_workers=0)"
   ],
   "id": "75e168056c043dbb",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:54.386396Z",
     "start_time": "2025-06-27T02:16:54.354425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)\n",
    "print(\"inputs: \",inputs[0])\n",
    "print(\"targets: \",targets[0])"
   ],
   "id": "fe4bac319d2bb9a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader:\n",
      "torch.Size([8, 218]) torch.Size([8, 218])\n",
      "torch.Size([8, 110]) torch.Size([8, 110])\n",
      "torch.Size([8, 166]) torch.Size([8, 166])\n",
      "torch.Size([8, 174]) torch.Size([8, 174])\n",
      "torch.Size([8, 135]) torch.Size([8, 135])\n",
      "torch.Size([8, 457]) torch.Size([8, 457])\n",
      "torch.Size([8, 125]) torch.Size([8, 125])\n",
      "torch.Size([8, 119]) torch.Size([8, 119])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 167]) torch.Size([8, 167])\n",
      "torch.Size([8, 169]) torch.Size([8, 169])\n",
      "torch.Size([8, 96]) torch.Size([8, 96])\n",
      "torch.Size([8, 191]) torch.Size([8, 191])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 196]) torch.Size([8, 196])\n",
      "torch.Size([8, 239]) torch.Size([8, 239])\n",
      "torch.Size([8, 147]) torch.Size([8, 147])\n",
      "torch.Size([8, 116]) torch.Size([8, 116])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 210]) torch.Size([8, 210])\n",
      "torch.Size([8, 200]) torch.Size([8, 200])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 219]) torch.Size([8, 219])\n",
      "torch.Size([8, 264]) torch.Size([8, 264])\n",
      "torch.Size([8, 152]) torch.Size([8, 152])\n",
      "torch.Size([8, 242]) torch.Size([8, 242])\n",
      "torch.Size([8, 214]) torch.Size([8, 214])\n",
      "torch.Size([8, 194]) torch.Size([8, 194])\n",
      "torch.Size([8, 177]) torch.Size([8, 177])\n",
      "torch.Size([8, 210]) torch.Size([8, 210])\n",
      "torch.Size([8, 187]) torch.Size([8, 187])\n",
      "torch.Size([8, 302]) torch.Size([8, 302])\n",
      "torch.Size([8, 170]) torch.Size([8, 170])\n",
      "torch.Size([8, 174]) torch.Size([8, 174])\n",
      "torch.Size([8, 242]) torch.Size([8, 242])\n",
      "torch.Size([8, 234]) torch.Size([8, 234])\n",
      "torch.Size([8, 235]) torch.Size([8, 235])\n",
      "torch.Size([8, 145]) torch.Size([8, 145])\n",
      "torch.Size([8, 122]) torch.Size([8, 122])\n",
      "torch.Size([8, 129]) torch.Size([8, 129])\n",
      "torch.Size([8, 228]) torch.Size([8, 228])\n",
      "torch.Size([8, 561]) torch.Size([8, 561])\n",
      "torch.Size([8, 219]) torch.Size([8, 219])\n",
      "torch.Size([8, 151]) torch.Size([8, 151])\n",
      "torch.Size([8, 344]) torch.Size([8, 344])\n",
      "torch.Size([8, 375]) torch.Size([8, 375])\n",
      "torch.Size([8, 326]) torch.Size([8, 326])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 156]) torch.Size([8, 156])\n",
      "torch.Size([8, 149]) torch.Size([8, 149])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 239]) torch.Size([8, 239])\n",
      "torch.Size([8, 172]) torch.Size([8, 172])\n",
      "torch.Size([8, 144]) torch.Size([8, 144])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 262]) torch.Size([8, 262])\n",
      "torch.Size([8, 240]) torch.Size([8, 240])\n",
      "torch.Size([8, 218]) torch.Size([8, 218])\n",
      "torch.Size([8, 159]) torch.Size([8, 159])\n",
      "torch.Size([8, 153]) torch.Size([8, 153])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 152]) torch.Size([8, 152])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 200]) torch.Size([8, 200])\n",
      "torch.Size([8, 243]) torch.Size([8, 243])\n",
      "torch.Size([8, 160]) torch.Size([8, 160])\n",
      "torch.Size([8, 182]) torch.Size([8, 182])\n",
      "torch.Size([8, 167]) torch.Size([8, 167])\n",
      "torch.Size([8, 252]) torch.Size([8, 252])\n",
      "torch.Size([8, 146]) torch.Size([8, 146])\n",
      "torch.Size([8, 155]) torch.Size([8, 155])\n",
      "torch.Size([8, 129]) torch.Size([8, 129])\n",
      "torch.Size([8, 153]) torch.Size([8, 153])\n",
      "torch.Size([8, 143]) torch.Size([8, 143])\n",
      "torch.Size([8, 338]) torch.Size([8, 338])\n",
      "torch.Size([8, 146]) torch.Size([8, 146])\n",
      "torch.Size([8, 210]) torch.Size([8, 210])\n",
      "torch.Size([8, 220]) torch.Size([8, 220])\n",
      "torch.Size([8, 161]) torch.Size([8, 161])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 196]) torch.Size([8, 196])\n",
      "torch.Size([8, 122]) torch.Size([8, 122])\n",
      "torch.Size([8, 205]) torch.Size([8, 205])\n",
      "torch.Size([8, 162]) torch.Size([8, 162])\n",
      "torch.Size([8, 418]) torch.Size([8, 418])\n",
      "torch.Size([8, 144]) torch.Size([8, 144])\n",
      "torch.Size([8, 206]) torch.Size([8, 206])\n",
      "torch.Size([8, 289]) torch.Size([8, 289])\n",
      "torch.Size([8, 254]) torch.Size([8, 254])\n",
      "torch.Size([8, 174]) torch.Size([8, 174])\n",
      "torch.Size([8, 310]) torch.Size([8, 310])\n",
      "torch.Size([8, 420]) torch.Size([8, 420])\n",
      "torch.Size([8, 141]) torch.Size([8, 141])\n",
      "torch.Size([8, 251]) torch.Size([8, 251])\n",
      "torch.Size([8, 150]) torch.Size([8, 150])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 221]) torch.Size([8, 221])\n",
      "torch.Size([8, 223]) torch.Size([8, 223])\n",
      "torch.Size([8, 132]) torch.Size([8, 132])\n",
      "torch.Size([8, 224]) torch.Size([8, 224])\n",
      "inputs:  tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 24564,  4892,   703, 15713,  2568,   460,\n",
      "          307,   973,   284,  1176,  5682,    13,   198,   198, 21017, 18261,\n",
      "           25,   198, 26764,   413,   540,  2568,   318,   257,  3665,   290,\n",
      "        13347,  3038,   329, 43067,  5682,    13,   632, 34547,  3288,  4133,\n",
      "          588, 34488,    11,  2344,    11,   290,  1660,   284,  4439,  8744,\n",
      "          290,   460,   307,   973,   287,   257,  4996,   286,  2842,    13,\n",
      "        12347, 13043,   389,   262,   749,  6768,   973,  3038,   329,  1588,\n",
      "           12,  9888,  1363,  2568,  8136,    11,   290,   389,  6007,   286,\n",
      "        16930,   257,  1363,   290,  9194,  8744,    13,  3086, 35658,   460,\n",
      "         2148,  3424,  2568,  3227,   290,   389,  6481,   973,   287, 12420,\n",
      "         3006,    13, 32116, 31067,  2568, 34547,   262,  3288,  1176,   286,\n",
      "        18180,   284,  7716,  8744,   290,   318,  2968,   287,  3006,   351,\n",
      "        23933,  1660,  4237,    13,  9461,    11,  4903, 49723,  2568,   635,\n",
      "         3544,   262,  4894,   422,   262,  3668,   284,  7716,  2568,   290,\n",
      "          318,  5033,   517,   290,   517,  2968,   287, 12420,  5479,    13,\n",
      "        24199,    11, 15713,  2568,  4394,   257,  3424,   290,  6942,   835,\n",
      "          284,  2222,  1176,   284,  5682,    13, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256])\n",
      "targets:  tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 24564,  4892,   703, 15713,  2568,   460,   307,\n",
      "          973,   284,  1176,  5682,    13,   198,   198, 21017, 18261,    25,\n",
      "          198, 26764,   413,   540,  2568,   318,   257,  3665,   290, 13347,\n",
      "         3038,   329, 43067,  5682,    13,   632, 34547,  3288,  4133,   588,\n",
      "        34488,    11,  2344,    11,   290,  1660,   284,  4439,  8744,   290,\n",
      "          460,   307,   973,   287,   257,  4996,   286,  2842,    13, 12347,\n",
      "        13043,   389,   262,   749,  6768,   973,  3038,   329,  1588,    12,\n",
      "         9888,  1363,  2568,  8136,    11,   290,   389,  6007,   286, 16930,\n",
      "          257,  1363,   290,  9194,  8744,    13,  3086, 35658,   460,  2148,\n",
      "         3424,  2568,  3227,   290,   389,  6481,   973,   287, 12420,  3006,\n",
      "           13, 32116, 31067,  2568, 34547,   262,  3288,  1176,   286, 18180,\n",
      "          284,  7716,  8744,   290,   318,  2968,   287,  3006,   351, 23933,\n",
      "         1660,  4237,    13,  9461,    11,  4903, 49723,  2568,   635,  3544,\n",
      "          262,  4894,   422,   262,  3668,   284,  7716,  2568,   290,   318,\n",
      "         5033,   517,   290,   517,  2968,   287, 12420,  5479,    13, 24199,\n",
      "           11, 15713,  2568,  4394,   257,  3424,   290,  6942,   835,   284,\n",
      "         2222,  1176,   284,  5682,    13, 50256,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:56.581975Z",
     "start_time": "2025-06-27T02:16:54.391260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[3])\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(input_text, tokenizer),\n",
    "    max_new_tokens=50,\n",
    "    context_size=1024,\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = tensor_to_text(token_ids, tokenizer)\n",
    "print(generated_text)"
   ],
   "id": "3520f02e1e624290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the origins and history of the Internet.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Describe the origin and history of the Internet.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Describe the origin and history of the Internet.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Describe the origin and history of the\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:16:59.958754Z",
     "start_time": "2025-06-27T02:16:56.589280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import loss_loader\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "id": "bda02c08bc5dcfa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.5710490226745604\n",
      "Validation loss: 3.468023490905762\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train as normal",
   "id": "dccf9e269d59d3f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:28:20.550381Z",
     "start_time": "2025-06-27T02:16:59.972935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time\n",
    "from gpt2_v2 import train_model_simple, build_tokenizer\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_num_threads(12)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# FineTune the model\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=format_input(val_data[3]),\n",
    "    tokenizer=build_tokenizer()\n",
    ")\n",
    "\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f\"Training completed in {elapsed:.2f} minutes.\")\n"
   ],
   "id": "a297eb6b0a9efce3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000005): Train loss 2.725, Val loss 2.635, Tokens seen: 6424\n",
      "Ep 1 (Step 000010): Train loss 2.191, Val loss 2.175, Tokens seen: 14624\n",
      "Ep 1 (Step 000015): Train loss 2.028, Val loss 2.044, Tokens seen: 21104\n",
      "Ep 1 (Step 000020): Train loss 2.043, Val loss 1.969, Tokens seen: 28240\n",
      "Ep 1 (Step 000025): Train loss 1.945, Val loss 1.948, Tokens seen: 36288\n",
      "Ep 1 (Step 000030): Train loss 1.764, Val loss 1.918, Tokens seen: 44584\n",
      "Ep 1 (Step 000035): Train loss 1.882, Val loss 1.893, Tokens seen: 53184\n",
      "Ep 1 (Step 000040): Train loss 1.854, Val loss 1.895, Tokens seen: 60104\n",
      "Ep 1 (Step 000045): Train loss 1.964, Val loss 1.880, Tokens seen: 72128\n",
      "Ep 1 (Step 000050): Train loss 1.807, Val loss 1.854, Tokens seen: 81440\n",
      "Ep 1 (Step 000055): Train loss 1.738, Val loss 1.857, Tokens seen: 88864\n",
      "Ep 1 (Step 000060): Train loss 1.783, Val loss 1.859, Tokens seen: 97120\n",
      "Ep 1 (Step 000065): Train loss 1.770, Val loss 1.855, Tokens seen: 104936\n",
      "Ep 1 (Step 000070): Train loss 1.792, Val loss 1.845, Tokens seen: 112192\n",
      "Ep 1 (Step 000075): Train loss 1.819, Val loss 1.834, Tokens seen: 119536\n",
      "Ep 1 (Step 000080): Train loss 1.771, Val loss 1.830, Tokens seen: 126736\n",
      "Ep 1 (Step 000085): Train loss 1.755, Val loss 1.831, Tokens seen: 135560\n",
      "Ep 1 (Step 000090): Train loss 1.626, Val loss 1.829, Tokens seen: 144096\n",
      "Ep 1 (Step 000095): Train loss 1.659, Val loss 1.816, Tokens seen: 154272\n",
      "Ep 1 (Step 000100): Train loss 1.681, Val loss 1.808, Tokens seen: 162040\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the origins and history of the Internet.\n",
      "\n",
      "### Response:\n",
      "The Internet was invented by the internet community in the early 1800s\n",
      " Checkpoint saved: checkpoints/checkpoint_epoch1.pth\n",
      "Ep 2 (Step 000105): Train loss 1.760, Val loss 1.824, Tokens seen: 168504\n",
      "Ep 2 (Step 000110): Train loss 1.507, Val loss 1.811, Tokens seen: 174832\n",
      "Ep 2 (Step 000115): Train loss 1.556, Val loss 1.825, Tokens seen: 182232\n",
      "Ep 2 (Step 000120): Train loss 1.597, Val loss 1.816, Tokens seen: 188792\n",
      "Ep 2 (Step 000125): Train loss 1.524, Val loss 1.807, Tokens seen: 197440\n",
      "Ep 2 (Step 000130): Train loss 1.629, Val loss 1.827, Tokens seen: 205488\n",
      "Ep 2 (Step 000135): Train loss 1.613, Val loss 1.806, Tokens seen: 213760\n",
      "Ep 2 (Step 000140): Train loss 1.543, Val loss 1.813, Tokens seen: 222032\n",
      "Ep 2 (Step 000145): Train loss 1.599, Val loss 1.820, Tokens seen: 230560\n",
      "Ep 2 (Step 000150): Train loss 1.519, Val loss 1.817, Tokens seen: 240576\n",
      "Ep 2 (Step 000155): Train loss 1.450, Val loss 1.818, Tokens seen: 248984\n",
      "Ep 2 (Step 000160): Train loss 1.617, Val loss 1.799, Tokens seen: 256312\n",
      "Ep 2 (Step 000165): Train loss 1.541, Val loss 1.808, Tokens seen: 264560\n",
      "Ep 2 (Step 000170): Train loss 1.605, Val loss 1.794, Tokens seen: 271744\n",
      "Ep 2 (Step 000175): Train loss 1.453, Val loss 1.801, Tokens seen: 280280\n",
      "Ep 2 (Step 000180): Train loss 1.524, Val loss 1.806, Tokens seen: 286704\n",
      "Ep 2 (Step 000185): Train loss 1.457, Val loss 1.791, Tokens seen: 297816\n",
      "Ep 2 (Step 000190): Train loss 1.414, Val loss 1.794, Tokens seen: 303968\n",
      "Ep 2 (Step 000195): Train loss 1.484, Val loss 1.799, Tokens seen: 313272\n",
      "Ep 2 (Step 000200): Train loss 1.499, Val loss 1.804, Tokens seen: 320096\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the origins and history of the Internet.\n",
      "\n",
      "### Response:\n",
      "The Internet was invented by the internet community in the early 1800s\n",
      " Checkpoint saved: checkpoints/checkpoint_epoch2.pth\n",
      " Training complete. Final model saved: checkpoints/final_model.pth\n",
      "Training completed in 11.32 minutes.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:28:21.069690Z",
     "start_time": "2025-06-27T02:28:20.568007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ],
   "id": "fe49e1158904c239",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAEiCAYAAAACr1D/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABcxUlEQVR4nO2dB1gUVxeGP3pRQEAQEBEUe8GOLWqisSW2aGyJPTGaqDG2xBQ1iYlGozHFmBjbb9Ro7Mbee+8VrIg0AZWmdOZ/zll3WRAUabsL532e687szM7e2V357j33FCNFURQIgiAIgmCQGOu6A4IgCIIg5B4RckEQBEEwYETIBUEQBMGAESEXBEEQBANGhFwQBEEQDBgRckEQBEEwYETIBUEQBMGAESEXBEEQBANGhFwQBEEQDBgRckEoRgQEBMDIyAjnz5/XdVcEQcgnRMgFwcAgIX5emzJliq67KAhCIWJamG8mCELeCQ0N1WyvWrUKkyZNgr+/v+a5kiVL6qhngiDoApmRC4KB4eLioml2dnY8C1fvOzs7Y/bs2XB3d4eFhQXq1KmD7du3Z3ut1NRUDB48GFWrVkVgYCA/t3HjRtSrVw+WlpaoUKECvv76a6SkpGheQ++3YMECdOvWDdbW1qhUqRI2bdqkOf7o0SO88847cHJygpWVFR9fvHhxtn1Ys2YNatWqxec6OjqiTZs2ePz4seY4vVe1atW4P9TP33//PcPr7927h549e6JUqVJwcHBAly5deAlBzcCBA9G1a1f8+OOPcHV15ff46KOPkJycnItPXxD0EKp+JgiCYbJ48WLFzs5Osz979mzF1tZW+eeffxQ/Pz9lwoQJipmZmXL9+nU+fufOHap2qJw7d05JSEhQunXrptStW1cJDw/n4wcPHuTXL1myRLl165ayc+dOxdPTU5kyZYrmPej17u7uyooVK5QbN24oo0aNUkqWLKk8ePCAj3/00UdKnTp1lFOnTvH77dq1S9m0aVOW/Q8JCVFMTU2533TuxYsXlblz5yqxsbF8fNmyZYqrq6uydu1a5fbt2/zo4ODA/SOSkpKUatWqKYMHD+bXXr16Venbt69SpUoVJTExkc8ZMGAA39OwYcOUa9euKf/9959ibW2tzJ8/v8C+F0EoTETIBaEICbmbm5vy3XffZTinYcOGyocffphByA8dOqS0bt1aad68uRIVFaU5l577/vvvM7z+77//ZjFVQ6//8ssvNftxcXH83LZt23i/U6dOyqBBg3LU/zNnzvBrAwICsjxesWJFHjBo8+233ypNmjTR9I1EOy0tTXOcBNzKykrZsWOHRsjLly+vpKSkaM55++23lV69euWoj4Kg78gauSAUEWJiYhASEoJmzZpleJ72L1y4kOG5Pn36sPl97969bNJWQ+cdOXIE3333XQbze0JCAp48ecKmdKJ27dqa4yVKlICtrS3Cw8N5f/jw4ejevTvOnj2Ltm3bslm7adOmWfbZx8cHrVu3ZtN6u3bt+PwePXrA3t6ezeu3bt3CkCFD8P7772teQ2Z+WlJQ9/fmzZuwsbHJcF3qL71WTY0aNWBiYqLZJxP7pUuXcvzZCoI+I0IuCMWQjh07YtmyZTh27Bhee+01zfNxcXG8Jv7WW2898xpao1ZjZmaW4Ritm6elpfF2hw4dcPfuXWzduhW7du1ioaY1aVqjzgyJK51z9OhR7Ny5E7/++iu++OILnDhxQjNo+Ouvv+Dr6/vM69T9rV+/PpYvX/7MtWmNPif9FQRDR4RcEIoINCt2c3PjGXXLli01z9N+o0aNMpxLs+aaNWuic+fO2LJli+Z8cnIjD3hvb+889YVEdMCAAdxeeeUVjB8/PkshV4sqWQ2okQd++fLlsX79eowZM4bv5/bt2+w8lxXUX/LcJyc/un9BKI6IkAtCEYIEc/LkyahYsSJ7rJO3OCV/yWrGOnLkSDabv/nmm9i2bRuaN2/OQkr7Hh4ebOI2NjZm8/Xly5cxderUHPWBrkGzZDJnJyYmYvPmzex1nhU0896zZw+b1EmMaT8iIkJzPlkHRo0axab09u3b8/VOnz7NnvEk9CTwM2fOZE/1b775hpcLyBqwbt06TJgwgfcFoagjQi4IRQgSvejoaIwdO5bXrKtXr86hYRQClhWjR49mEzOZ2ilMjdapSXhJFH/44Qc2SVPI13vvvZfjPpibm2PixIkcAkbr7zQjX7lyZZbn0iz64MGDmDNnDq/x02x81qxZbJ4n6H3JxE5iTYMUWo+n9XTqN0HH6PWffvopLwfExsaibNmybM6XGbpQXDAijzddd0IQBEEQhNwhCWEEQRAEwYARIRcEQRAEA0aEXBAEQRAMGBFyQRAEQTBgRMgFQRAEwYARIRcEQRAEA0aEPI/MnTsXnp6enL6S0kiePHkS+saUKVM4e5Z2o9hg7bzUlEKTyjtSLWvKk33//v0M16ASl2+88QbH7VLiDorp1S5tSezfv58zbVH5TMoMtmTJkkL7vCiWuFOnTpwJjO5vw4YNGY5TlCUlKqEc2xTbTKUyb9y4keGchw8fcoIRij+mkpiU45tSgGpz8eJFjoum/pcrVw4zZsx4pi+rV6/mz5fOoZhnSlX6sn3J6/1S6c7M3zklVDHU+yWmTZuGhg0bcl51+g1SDnftOuz69lvOSV/yer+tWrV65nseNmyYQd4vMW/ePM7jT79Jak2aNOFkRS/zHoEGdL/5hq6rthgyK1euVMzNzZVFixYpV65cUd5//32lVKlSyv379xV9YvLkyUqNGjWU0NBQTYuIiNAcp/KO5cqVU/bs2aOcPn1aady4sdK0aVPNcaoaVbNmTaVNmzZc/nLr1q1K6dKllYkTJ2rOoRKTVBpyzJgxXEry119/VUxMTJTt27cXyudFffriiy+UdevWcTWt9evXZzg+ffp0rhK2YcMG5cKFC0rnzp0VLy8vJT4+XnNO+/btFR8fH+X48eNcHczb21vp06eP5nh0dLRSpkwZ5Z133lEuX77MpUKpytaff/6pOefIkSN83zNmzODPgaqEURnRS5cuvVRf8nq/VPGL7kf7O3/48GGGcwzpfol27dpxtTfqy/nz55WOHTsqHh4eXH1NH3/LL+pLftxvy5Yt+b21v2f63gzxfgkqd7tlyxYuu+vv7698/vnn/HuizyAn75FiYPebX4iQ54FGjRpx7WU1qampXEZy2rRpir4JOf3BzgoqYUn/UVavXq15jmo2kzgcO3aM9+k/g7GxsRIWFqY5Z968eVzjWV3zmepe02BBGyoTSX+MCvvzyixsVOLSxcVFmTlzZob7trCwYHEi6D80vY5qaKuhspxGRkZKcHAw7//++++Kvb295p6JTz/9lMtoqunZs6fyxhtvZOiPr6+v8sEHH+S4L3m9X7WQd+nSJdvXGPL9qqEa6nQPBw4c0Lvfck76ktf7VQv5xx9/nO1rDPl+1dBvcMGCBUX++80LYlrPJUlJSThz5gybCdVQXmrap4pS+gaZMskMW6FCBTankvmJoHtITk7OcB9kJqVc2+r7oEcymZYpU0ZzDqXypJSaV65c0ZyjfQ31Oepr6PLzunPnDsLCwjK8N+XuJnOZ9j2SeblBgwaac+h86iPl/1af06JFC05Bqn2PZO6k3N85+Rxy0pf8gsyHZFqsUqUKF0l58OCB5lhRuF9KRUs4ODjo3W85J33J6/2qoTz6pUuX5iI4lBqXys2qMeT7pToAlNqXytmSib2of795QXKt55LIyEj+oWn/YAja9/Pzgz5BfzRpDYj+oIeGhnIhClr3pEIY9EeW/lDTH/XM90HHCHrM6j7Vx553Dv0Hio+P5z/8uvq81H3M6r21+0+ip42pqSn/0dQ+x8vL65lrqI9RDe3sPgfta7yoL/kBrYdT7nHqL9Xl/vzzzzl/Of2RoRKghn6/lB+e8q1TxTQSMPV76ctvOSd9yev9En379uX89DRIJ38GyjlPAy0qGmOo90t14km4aQ2a1p6pEh7VDKDiP0X1+80rIuTFAHUBCoIcSUjY6T//v//+y85HQtGjd+/emm2aodD3ThXRaJZOBUUMHXIyooHo4cOHURzI7n6HDh2a4Xsmh0L6fmnwRt+3IUITDhJtskCsWbOGS+EeOHBA193Sa8S0nkvIlEUzm8xeirTv4uICfYZGkZUrV8bNmze5r2RKioqKyvY+6DGr+1Qfe9455HlKgwVdfl7q6z/vvemRqoVpQ56u5NmdH5+D9vEX9aUgoCUV+g7oOzf0+x0xYgRXaNu3b1+GMqX69FvOSV/yer9ZQYN0Qvt7NrT7pZkueZJTKVzy3Pfx8cHPP/9cZL/f/ECEPJfQj41+aFRLWdv8RftkFtJnKMSIRuw0eqd7oFKV2vdBpjlaQ1ffBz2SuUv7D/+uXbv4h08mL/U52tdQn6O+hi4/LzIP038u7fcmMxqtBWvfI/2npLUvNXv37uU+qv840jkU9kVrY9r3SDMIMjPn5HPISV8KgqCgIF4jp+/cUO+X/PpI1MjUSn3NbPbXp99yTvqS1/vNCprJEtrfs6Hcb3bQe1Ed+qL2/eYreXKVK+ZQiAJ53y5ZsoS9gIcOHcohCtoek/rA2LFjlf379yt37tzhcCEKzaCQDPKCVYdRUFjL3r17OYyiSZMm3DKHdLRt25bDYChMw8nJKcuQjvHjx7P35ty5c7MM6Siozys2NpbDTajRz3r27Nm8fffuXU0IFL3Xxo0blYsXL7JHd1bhZ3Xr1lVOnDihHD58WKlUqVKGcCzyVKVwrH79+nE4DN0P3XPmcCxTU1Plxx9/5M+BIgayCsd6UV/ycr90bNy4cew9S9/57t27lXr16vH9JCQkGOT9EsOHD+cwNvota4dbPXnyRHOOPv2WX9SXvN7vzZs3lW+++YavTd8zfb4VKlRQWrRoYZD3S3z22WfslU/3Q78V2qdIip07d+boPVIM7H7zCxHyPEIxiPRlUswhhSxQTK6+QaEVrq6u3MeyZcvyPv0RUEN/UD/88EMO86AfeLdu3fgPhjYBAQFKhw4dOI6YBgE0OEhOTs5wzr59+5Q6derw+9AfFIqBLazPi96bBC1zozAsdRjUV199xcJE/0Fbt27NcaraPHjwgIWsZMmSHK4yaNAgFkVtKA66efPmfA36LEmkMvPvv/8qlStX5nukMBeKi9UmJ33Jy/3SH3r6Q0Z/wEhUy5cvz3GwmQdMhnS/RFb3S037d6ZPv+Wc9CUv9xsYGMii7eDgwJ8r5QEgcdKOIzek+yUGDx7Mv1d6D/r90m9FLeI5fY8AA7rf/MKI/snfOb4gCIIgCIWFrJELgiAIggEjQi4IgiAIBowIuSAIgiAYMCLkgiAIgmDAiJALgiAIggEjQi4IgiAIBowIeR6hjENTpkzhx+KC3HPRp7jdLyH3XPRJLKL3K3HkeYTSTVJZRkrwT2kAiwNyz0X/novb/RJyz0X/nmOK6P3KjFwQBEEQDBgRckEQBEEwYKQeeRZQOcdz585xkXhj4+ePdWJjY/kxODiYzTbFAbnnon/Pxe1+Cbnnon/PsQZ2v1R1jUqj1q1bF6am2cu1rJFnwalTp9CoUSNdd0MQBEEQcPLkSTRs2FA/Z+RUNH7dunXw8/Pjgu5NmzbFDz/8wPWOs6NVq1Y4cODAM8937NgRW7Zs4e2BAwfif//7X4bj7dq1w/bt23PUL5qJqz88dV1fQRAEQShMQkNDeVKp1iS9FHIS5I8++ohHGmTO/vzzz9G2bVtcvXoVJUqUyPI1JPxJSUma/QcPHsDHxwdvv/12hvPat2+PxYsXa/YtLCxy3C+1OZ1E3N3dPRd3JgiCIAj5w4uWeHUq5JlnyEuWLIGzszPOnDmDFi1aZPkaBweHDPsrV66EtbX1M0JOwu3i4lIAvRYEQRAE/UGvvNYpti8rsX4eCxcuRO/evZ+Zwe/fv58HBWSmHz58OM/cBUEQBKGoYapP3nmjR49Gs2bNULNmzRy9htawL1++zGKe2az+1ltvwcvLC7du3WKTfYcOHXDs2DGYmJg8cx3K8qOd6Uft2SgIgiAI+o7eCDmtlZMoHz58OMevIQGvVavWMx7mNENXQ8dr166NihUr8iy9devWWTrdff3113m8A0EQhMInNTUVycnJuu6GkAvMzMyynFwapJCPGDECmzdvxsGDB3PsXPb48WNeH//mm29eeG6FChVQunRp3Lx5M0shnzhxIsaMGaPZpxjD6tWrI68kp6bhYlA0gh49QZc6ZfN8PUEQBDUUORwWFoaoqChdd0XIA6VKlWJ/LiMjI8MUcvohjhw5EuvXr+fZMpnCc8rq1avZHP7uu+++8NygoCBeI88ulIwc47S92vMrUQAJefd5R3m7ZWUnlLI2z5frCoIgqEWcfIHI4TcvQiDoRv+ePHmC8PBw3s9LqLOprs3pK1aswMaNG2FjY8M/TIKS2lNcOdG/f3+ULVuWzd+Zzepdu3aFo6Njhufj4uLYTN69e3ce5dAa+YQJE+Dt7c2x5IWJtbkpytha4H5MIgIePEEdEXJBEPLJnK4W8cx/AwXDQa1zJOb0XebWzK5Tr/V58+axpzoleaHRiLqtWrVKc05gYCAHxWvj7+/Pa+lDhgx55pr0QVy8eBGdO3dG5cqV+Zz69evj0KFDLxVLnl94Oqq86e8+eFzo7y0IQtFEvSZOM3HBsFF/h3nxc9C5af1FkMk9MxRSlt1raYSzY8cO6Ask5CfuPERA5BNdd0UQhCKGmNMNn/z4DvUqjrwoUr60arQVIDNyQRAEoQAQIS8k07oIuSAIQsHg6emJOXPm6PwaukKEvIAp76iakd99IKZ1QRCKN2RGfl6bMmVKritWDh06FMUVvYgjL8qUfzojf/g4CdHxybCzMtN1lwRBEHSCtuMyOTVPmjSJnZfVlCxZUrNNflDknf+8OtxqnJycUJyRGXkBU9LCFKVLqrzlA2VWLghCMYZCgtWNwoxpFq7ep3LWFIa8bds2jjSiKCOKTqIQ4i5dunApTxJ6qpa5e/fu55rFjYyMsGDBAnTr1o29witVqoRNmza9VF8pYorel97T1tYWPXv2xP379zXHL1y4gFdffZX7TMepz6dPn+Zjd+/eRadOnWBvb891QGrUqIGtW7eioJAZeSHgVdoakXEUS/4YtdztdN0dQRCKIDSDjU9O1cl7W5mZ5JsH/WeffYYff/yRM3KSEN67dw8dO3bEd999x+K+dOlSFkmayXt4eGR7HconMmPGDMycORO//vor3nnnHRbYnBTlotofahGncttUZpvynvTq1UsTSUXXq1u3LodRU9jz+fPnOeUqQedSuW3KVkpCTqW5ta0N+Y0IeSGZ108FPEJApDi8CYJQMJCIV5+km9Dbq9+04wRY+QGl3X799dc1+yS8Pj4+mv1vv/2Ws4HSDJvSe2fHwIED0adPH97+/vvv8csvv3ChLSqq9SL27NmDS5cu4c6dOyhXrhw/RwMImlnTejxZBWjGPn78eFStWpWP06xfDR2jpGRU64OgQUlBIqb1QsDzqcMbZXcTBEEQsqdBgwbPZOscN24cqlWrxnnJaWZ77do1FsvnUbt2bc02zYrJ/K1Oh/oi6Pok4GoRJ6j+Br0/HSOoPsd7772HNm3aYPr06bwEoGbUqFGYOnUqV/OcPHkyJykrSGRGXtCcmI9Od0/gf3gVdx/Y67o3giAUUci8TTNjXb13fkGiqw2J+K5du9jcTqm2KelXjx492HT9PMyemrnVkOmfTOb5BXnY9+3bF1u2bOF1fRJsKuRF6/Ik8JQSnI7t3LmTU4zPmjWLa4sUBCLkBc3phSgf4YdqxhVx9YGLrnsjCEIRhYQqv8zb+sSRI0fYTE4CqZ6hBwQEFOh7VqtWjdfmqaln5bTOTfnttStjUhpwap988gmb8RcvXqzpJ71u2LBh3KjC5l9//VVgQi6m9YLGqQo/VDIKZoe3uMQUXfdIEATBYKC153Xr1rEzGXmK0yw4P2fWWUHmclrfJoe2s2fP8to6FfBq2bIlm/7j4+N5fZ4c38iBjgYbtHZOAwBi9OjRnCqc1tjp9fv27dMcKwhEyAsaJ5UjRA1zVfykFE8RBEHIObNnz2bv9aZNm7K3Opms69WrV+DWjY0bN/L7tmjRgoWdHNbUBb3IS51KY5O404ycQtM6dOjAnvIExb+T5zqJNznX0Tm///57wfVXyUnlkmIG1S8nswiZVdzd3fN2sctrgTWD4WdWDe1jv8Lv79RDx1q5rzsrCIKQkJDAsz0vLy9YWlrqujtCAX2XOdUimZEX0ozcI/UeRXrijoSgCYIgCPmICHlB4+gNGBnDOi0OTogS07ogCIKQr4iQFzSmFoCDKhmAl1GYxJILgiAI+UrRi1XQR/r+i4tRFjj51wWUkRm5IAiCkI+IkBcGjhXhYaVKXnA/JhFPklKKZLynIAiCUPiIab2QKGVtjlLWqkxDgQ/FvC4IgiDkDyLkhcGTh8CWsZhv+iPvSvEUQRAEIb8Q+25hYGoJnFqIRlDggBhxeBMEQRCKxoycEslTOTgqzO7s7IyuXbtyjdnnsWTJEs66o90yB9FTjptJkybB1dWVE+xTVp4bN25AZ5hbA/blebOycZCEoAmCIAhFQ8ipYDulsTt+/DhXt0lOTkbbtm3x+PHzhY7K0YWGhmoa5brVhorJU+3ZP/74AydOnOBqOpTWjzLo6DoxjLdRMAIiZUYuCIKQW1q1asX5zJ9XmaxOnTooLujUtL59+/ZnZts0Mz9z5gznt80OmoW7uGRdSYxm43PmzMGXX36JLl26aArClylTBhs2bEDv3r2hs+Ip17ejklEQ9sqMXBCEYgjlSqcJW+a//cShQ4f47z4VRtGuJS4YmLNbdHQ0Pzo4ODz3PCpjV758ec5BS2J95coVzTHKWRsWFsbmdDV2dnbw9fXFsWPHoOsZOVVBC4lOQEJyqu76IgiCoAOGDBnC1lfKIZ4ZKgFKlcVExA1YyKksHZlKmjVrhpo1a2Z7XpUqVbBo0SKuTLNs2TJ+HVXFUf8wSMQJmoFrQ/vqY5lJTExETEyMpsXGxqKgyplWNgnmx3sSgiYIQjHjzTffhJOTE1tfM0/OVq9ezUJPVcWotnfZsmVhbW3N5UT/+eefPL1vWloavvnmGy48YmFhwWZ3batAUlISlyUlvyryuaKJIvlwqa28ZKr38PDg17q5uWHUqFHQJ/TGa53Wyi9fvozDhw8/97wmTZpwU0MiTqXi/vzzT3z77be5em/6wtTl5wqM0iohN6a860jg4imVytgU7HsKglD8SHrO0p2RCWBmmcNzjQEzqxefa14ix10zNTXl0p8k5F988QUvkxIk4lT6kwScRL1+/fr49NNP2R9qy5Yt6NevHypWrIhGjRohN/z888+YNWsW60TdunV5Mti5c2e25lK9c/Kp2rRpE/79918WbKo2Ro1Yu3YtfvrpJ6xcuRI1atTgCSGZ//UJvRByGglt3rwZBw8efOmyoWZmZvzF3Lx5k/fVa+f379/n0ZUa2s/O+WHixIkYM2aMZj84OBjVq1dHvmJREvj0Liatv40nF0NxV0LQBEEoCL53y/5YpbbAO6vT92d6A8nZ/C0q3xwYtCV9f04t4MmDZ8+boloSzSmDBw/GzJkz2dmZnNbUZvXu3bvzMii1cePGac4fOXIkduzYwSKbWyH/8ccfeWCg9pH64YcfsG/fPvanmjt3LgIDA1nQmzdvzoMLmpGroWOkK7RcS3pDQp/bfhRJ0zqZLEjE169fj71793I91peFRnGXLl3SiDZdgz70PXv2aM4hczl5r2vP5LUhcwmN/NSNwuEKBKtS8HRUjV4DxOFNEIRiSNWqVdmSSrNigiZh5OhGZnX133SyrpJJnfylSpYsyUJOgpobYmJiEBISwsu22tD+tWvXeHvgwIE4f/48L92S2Xznzp2a895++23Ex8ejQoUKeP/991mvUlJSoE+Y6tqcvmLFCl7vJvFUr2HTiIzivwkyw9BaiXq9gtY5GjduDG9vb0RFRfHIjsLP3nvvPT5Ooylaa586dSqPsEjYv/rqK17XoDh1XVPe0ZofZUYuCEKB8HnI803r2oy/+ZxzM83zRl9CfkGiTTNtmg3TbJzM5i1btuRj9DedTOE0WyYxp/Bh+ptO69gFRb169dhRetu2bdi9ezd69uzJM/A1a9awUzXlN6HnyVHvww8/1FgUaIaO4i7k8+bN40e1eUUNfbE0QiJoFGZsnP6DevToEY+KSPTt7e15LeXo0aMZTOETJkzgWPShQ4ey2JO5hBwbMieOKXSCTqPt2a9hYpaG2Q/G6rYvgiAUTV5izbrAzn0BJJQff/wxT+QoPHj48OGa9fIjR45wNNK7776rcVS7fv16rpc7bW1teSJH11UPFtTvo20ip/N69erFrUePHmjfvj0ePnzIVgGaWFLoHDWagJJVgSzBNABAcRdyMq2/iP3792fYJ6cDas+DfhA0c6emVyhpsAs5iKbG9hgXFY/ElFRYmGYaIQuCIBRxyFxOgkn+SWT6Vk/cCLKk0kyYJmg0WZs9ezb7OOXFb2n8+PGYPHkyz/zJV4omi2RKX758OR+n96DlWfK3ookjOd/REm2pUqXYMY/M/RTCTF70FC1Fwq69jq5r9MLZrdjwNATNxegRSiqPEfQoHhWdSuq6V4IgCIUOmdcXLlyIjh078oxZDSXzun37NmfjJOEkyyoti6rzjOSGUaNG8evHjh2L8PBwHhSQlzoNGgha2qWMoJTK28TEhFOHb926lUWdxHz69OnsEE2CTub+//77D46OjtAXjJScTIuLGRSTTusiFH7wsl70L2RWNSA2BG8lTsFH/fuidbWM8e6CIAgvgtJN05ou+QDpfMlQKLDvMqdapDcJYYrbrNzbOFiqoAmCIAh5RoRch6lapQqaIAiCkFdEyHU0Iychlxm5IAiCkFdEyAsbp6pIMbfDY1jIjFwQBEHIMyLkhY1HY0R+6I+Pkkez13pyapqueyQIgiAYMCLkhY2REcrYWcLSzBipaQqLuSAIQm6gZCmCYZMf36HEkesASlhDOdf9wmI457pX6fzLmCQIQtHH3NycY5wphziVBaV9dWY0wTCgyG9KOxsREcHfJX2HuUWEXBecXoSlcTOxxrQB7kbWAFT+b4IgCDmC/vBT3HFoaCiLuWC4UNIbqqimnYr8ZREh1wVKGpxTQlDZ6B4Oi+e6IAi5gGZwJABUiYsyjgmGB2WRoxrtebWmiJDrOJZ8uXiuC4KQS0gAqAKXvlThEnSDOLvpUMjLGUUgLPKRrnsjCIIgGDAi5LqgRGmkWjnA2EiBedRNpEgImiAIgpBLRMh1hPHTWbmnEoSQqARdd0cQBEEwUETIdYSRs0rIKxsHcQiaIAiCIOQGEXJd4eqDO+ZVEKGUklStgiAIQq4RIdcV9Qdiee0lWJLaXoqnCIIgCLlGhFyHlH+a0U1m5IIgCEJuESHXIZ6O1jBFCoIjo3XdFUEQBMFAESHXIfVOj8c1i0Go+ugAF1ARBEEQhJdFhFyHWFnbwMwoFZ64h9BoqYImCIIgGJiQT5s2DQ0bNoSNjQ2cnZ3RtWtX+Pv7P/c1f/31F1555RXY29tza9OmDU6ePJnhnIEDB3LqQu3Wvn176BvGzumpWu+Kw5sgCIJgaEJ+4MABfPTRRzh+/Dh27dqF5ORktG3bFo8fZ+/8tX//fvTp0wf79u3DsWPHUK5cOX5NcHBwhvNIuKkykLr9888/0DucqmiEXGLJBUEQBIMrmrJ9+/YM+0uWLOGZ+ZkzZ9CiRYssX7N8+fIM+wsWLMDatWuxZ88e9O/fX/O8hYUFXFxcoNc4VeMHT6MwrI0QhzdBEATBwNfIo6NVYubg4JDj1zx58oRn8plfQzN3GhRUqVIFw4cPx4MHD7K9RmJiImJiYjQtNjYWhYKtG5JMSvA6eXzY9cJ5T0EQBKFIoTdCnpaWhtGjR6NZs2aoWbNmjl/36aefws3NjdfKtc3qS5cu5Vn6Dz/8wCb8Dh06ZFuzl9bq7ezsNK169eooFIyMkFDKmzdNH94onPcUBEEQihR6U4+c1sovX76Mw4cP5/g106dPx8qVK3n2bWlpqXm+d+/emu1atWqhdu3aqFixIp/XunXrZ64zceJEjBkzRrNP6+2FJeapFdtgU7g1/GIskJamwNg4bwXmBUEQhOKFXszIR4wYgc2bN7MDm7u7e45e8+OPP7KQ79y5k4X6eVSoUAGlS5fGzZs3szxO6+m2traaRl70hYVNuy8xJnUUjqRUwf1YqYImCIIgGJCQK4rCIr5+/Xrs3bsXXl5eOXrdjBkz8O2337KzXIMGDV54flBQEK+Ru7q6Qt8wNTGGu70VbwdESgiaIAiCYEBCTub0ZcuWYcWKFTwLDgsL4xYfn54chTzRyfSthta8v/rqKyxatAienp6a18TFxfFxehw/fjyHtAUEBPA6eZcuXeDt7Y127dpBH/F0sISH0X0ERsbouiuCIAiCgaFTIZ83bx57qrdq1Ypny+q2atUqzTmBgYEcB679mqSkJPTo0SPDa8jUTpiYmODixYvo3LkzKleujCFDhqB+/fo4dOgQm9D1DkXB3NDeOGjxCaKCxeFNEARBMCBnNzKtvwhyUNOGZtnPw8rKCjt27IDBYGSEeCs3lIiNghJ+DUBbXfdIEARBMCD0wtmtuJPsWIkfTR9ez9HgRhAEQRDUiJDrAXYetfjRMf4Ojt3KPnGNIAiCIGRGhFwPsHarwY8+Rrfwx35ZJxcEQRByjgi5PuDRGGnmNqhgHIYyd9bhSojkXRcEQRByhgi5PmDtAOOWE3hzmMl/mL8/68Q1giAIgpAvQn7v3j1OsqKG6oFTnvT58+fn5nIC4TsMET7D0TvpK2y+fB/3HkpyGEEQBKGAhLxv376cTpWgZCyvv/46i/kXX3yBb775JjeXFEzN4dRtOqpW8kZqmoIFh27rukeCIAhCURVyKm7SqFEj3v7333+5WtnRo0e5VjjVFBdyz7CWFfnxwulDePg4SdfdEQRBEIqikFP9b3WWtN27d3MWNaJq1aoZsrAJL0/TCvZYYfsrNph8hj1b1+i6O4IgCEJRFPIaNWrgjz/+4LSnu3bt4vrfREhICBwdHfO7j8UKI2MTuLmrisf4XJmOJwmJuu6SIAiCUNSEnAqX/Pnnn5wjvU+fPvDx8eHnN23apDG5C7nHvdu3iEFJVMZdnN/4i667IwiCIBS1XOsk4JGRkYiJiYG9vb3m+aFDh8La2jo/+1csMbUpDb9qI9Do2nRUu/YLUh6/B9MS6Z+zIAiCIORpRk5lRhMTEzUifvfuXcyZMwf+/v5wdnbOzSWFTNTu8gluwx32iEHAusm67o4gCIJQlISc6nsvXbqUt6OiouDr64tZs2aha9euXGZUyDuWlpa4VPNT3va8tQxKxHVdd0kQBEEoKkJ+9uxZvPLKK7y9Zs0alClThmflJO6//CJruvlFy469sU+ph9A0B5z3v6Xr7giCIAhFRcifPHkCGxsb3t65cyfeeustGBsbo3HjxizoQv5QytocZ3y+RZukmZh5TdbIBUEQhHwScm9vb2zYsIFTte7YsQNt27bl58PDw2Fra5ubSwrZ0Oe1+kg1tsDRWw9wMShK190RBEEQioKQT5o0CePGjYOnpyeHmzVp0kQzO69bt25+97FYU7aUFTr7uMEEqbi68SfgrMo3QRAEQRByHX7Wo0cPNG/enLO4qWPIidatW6Nbt27yyeYzQ1tWQMqFf9E7Yi5Sd9jBpMobQAlJvCMIgiDkUsgJFxcXbuoqaO7u7pIMpoCo6mKLOO9OuBqwGdUT7wIbhgO9/gZMVWlyw2MTcCU4BpeDo3E5JBqXg2MQm5CM+f0boHEFEXxBEISiTK6EPC0tDVOnTuWQs7i4OH6OnN/Gjh3LFdDI8U3IX4a2qowv/xqEFebfwfLGDtye2w3Tbb/A+ZB4hMdmncZ1xIqz2DLqFZSxtSz0/gqCIAiFQ64Ul8T6t99+w/Tp03Hu3Dlu33//PX799Vd89dVXOb7OtGnT0LBhQx4EUCIZikOnpDIvYvXq1VyghWKta9Wqha1bt2Y4rigKr+O7urrCysoKbdq0wY0bN2DI+Ho5IM29EYYkj0OCYoYKj46g9+3PER0bC2MjoJJzSXSrWxZfvlENK97zRVUXG0TGJbGYJ6em6br7giAIQkGh5AJXV1dl48aNzzy/YcMGxc3NLcfXadeunbJ48WLl8uXLyvnz55WOHTsqHh4eSlxcXLavOXLkiGJiYqLMmDFDuXr1qvLll18qZmZmyqVLlzTnTJ8+XbGzs+P+XLhwQencubPi5eWlxMfH56hf9+7dU+ijoUd94titSKX+t7uUT2f+qiROcVKUybbKowXdlMcJSc+cezsiTqk5abtS/tPNytTNV3TSX0EQBCH35FSLciXkFhYWir+//zPP+/n5KZaWlkpuCQ8P504fOHAg23N69uypvPHGGxme8/X1VT744APeTktLU1xcXJSZM2dqjkdFRXGf//nnH4MW8gzcPqgo35dTlCsbsj1l26UQFnJqtC0IgiAYDjnVolyZ1slTnUzrmaHnateunWvrQHR0ND86ODhke86xY8fYVK5Nu3bt+Hnizp07CAsLy3COnZ0dp5FVn1Mk8HoFGH0BqN4l21Pa13TF+6+oSqKOW30RtyNU/gyCIAhCMXd2mzFjBt544w3s3r1bE0NOIkkJYjKvV7+MA93o0aPRrFkz1KxZM9vzSKQpJaw2tE/Pq4+rn8vunMxQARhqamJjY2EQWGlle4sKBPZPBzr+CJinV6Cb0L4qzt+LwqmAR/hw+Vms/7AZrMxNdNNfQRAEId/J1Yy8ZcuWuH79OseMU9EUapSm9cqVK/j7779z1ZGPPvoIly9fxsqVK1HYkNMdzdrVrXr16jAo0tKAf/oC55cDK3oCSY81h8xMjPFb33ooXdICfmGx+GLDJXYGFARBEIoGuY4Tc3Nzw3fffYe1a9dyo3C0R48eYeHChS99rREjRmDz5s3Yt28fx6M/D4pdv3//fobnaJ+eVx9XP5fdOZmZOHEim/XV7erVqzAoKNzvjVmAuQ0QcAhY3hNITDejU/jZr33qsnf7urPB+OfkPZ12VxAEQcg/dBrwTTNDEvH169dj79698PJSrec+DzLl79mzJ8Nzu3bt0pj46Rok2NrnxMTE4MSJE5pzMmNhYcE54tVNXRDGoPDwBfqtByxsgbuHgWXdgdsHgJQkPtykoiPGt6vK21M2XcGlIJU/giAIgmDY6FTIyZy+bNkyrFixgsWT1rCpxcfHa87p378/z5jVfPzxx9i+fTsno/Hz88OUKVNw+vRpHhAQRkZGvNZOFoJNmzbh0qVLfA2yIFCcepGmXMN0Mb93HFjaGZhTS2V6BzCsZQW0qVYGSalpGL78DKKeqERe36AB3tTNV/HB36eRkJyq6+4IgiDoNToV8nnz5rEpu1WrVpy8Rd1WrVqlOScwMJBzuqtp2rQpC//8+fPZe57qoVMlNm0HuQkTJmDkyJEYOnQoJ5yh7HMk/pRApsjj3gAYvAPw6QOUcALK1leZ3p8OcuYZ/4DpJVfBI/oUxq08hbQ0/VsvJ/P/gsN3sOPKfaw8Gajr7giCIOg1RhSDltOTyaHteZDT24EDB5CaatizKMofX65cOfbCf9GavV5DM/HE6HTv9oe3gV/Sq9M9ViwQ6tQM3u0+BCq+Bhjr3ps9JCoe7X46iNjEFN53tbPEgfGvwtxU0v4KglC8CMqhFr1U+Bl5dL/oOJmxBT2BZuLaIWolnIEei4AbuxHvtwMlEh/AO3IvsHwvYFcOeP0boObzB2sFCVkHJqy5yCLuU64UQqPiERqdgPXngtCroYfO+iUIgqDPvJSQL168uOB6IhQ8FiWBmt25WaWlYdL8FfAM+g99LI7AKvoeYGaVfi6FsJlYACa5LpD30iw7cReHb0bC0swYP/X0wZ5r4fhu6zX8vv8Wutdzh6mJzMoFQRAyI38ZiyvGxmjV6nV8k9IfLVLnIanLX4D36+nHD/+kcpTb9z0QVfDhanciH2PaVj/e/qx9VVRwKom+vh6wtzbD3QdPsOVSup+EIAiCkI4IeTGmZWVnlHOwQkSCMdanNE6ffZPbhP82IDYEOPCDStCXdgXO/A948jDf+5GapmDc6guIT05F04qO6N/Ek58vYWGKwc1UIYlz993US8c8QRAEXSNCXowxMTbCO77leXvpsbvpGd+MjID39wLdFwKer5CyA7f3Af+NAmZ6A+uG5ms/5h+8jTN3H6GkhSlm9KgNY8pc85T+TT1hY2GK6/fjsOtaxiQ/giAIggh5sadng3LsEX4lJAZnA6PSD5haALV6AAM3A6POAa0nAS61ACUVMC+R0TP+wiog/lGu3t8vLAY/7brO25M6VYe7fXqeeMLOygz9m6oGG7/tvSnpZQVBEDJReJ5Mgl7iUMIcnWq7Ye3ZICw7fhf1y9tncVIF4JWxqvbgFmCkNf4LOgmsHwoYmwEVWgKWpYCUBCA5XvX42pdA+aaqc69sADaPpik/UKYGUsvUxqZLliiX5oIKVWvj7fpZh1eQeX3R4QBcCo7GwRuRaFnZqaA+DkEQBINDhFxA/yblWci3XAzFF29U4wIr2eJYMeN+UhzgXAMIvwLc3P3s+bHaFeeU9Jl7wCGYBBzCBErgYwEoQVYw8vsLqNZJdZwGAsamgIkZHEtaoE8jDyw6cgdz994UIRcEQdBChFzgmG0fdztcCIrGqlP38NGr3jl/sXcbbht27sWFA+tgZWaMyu5OqOvlAo8yjjAq1yj9XEo68+EJIDURgVdPYt/+XahhdAd1zO7BNCUesC2bfu6Ff4Bdk4FqnYHaPTG0eQO2GJwMeIgTtx/At4Jj/n4IgiAIBooIucD0a+KJC6svYMWJQAxrWZEd4XLKvYdPMPFQEuJTOwCU1O+mqlV0KoEe9RPxVr0ErsAGSztulD994PlHuJ3shs4+bmjQqzbw4CZgr1U05/5VIDEGOL+Mm4uNKxa6tcT0oNr4ba+jCLkgCEJuUrQWF4pMitaXgMS1ybQ9ePQkGfP71UfbGlmXfM0M/XwGLD6Fg9cj4OvlgJGvVWIz/bbLoUhIVhVroTHBK5Wc0KO+O16vXgY/bPfD4iMBcLaxwM5PWqCUtfmzFyYnusBjwKV/gSvrgYT0am3X08oiceBu1Krg9nI3ST91Cp9LSwZSqSUBaSnp21RsprR3xmUBU0uVc5+J2cu9lyAIgj6maBWKLpZmJujZsBz+PHAbfx+/m2Mh33A+mEWcPN+nvVWLE7k0r1Qa33Spga2XQrHmTBBOBTzCgesR3GwtTRGToMqj/kP32lmLuDq9rGczVeswA7ixi0U9+do2xKAE5h8OwXwScqq7vuNzVSY6bnFP29P9Kh1UtdoJEuuZFbK/mSpvAH1WpO//XAcgkz/3x0wl6NTMrIGy9YC35qefG3JOlQLXxlVTpEYQBKEwECEXNLzrW55jug/diMTtiDgW5efxIC4R3/x3lbc/bl0pw/k2lmacH50aZW1bdzYIa88EISQ6gY/3aVQOr1Z1zlnHKBSu2pvc7t0LxqfztuDW1fvwD4tFFQdj4Oz/sn9tnFbsOYkxQV73tG1irkqCo962ts9oEaCZu2Y/GUiIUjXC0TvjTH9JJyApVnWdUuUBe09Vc/ACXH0Az+Y5u1dBEISXRIRc0FDOwRqvVXHGHr9wnpVP7lTjuedP3XKNTfFVXWwwtEX2M12v0iUwtm0VfNKmMo7eeoBbEXHo1bBcrvpYoVxZVKlRH7cuhXG2t1961wFe/QIwL5k+Y7awSd+mUq5qaKY86WHOqrzRuV9FqmbxNLNPfgIkUYtTbdN6vxp6rqQTEBWvOv/BDVVTU6ldRiFfPRCwcQMcK6gGBA4VAWtHVa57SsYjCILwEoiQCxno16Q8CzmZxMe3qwJr86x/Ivv9w7H+XDCvf0/vXhtmOShoQhnbyOxOLS982MobWy+FYfPFEIx5vTI8W1IQWw55mVKtJKpkDaAGh+zPo4EDJc1JTQFigoFHAU/bHVXpWPeG6efSGj2t+Wf5fiZA3XeAzr+q9mntfvnbqmI3tH5PJn3qk9qtxa2u6nw+NwXYNl51jM5hK4OZJoQPTlVVCX7UnF6sGjw4V1dZDfSghK1BQN8ffb7aVQUFQceIkAsZaFHJCeUdrblQyYZzIVy4JDOPE1PwxfrLvD2omRfqlCtVqH2sWdYOr1V1xl6/cMzbfws/9KgNvYDM9PZkVqdMdC2zPoeE9c2fVIl1uN2E8igARmS6p6x5JOZqEmNVqXGzo9bb6UKupAGnFz1//V9byLeOUzn6EeTQV7oyJ+mBczXVwEOdxCevpCQC968AIWcBr1bpzoRk5YjwA0pXUQ1UXgQNaiKvA2GX0hu9nqwu3RcAZeurzqMBVOx9oJQHULJM/vgr0OBo1yTg1l7gPv3ujQD3BqoiQ5XaAK51C94vIjlBNUikwRxZbujRNBv/EqHYIUIuPDNrprVyKh+69FgAr2UbZTL3zt51HcFR8XC3t8LYtpV10k+KdSchJw/5UW0qoWwprRKs+oylLdBgMB4+TsJ/F0Kw9mEQLj15hEZuFvimQ3lUcdMKq6M/2N3+VAk6O/A9ST9G3wnNptXQjLrV5+miTiLN3vlPH7XPJap0BKh0bbifyqEv7KKqEVXfTBdyev2G4YCNi8qRL8OjS8bSt2mpKnElx7/gsyrxJhGn5Qai3bR0IQ8+A/zvafIfOw/AqcrTVlX1SH4FbAkBcPQ3YM/X6dfJjLlN+vbF1cC+qapt8lewc1eJOj3augP1BwK2rk/7m/asAKckAcGngcgbQP0B6Z91wKGnIs4fMBB0StX2f68aRFBtgpclLhy4fQB4HKHVItO3KZOiug8R14D5rZ4dFLKwWwNNRwBNRz499zrwb7+nv4FU1QCRHtXbvh8ALcY/vRVF9fui36VgsIiQC8/wdgN3/LjTH35hsVzMpIFnuln5/L0oLD5yh7e/61YrW9N7QUOpZJtUcMSx2w8wb/9NTO1aC/pOYkoq9vlF8OBjn184UjTV3IxxIiQZHRffwvuvKBjdxp6jCFgkfXrn7OIs5J/mvDO9/lY90h93msWGX3varmacjZOgUAhgdtR9F+gyV7VNwrao3bPnkBmalgHUAkpQOCF5+T8OB6IDVe3mrvTjAzYDXlSwh8rgOaWHB5apqcr5T40GJzQIIadCNbSMQMIdHax6DS1tUFNTszuAp/049CNwbC5gVw6wK6ua9QceB5Ifq0SSzlVbC5qPUV3Pq6XqkbIY3tipEmK3eunXVy+H0GdIlg5ytowNVYUyUmv+CVCvn+pcGiysey/7zzYmROu6KaoBC/WNBmr83aWoci1QI8uHGhq40YAqO0i41dBn81sD1edZvrkqSsSjCWCdxVIS3Rv1KTpINQjkFqSyrtAAjD4vWqbJT7QLORER/ipLFr0npY4mK5KZJXQOfTb0m6aWOftlISBCLjwDhYR1qeOGf08HcVU0tZAnp6bhs7UXQfrTrW5ZnadKHfmaNwv5suOBcLG15Fl6ZuuBrqE4exr8rDsbjP8uhiDqSbonfK2ydnirXlk0qeiIX/bc4HX/Pw7c4hj8ad1qoal33nwJcjwAoD881CgyIDM0K37926dCFJrxkUSUcuurITEgJ8AytYCydVUCRwJOQpv5e6FUvNRozZn+ONOMkx/9nv6xvpku5JXbAR9fUEUDvOj7bT5a1Uj4qAxvVKCqkbCT8JBgq6F9dSTC/Uvpz1uXBrxaqP4oq4W8eueM70MzZWo0gydxVXPvpGo5JLslEeqLmlLlVNUFSzqrBislSj99fNq0EySVawh8HqQSNhINek9KY0yNRE3bqZM+pwH/qZZpaEBC3zE19T69jxqyntDAIPSCqh2fq6mFwCGWLT9VWTOIIz8De7/N/rOn5Qa1kN85pBrs8KCrtur3RX0gKwilaaaBgvq7pJLJd4+oLBTU6Dh/L0+FcdyN9D6fnA+cWpD+nnQ/ztUA1zqAWx2gZg/AKpdLffTZ0mcZ/1D1u6R+0DY9V69/+nn7vlfdn7qP8VFavwGjpw61hRuCKglhsqA4JoTJzOXgaLz562GYmRjh6Get4WRjwV7iM3f4c6GV3WNa8qOumbXTH7/upVRywMCmnpj0ZvUMZVB1AdVNvxoagz3XwrHxQjBuR6T/oS9ja4Gudcuiez13VC6jZRIGsOvqfXy14TLCYlQhelREhnLfZxtrr0vYJBujEgFtxy+1s50hQDPTqHuqtWeaWdIMl2ajNNPP7R9iMo37bQbuHFQJp00Z1VJEyaePFKVAz+kTNMu+exQIOKwSVPJFUPPOWpUfAEFVDjeNUAk7t6dLFjTYI0sO5XtQz+TJp4CEX42plWqQRxYeMu9PuJN+7paxGcU5MyPOpC/JHJ8HXFqjek8a9D15kPFcEn0aGBHXNqssDurIE44+efpI33WvZemv+3cA4L81m+UbEucH6Q6hFHWSncMqWU3GXMkY1ZIHJCGMkGeHsroepXAuMAorTwbijdqu+HmPKqSKxFIfRJygsDZ7a3N8s/kqlhwNwKMnSZjZw4cT1LwsgQ+e4NDNCLiVskI1F1sW3ZzO8GMSknH4RiSbzPdfj0BEbLqp08rMBO1ruvDsu2nF0tmmv6Wsd40rOGDGdn8sO3EXq88EYZ9/OCZ1qoFOtV31y9pAfcnqj5U+9fFFULRBmeqqll/QzJHW4akZCrZuKkdItTMkzYpJ0EMvqvwg1NR8S+VgmZNBDpnpKVkTOSWSbwGFbMY9Ta6kHvCohbzCq6plCLZMOKueJ0vP05TOGmEmGg9XNfWgkQZgIeeA0PPAo7sZzz2zOOtCTszT6A/175Ue1SJuYqHqAw1QrRxU+SVo6cL8aYnlRkNVNSCs1H2kx6fb5PCqA3Q6Iz948CBmzpyJM2fOIDQ0FOvXr0fXrl2zPX/gwIH43/+eTf5RvXp1XLlyhbenTJmCr7/+OsPxKlWqwM/vOWtGmZAZuYr154LwyaoLcLWz5Bjzk3cesjl9yaCG+iUqlGHuXDDGrb7A687Ux3nv1svx+v2TpBT8vu8WJ8NJSk3LUAudYuSpVXGxRVVXG1QpY4MSFqZsMr8RHsfCTWJ7OuCR1po3YG1ugubepVmcO9RyRUmLl/sPfubuQ3y69hJuhsfxPnnpf9u1puE49QmCGvLDoJkx51soo1oGKIyUx0d+Vg0kOM0y5ZmwfppfoqTKQbBO3/RZNi29kHWJBFwd5qkHGMSM/PHjx/Dx8cHgwYPx1ltvvfD8n3/+GdOnT9fsp6Sk8OvffvvtDOfVqFEDu3enj8RMTcXwkBs61nLF1M3XEBqdwI1mllO71tQ7ESfIXG1nbYbhy85wKth3FpzA4oENn2uWJjHedjkMUzdf1WScoypwT5JScTvyMaLjk3HizkNu2pRzsOKlPvLc14aKxLxaxZkz1jXwtIeFae5js+uXd8CWUc05vI4GGeShf2zWATTycuBwvzoepVDHvRTs9cQyIgjZQmJZulLhv2+zj3N+rrbvhAGiU4Xr0KEDt5xiZ2fHTc2GDRvw6NEjDBo0KMN5JNwuLjnLFS5kDwkRZWD7ff8t3h/XrgrPzPUVEtHl7zXG4CWneEng7T+OYemQRnC1e3YWe+N+LKb8dwVHbqrW2GimO6lTdbStXoYHKlREhjLQ+YXGwv9+LHvw+4XGIDw2EfceqgTcwtQYTSs6snC3quwMD0frfP/8R7epjDdru+KztZdw+m56zno1no7WLOx1Pez5sZqrba6WFQoDWrq4HRmHVlVymJpXEIQcYdBT1YULF6JNmzYoX54ScKRz48YNuLm5wdLSEk2aNMG0adPg4fFsYhMhZ5ne/jkZyAJBzmT6DoWlrR7WBP0XnmTTd/ffj2LpEF94O6u8j2MTktlDnKqvkSmcRG94y4pcutXKPH0GTeFfNdzsuGlD8d9+YTFITVPQoLxDhtcUFN7ONnxPl4NjcO7eI5wPjGJPeLIaBDx4wm3DeVWoEt1Pq8pOmNXTh/Pd6wtXQqLRZ/5xLpizZliTDCGNgiAUEa91mgW9aI1cm5CQEBbnFStWoGfPnprnt23bhri4OF4Xp3V3Wi8PDg7G5cuXYWOT0UtYTWJiIjc1dD6tuxf3NXI1FHZmbGT0UjXKdU3Qoycs5iR29tZmWDyoEe5ExuH7rX4aR7Q21cqw415+z6QLi6gnSSzo2k0d3ka57z/vWA36AFk/es0/zoMg4r3mXvjyzXx0MBOEIkpO18gNVshplj1r1iwWdHPz7NcJo6KieMY+e/ZsDBkyJMtzsnKQI0TIDRuqzjZoySlcDIrOkKKczNFUECbH1dcMBPqvvP1yGIYvPwtTYyNsH91CY4nQFQGRj9Hzz2O8JEEDKiqyU6F0CewdlylLmSAIuRZy/VxMy8EfrEWLFqFfv37PFXGiVKlSqFy5Mm7eVMUaZ8XEiRMRHR2taVevqkpzCoaNY0kLrHi/MZp5O7KIk7MeFYLZ8UmLIifi6sEweci3rurMywYUkqfLcTpZRcjpkEScvP03jWjOeQnISkJlcgVByB8MUsgPHDjAwpzdDFsbMrPfunULrq5a6SEzYWFhAVtbW03LzgQvGB4U9rVoYEPMe6ce9o5rydnf8uJNbgiQ2ZoE8+D1CE5KowvCYxLw7oIT7NlPM/Bl7/myo6SvlyqXPHnhC4JQBIScRPb8+fPciDt37vB2YGCgZqbcv79WajwtJzdfX1/UrFnzmWPjxo1joQ8ICMDRo0fRrVs3mJiYoE+fPoVwR4I+QsJNM9WsvNeLIlT/fUhzVX34b7dc5Rzvhb2kQTNxcsKjUL3l7/tyZkCidTWVJWT3tfuF2idBKMroVMhPnz6NunXrciPGjBnD25MmTeJ9clZTi7oaMn2vXbs229k4rSmQaJOzGznBOTo64vjx43By0m1ecEEoTEa85g1nGwsuR7vwsKrITWEQ/SQZ/Z5GDFD++xXvNc4wgGpdVZWe9FTAI47TFwQh7+iNs5s+IZndhKKUmY+yzO0d2woudgVbJSouMYXN6eQ9X7qkOVZ90AQVnZ51tnt99gEW+l/61EVnH7cC7ZMgGDJF2tlNEIQX07VOWdTzKMWZ6qZvu1ag7xWflMqJeEjES1mb4e8hvlmKOPHaU/P6XjGvC0K+IEIuCEUU8mL/ujOl1AUnjDkdkDHVbH5BueqH/n2ac/HbWJhi6eBGnEAoO9TmdSouk6KV214QhNwhQi4IRZha7nbo1aAcb0/edIUz0uUH9x4+wdJjARi4+CTqfrMLh25Ecnjf4kENUdv9+fWgyUpAs3ZKXnM2MCpf+iMIxRmDTtEqCMKLoRz5Wy6F4kpIDFaduoe+vh65yu535u4jrvZGoWO0xq2Nu70VZvSonaPUq6YmqjSyZCXY43efC8EIgpB7RMgFoYhTuqQFPmlTmRPEzNzhhzdquXKluBdBhWMoU9yua/c5Jj02IUVzjNL11vew58Q6VGK1cpmSL1UVr3W1MiohvxaOiR30I5WsIBgqIuSCUIyK39BM+qfd1zGlc41szw2Njsffx+7y+ZRSVQ2lWKXKZSTeLSs55WgwkB0tKjvxYIDqrd998BjlHUvk+lqCUNwRIReEYoCZiTHnl3934Qn8ffwu+jTyQBWX9AyGFIVK69WLj9zhGu3qtXQq79qtblkWbyqTml+Fc+yszNDQ0x7Hbz9kU/2gZl75cl1BKI6IkAtCMaF5pdJoX8MF26+E4ev/rmD5e75ITlWw9VIoC/iFoGjNub5eDiyur1cvU2BV76j6HAk5mddFyAUh94iQC0Ix4os3qmGvfziO3nqAsf9ewKGbkZqyrlTLvIuPGwY283ymDntBQGvrU7dcw4k7D7hOvD7VT8+Off7hCImKR62ydqjqYsufmSDoGhFyQShGUOGSYS0q4Je9N7HuXDA/R6lc+zUuz97sVDGusKjgVJILqlA1NApf61gr+8JG+sDhG5EYtPiUZt/cxBjVXG04xI9C7nzcS3HZ2IKyYBQ2tNzyMg6Mgu4QIReEYsbwVt68Hk7FVN5tXB4darrqbGZJs/Lbh++weV2fhZwKwYz5V1XcqaJTCTx4nMRx8LQcoVqSUNWEoHS4Nd1I2O3Qs2E5VC5jmJUUd14Jw8crz+PDVhUxsnUlXXdHeAEi5IJQzLAyN+GyovoApWtdcPgOm6zJwU4fZ7M0M/107UWuq17JuSTXVbc0M8a9h/G4EBSFi0FRLOaXg6M5He7JgIfcyOv/r/4N0NS7NAwJut9ZO68jPjkVs3ZdR6qiYHSbynm65onbD9gfg/w0hPxHhFwQBJ3R0NMBNpamePg4ifO01y9vn6/Xj0lI5kQ2TSs65roOPXn5774WzlYLKvRCAyHCw9GaW6enhV9oIHI7Io5F/d9T91jMBy4+hV/71kW7Gi65vodNF0JwLvARPm5dCaWszVHQHLwRCf/7sVzTnsR3zu4bMDEyytXMnAYFP+26zks5xH8jmvNShJC/iKeGIAg6DYtrWVlVYnivX/4VUaFkNgsO3UaLGft4XXvAopN4nJie0Can+IfFskMeMbFD1efmkCdrQqUyNuhR3x1/v9eIIwSSUtMwfNkZrDkTlKsc9mTOH/XPOSw+QulwT+XqHl6Wvw7e5sf+TTzxWYeqvE0z87n7VGL8Mt8BmefVIk78vOdGPvdWIETIBUHQKRSGRtA6eV6hWTGJZutZB1iAaR2boDC3fgtP8Az9ZYSIRDQpJQ2vVnHCwKaeOX4tzf5/61sXb9d3B4Xkj1t94aXqwtMAotOvh7HubDBotaGkhSlbLKg4DfWroLgaEoPDNyN5UDKomSeGtayICe2r8LGZO/zx+/6ciXlkXCL6/nWcrQmmxkZsTSC/ud3X7vMShJC/iJALgqBTaEZOYuUXFougR09ydQ0y4e65dh8dfz7EohkcFQ8XW0v80L0W1g5vCltLU3bwo3rpUU+ScnTN77deYxMzpbid+bbPS3twU055yj//XnNVjPy3m69i9k5/7uvz7mPVqUB0mXsYtyIeo4ytBf55vzH7NJQwN8GRmw8w8p9zBVY1bsFh1Wy8Q00XuNtb8/aHrbwxrq1qjXzGdn/8eeDWc69x434suv1+hD9vSvyzdEgjfPJ6ZXSqrVqC+EVm5fmOCLkgCDrFvoS5Zm2csry9LGfuPkTPP49hyP9Os/CSaJMZfP/4VujV0IOv/c/QxnAoYY6LQdHoPf84zxifx+6r97H02F3entXTh8U8N5D4U+y+WgjJzDxl0xWkZVGFjszmn6w6j0/XXkJCchqnsd066hX4VnDkrHp/DWjA6/S7rt7HhDUXs7xGXgiLTsCm8yG8PbRFhQzHRrxWCWNeV93DtG1+GvN7ViF6b807yo6A5R2tse7DpmhaUeXgNqq1N8/Kd169zzN/If8QIRcEQedQEZWXNa/TzO/9pafRfd4xnAp4BAtTYzYFH5rwGj5oWRGWZunObZTgZuXQxnCyseCZP4n5/ZiELK9Lz49fc4G3aTatXsPPLSTmJITfdqnBQva/Y3d57Zsqyqm5FhrDpnQqJENmbTJnLxnYMENcPwni733r8XHKAUDZ+Z43u39ZlhwNQEqawtXosipFO6p1JYxuo3J4+27rNfZB0GbFiUAMWHySi+s08nTA+g+boaJTSc1xb2cbLthDyKw8fxEhFwRB57Su6syPx249eKFDF60RT9/mh3ZzDvLslMzyvRuW4xk4OWdlV8yFYrr//aAJXO0suVgLzeLJBK8NzXIp4x0Vi6nhZovxT9eH84N+TTwxp1cdXjMmwR729xm+FxLArnOPcGIc6hsNOMicbZxFKF6b6mUwu6ePZkAwe9f1fOkbfeYrTqgsEO+/knE2rg2FoY16zZu3yQdh0eE77Jfw3Zar+Hz9Jd5+q25ZdvYjC0hWgwHqO6UJpsGLkD+IkAuCoHMoI5qHgzV7eZOzVXacDniIjr8cwh8HbrETWdvqZbDzkxaY3r02XO2sXvg+XqVLsJiXc7DC3QdP0POPY1x9Tc1fh27z+1uZmXCoWW5D1rKjS52ymN+/PlsP9viFs1c9CWDiU4e6LaNe4ZC8F13jmy41efvXvTcx/+Dz16xzwr+n7yEmIYUz7akHVdlB690jXlWJOZXGpfX8vw6pHPnGvl6ZlyKy+9xoMNWxpmpW/utemZXnFyLkgiDoHDI/U5Y3Ym8W5nUKxSJT8tt/HsPtiMecVnZ+v/qY378Bm2xfNk0tiTmJOs3IaWZ+KyKOE7uQZzYxuVP1DGbh/OS1qmWwdHAj2FiYcpIZMpXTmv7CAQ2znMVmBaXUVXuTf7/Vj5PP5BZynFN71A95xStLS0Dm72ps28qc9Y24HByjibGnWPMXOQWObK0aBGy9FMbe+ULekYQwgiDoBa2rOfM6Lc1UycStFpSjtyLx2dpLCHyo8minkK4v36iep3roNHtfNbQx3llwgmu09/rzGKzNTXmNuGMtF/RqWA4FCTmwrfqgCSebobjz3CTCIfN7THwKWydoVk8haurkNC/Djiv3EfQongcR3eu55+g1JNbj21VBCQtTdl6b9GY11C//fEuCGio2Q17xVC73l703MLdvvZfus+SB16MZ+cGDB9GpUye4ubnxl7Jhw4bnnr9//34+L3MLCwvLcN7cuXPh6ekJS0tL+Pr64uTJkwV8J4Ig5BVfL0cOsSKP8kvB0VwR7Yv1l9D3rxMs4m52lvjf4EYcCpYXEVfjbKtaj67uaovIuCTNe0zrVrtQRKK6my2mvVUrT9nsPm1fhYvdkM8bebzve0mvfxJEWk4gKO++toPgi6DP6KNXvbHxo2Y5FnHttXKCSuiS0+LLQNaH+lN3Y+YOv3z33DdUdCrkjx8/ho+PDwvvy+Dv74/Q0FBNc3ZOX9NZtWoVxowZg8mTJ+Ps2bN8/Xbt2iE8PO/JJgRBKDjIPEshV+r103Y/HcTyEyqT8buNPbDjkxZ59iDPDHmFU5x2g/L2PIj4qVedfBkkFBYkpt92qYkuddzYmvDBsjPYcSXjxOZ5UPpaSjRDn33/JuVRWFCGvHY1yvAARDvz24sg68PEdZc4pe/cfbc4Z0ByAcXUGxI6FfIOHTpg6tSp6Nat20u9joTbxcVF04yN029j9uzZeP/99zFo0CBUr14df/zxB6ytrbFo0aICuANBEAoiDI1ym4dEJ7AD3Ir3fTG1a60Cq1dOwr16WBOc+rINm7wNDVpj//FtHxZGykJHKWFX5nDNfP7TePDu9crmOlY+t6hn5ZsvhuBmeOwLLQc0A6doBYLuVR2GN+R/pwslda0+Y5DObnXq1IGrqytef/11HDlyRPN8UlISzpw5gzZt2mieI5Gn/WPHjmV7vcTERMTExGhabKw4YAiCLiDPbVrrJcv24GZe2D76FU1CkYKe2dIauSHnrKe15l4NyrE3/2frLnFu9OfFmd+JfIxd11T57Yc0zz7krKCg2H6KOqAukvd9dpD5nJLo0AycoBDDP/s1wIIBDTi64OD1CPT568VJfgoK+oxDo+O59Ousnf4YuPjkS6XjzQ8M6pdL4k0z7AYNGrD4LliwAK1atcKJEydQr149REZGIjU1FWXKqEb1amjfz081ksuKadOm4euvvy6EOxAE4UWm7o0jmvF2QXmNF1UoJez07rXgWNIcv++/xR74D+KS8OUb1bL0RKcYcBJRihag8D9dQLNycpb770IIb2f+zsmjfsLai5xzngZ3tIxAa/nEq1Wc2VozeMkpztjXY95RLB3syxXpCpLwmAR+v4vBqtK1tJ15EEG5AoY8Tc1bGBiUkFepUoWbmqZNm+LWrVv46aef8Pfff+f6uhMnTuR1dTXBwcFslhcEofARAc+bZWFC+6rsgc4JW47cwcPHiewgSLN2NY8eJ2H1mXsvTABT0NQsa8dFc6iYym97b7KPgprEFFXRGvKqJzM6JcKhGHpt6nrYcy79/otOIuDBE04Pu2RQQ75ufhL9JBlfb77CKWgpZDDLynfOJVHb3Q613EuhnsezmfEKEoMS8qxo1KgRDh8+zNulS5eGiYkJ7t/PWA6R9mktPTssLCy4qSHzuiAIgqHy3isVeGY+fvVFziJHmermvVtPs3yw7Phdzudes6wtGld4OY/z/IYqo5GQbzwfjJGveaOCU0nOG/DB32dw6EYkO+JRalrKapcVFZxKYt3wplzm9WpoDIcSkum9eaX8WZKhlL1UBpdS+xJk3KjkbMODBZVw23Hkw8t4/Oc3BrlGrs358+fZ5E6Ym5ujfv362LNnj+Z4Wloa7zdp0kSHvRQEQShcutV150IrlmbGOHA9gsP4aCZOaWEpvat6Nq7reGwSQsomR2v7v+27iej4ZPRbeJJF3NrchHPOZyfi2qGEqz5ojKYVHfE4KRWDlpzkgUFeIT+C7vOOsohTEiJK5HP563YcQUEZ7AY09UQ9D3udirjOZ+RxcXG4eTPdyeHOnTsszA4ODvDw8GCTN5m5ly5dysfnzJkDLy8v1KhRAwkJCbxGvnfvXuzcuVNzDTKRDxgwgNfRabZOr6EwN/JiFwRBKE7QOvLy9xrzOjKFmVFmPConSmu6lNe949MiJrrm4zaVOBHQxvMhvO58/X4cV7FbMrgRC2VOsLE0w+JBDTlX/uaLofh45XlExCbyWnVuBivUD3JcoxwDno7W+HuIL2cF1Ed0KuSnT5/Gq6++qtlXr1OTEC9ZsoRjxAMDAzN4pY8dO5bFnULKateujd27d2e4Rq9evRAREYFJkyZxohjycN++ffszDnCCIAjFAUo4Q+F1/Ree5GIxP+1WFVqhqADtdXNdQtXWKGJhn38EiziFwv09pBHHm78MFqYm+KV3Xa5yt/hIAPsJUGlcKiVLXvI5hYr3UGW9uMQULp6zZFAjvqa+YqTkZx28IkJQUBDKlSuHe/fuwd09ZykLBUEQ9BnKK99v4QnOVU8hfkcnvgbbAorNzw2U6/7tP46xiC97z5dz4ecWRVE4BGzGdn8uxEMTcko/O65tFbjYWT73tZRQZ+Q/5zgmn/wHKJ+/rj6nnGqRCHkWiJALglAUoYxoFJbW3Ls03qitH2Z1bSgemzzu86vq3L2HTzBjhz+HtxEUd/5+iwr4oEUFzhOfmVWnAjlzHK3XU9KZn3vX1en6twh5HhAhFwRBKDqcC3yE77Zcw+m7j3ifzOTj2lZGj/rlOHSMZHDegVs8gycosc533WpybL4uESHPAyLkgiAIRQtFUbD9chimb/fjWvREVRcbTOxYDYeuR2DB02xsVJ6VKrvp2pv/ZbTI4OPIBUEQBOFFkDB3qOWK16o54+9jdzktLIWVUYy4GsqCRzH4hoZ+uCwKgiAIQiFgYWrCYn1gfKunnvtGbF6f9baPQYo4ITNyQRAEodhRytockzpVx/stvJCYnAbPPHjJ6xoRckEQBKHY4mpnBUNHTOuCIAiCYMCIkAuCIAiCASNCLgiCIAgGjAi5IAiCIBgwIuSCIAiCYMCI13oWUA1zgqqvCYIgCIIuUGuQWpOyQ4Q8C+7fv8+PVM9cEARBEHStSR4eHtkel1zrWZCSkoJz585xDXNj47ytPsTGxqJ69eq4evUqbGxs8q2PgqDPyO9eKK7E5uNvn2biJOJ169aFqWn2824R8gImJiYGdnZ2iI6Ohq2tra67IwiFgvzuheJKjA5+++LsJgiCIAgGjAi5IAiCIBgwIuQFjIWFBSZPnsyPglBckN+9UFyx0MFvX9bIBUEQBMGAkRm5IAiCIBgwIuSCIAiCYMCIkAuCIAiCASNCXsDMnTsXnp6esLS0hK+vL06ePKnrLglCgXHw4EF06tQJbm5uMDIywoYNG3TdJUEocKZNm4aGDRtyAhhnZ2d07doV/v7+KCxEyAuQVatWYcyYMezBePbsWfj4+KBdu3YIDw/XddcEoUB4/Pgx/85pACsIxYUDBw7go48+wvHjx7Fr1y4kJyejbdu2/P+hMBCv9QKEZuA0Svvtt9806fbKlSuHkSNH4rPPPtN19wShQKEZ+fr163l2IgjFiYiICJ6Zk8C3aNGiwN9PZuQFRFJSEs6cOYM2bdponqO87bR/7NgxnfZNEARBKDgoPSvh4OCAwkCEvICIjIxEamoqF17RhvbDwsJ01i9BEASh4CDL6+jRo9GsWTPUrFkThYGUMRUEQRCEfILWyi9fvozDhw+jsBAhLyBKly4NExMTTW1zNbTv4uKis34JgiAIBcOIESOwefNmjt5wd3dHYSGm9QLC3Nwc9evXx549ezKYXGi/SZMmOu2bIAiCkH+QzziJODl37t27F15eXihMZEZegFDo2YABA9CgQQM0atQIc+bM4XCEQYMG6bprglAgxMXF4ebNm5r9O3fu4Pz58+z04+HhodO+CUJBmtNXrFiBjRs3ciy52g+K6pJbWVmhoJHwswKGQs9mzpzJX2ydOnXwyy+/cFiaIBRF9u/fj1dfffWZ52lAu2TJEp30SRAKI9QyKxYvXoyBAwcW/PuLkAuCIAiC4SJr5IIgCIJgwIiQC4IgCIIBI0IuCIIgCAaMCLkgCIIgGDAi5IIgCIJgwIiQC4IgCIIBI0IuCIIgCAaMCLkgCIIgGDAi5IIg6Dwr1oYNG3TdDUEwWETIBaEYQ+kjSUgzt/bt2+u6a4Ig5BApmiIIxRwSbcoJrY2FhYXO+iMIwsshM3JBKOaQaLu4uGRo9vb2fIxm5/PmzUOHDh24ilOFChWwZs2aDK+/dOkSXnvtNT7u6OiIoUOHchU0bRYtWoQaNWrwe7m6unLJR20iIyPRrVs3WFtbo1KlSti0aZPm2KNHj/DOO+/AycmJ34OOZx54CEJxRoRcEITn8tVXX6F79+64cOECC2rv3r1x7do1PkZledu1a8fCf+rUKaxevRq7d+/OINQ0EKAyjyTwJPok0t7e3hne4+uvv0bPnj1x8eJFdOzYkd/n4cOHmve/evUqtm3bxu9L1ytdunQhfwqCoMdQ9TNBEIonAwYMUExMTJQSJUpkaN999x0fpz8Rw4YNy/AaX19fZfjw4bw9f/58xd7eXomLi9Mc37Jli2JsbKyEhYXxvpubm/LFF19k2wd6jy+//FKzT9ei57Zt28b7nTp1UgYNGpTPdy4IRQdZIxeEYg7VD6dZrjYODg6a7SZNmmQ4Rvvnz5/nbZoh+/j4oESJEprjzZo1Q1paGvz9/dk0HxISgtatWz+3D7Vr19Zs07VsbW0RHh7O+8OHD2eLwNmzZ9G2bVt07doVTZs2zeNdC0LRQYRcEIo5JJyZTd35Ba1p5wQzM7MM+zQAoMEAQevzd+/exdatW7Fr1y4eFJCp/scffyyQPguCoSFr5IIgPJfjx48/s1+tWjXepkdaO6e1cjVHjhyBsbExqlSpAhsbG3h6emLPnj156gM5ug0YMADLli3DnDlzMH/+/DxdTxCKEjIjF4RiTmJiIsLCwjI8Z2pqqnEoIwe2Bg0aoHnz5li+fDlOnjyJhQsX8jFySps8eTKL7JQpUxAREYGRI0eiX79+KFOmDJ9Dzw8bNgzOzs48u46NjWWxp/NywqRJk1C/fn32eqe+bt68WTOQEARBhFwQij3bt2/nkDBtaDbt5+en8ShfuXIlPvzwQz7vn3/+QfXq1fkYhYvt2LEDH3/8MRo2bMj7tJ49e/ZszbVI5BMSEvDTTz9h3LhxPEDo0aNHjvtnbm6OiRMnIiAggE31r7zyCvdHEAQVRuTx9nRbEAThmbXq9evXs4OZIAj6iayRC4IgCIIBI0IuCIIgCAaMrJELgpAtsvImCPqPzMgFQRAEwYARIRcEQRAEA0aEXBAEQRAMGBFyQRAEQTBgRMgFQRAEwYARIRcEQRAEA0aEXBAEQRAMGBFyQRAEQTBgRMgFQRAEAYbL/wGnnH8eVzXF6QAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:28:21.253110Z",
     "start_time": "2025-06-27T02:28:21.077681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[1])\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=1024,\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = tensor_to_text(token_ids, tokenizer)\n",
    "print(generated_text)"
   ],
   "id": "1818eb1a3d9ca2c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Write a topic sentence that summarizes the paragraph\n",
      "\n",
      "### Input:\n",
      "Increasingly, scientists are turning to computers as powerful tools for processing and analyzing data. Computer simulations can reflect the behavior of complex systems, from the evolution of galaxies to the stock market. The data gleaned from these simulations provide insight into how the real world works.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save model",
   "id": "ffe50f3c0d3864f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:30:37.884335Z",
     "start_time": "2025-06-27T02:28:21.276491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def generate_response(entry, model):\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=text_to_tensor(input_text, tokenizer),\n",
    "        max_new_tokens=35,\n",
    "        context_size=1024,\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    generated_text = tensor_to_text(token_ids, tokenizer)\n",
    "    response = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "    return response\n",
    "\n",
    "# Generate and attach responses\n",
    "for entry in tqdm(test_data, desc=\"Generating responses\"):\n",
    "    entry[\"model_response\"] = generate_response(entry, model)\n",
    "\n",
    "# Save to file\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as f:\n",
    "    json.dump(test_data, f, indent=4)\n"
   ],
   "id": "97b67f7b32b44749",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|| 100/100 [02:16<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:30:37.899133Z",
     "start_time": "2025-06-27T02:30:37.896801Z"
    }
   },
   "cell_type": "code",
   "source": "print(test_data[0])",
   "id": "3f9cb2c8c0a22d6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Go through the passage and edit it so that every sentence follows a similar verb tense (e.g. present, past, or future).', 'input': 'John starts the process of cleaning the kitchen. He scrubbed the countertops and spraying down the cabinets. Then he swept the floor and mops it.', 'output': 'John starts the process of cleaning the kitchen by scrubbing the countertops and spraying down the cabinets. He then sweeps the floor and mops it.', 'model_response': 'John cleans the kitchen.'}\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:30:39.371598Z",
     "start_time": "2025-06-27T02:30:37.917323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_name = \"gpt2-124M-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ],
   "id": "2e5f0b4a2aae83e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-124M-sft.pth\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate the model",
   "id": "2d16eb445d3fd5da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:39:25.786333Z",
     "start_time": "2025-06-27T02:39:25.762109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import psutil\n",
    "\n",
    "def is_process_running(name_substr: str) -> bool:\n",
    "    \"\"\"Check if any running process contains the given substring in its name.\"\"\"\n",
    "    return any(name_substr.lower() in (proc.info[\"name\"] or \"\").lower()\n",
    "               for proc in psutil.process_iter([\"name\"]))\n",
    "\n",
    "if not is_process_running(\"ollama\"):\n",
    "    raise RuntimeError(\" Ollama not running. Please launch it before proceeding.\")\n",
    "\n",
    "print(\" Ollama is running.\")\n"
   ],
   "id": "56c837af437f0965",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ollama is running.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:40:37.205584Z",
     "start_time": "2025-06-27T02:40:37.200051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "    prompt: str,\n",
    "    model: str = \"llama3\",\n",
    "    url: str = \"http://localhost:11434/api/chat\",\n",
    "    seed: int = 123,\n",
    "    temperature: float = 0.0,\n",
    "    num_ctx: int = 2048\n",
    ") -> str:\n",
    "    \"\"\"Send a prompt to a local chat model and return the generated response.\"\"\"\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"options\": {\n",
    "            \"seed\": seed,\n",
    "            \"temperature\": temperature,\n",
    "            \"num_ctx\": num_ctx\n",
    "        }\n",
    "    }\n",
    "\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=json.dumps(data).encode(\"utf-8\"),\n",
    "        method=\"POST\",\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    response_text = []\n",
    "\n",
    "    try:\n",
    "        with urllib.request.urlopen(request) as response:\n",
    "            for line in response:\n",
    "                line = line.decode(\"utf-8\").strip()\n",
    "                if line:\n",
    "                    message_chunk = json.loads(line)\n",
    "                    content = message_chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    response_text.append(content)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to query model: {e}\")\n",
    "\n",
    "    return \"\".join(response_text)\n"
   ],
   "id": "576103fbe032757",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:43:31.849076Z",
     "start_time": "2025-06-27T02:43:02.347553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = query_model(\"What do Llamas eat?\", \"llama3\")\n",
    "print(result)"
   ],
   "id": "8034456625eb72ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:49:30.416928Z",
     "start_time": "2025-06-27T02:49:30.410605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_model_scores(data, response_key=\"model_response\", model=\"llama3\"):\n",
    "    \"\"\"Generate integer scores (0100) for model responses using LLM evaluation.\"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for entry in tqdm(data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            \"Given the input below, the correct output, and the model's response, \"\n",
    "            \"score the model's response on a scale from 0 to 100, where 100 is the best.\\n\\n\"\n",
    "            f\"### Input:\\n{format_input(entry)}\\n\\n\"\n",
    "            f\"### Expected Output:\\n{entry['output']}\\n\\n\"\n",
    "            f\"### Model Response:\\n{entry.get(response_key, '').strip()}\\n\\n\"\n",
    "            \"### Respond with the integer number only.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score_str = query_model(prompt, model=model).strip()\n",
    "            score = int(score_str)\n",
    "            scores.append(score)\n",
    "        except ValueError:\n",
    "            print(f\"[Warning] Invalid score format: {score_str!r}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Scoring failed for entry: {e}\")\n",
    "\n",
    "    return scores\n"
   ],
   "id": "6ef00a7087af6e6c",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:54:37.286027Z",
     "start_time": "2025-06-27T02:53:31.801139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scores = generate_model_scores(test_data)\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average: {sum(scores)/len(scores):.2f}, Max: {max(scores)}, Min: {min(scores)}\")"
   ],
   "id": "a437d382509a0058",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  20%|        | 20/100 [00:12<01:13,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0/100. The reason is that the model did not provide any response at all, whereas the task was to explain why the output is not accurate (which implies that there should be some kind of analysis or critique).\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  21%|        | 21/100 [00:14<01:49,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: 'I\\'d score the model\\'s response a 0 out of 100.\\n\\nThe expected output is \"7, 9\", but the model\\'s response is simply \"1 2 3\", which is completely incorrect and unrelated to the original task. The model failed to identify the non-repeating elements in the array and instead provided a random sequence of numbers.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  27%|       | 27/100 [00:18<01:20,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: 'I\\'d score the model\\'s response a 0 out of 100. The model completely missed the point of the instruction and didn\\'t provide the area code, which was the main request. Instead, it provided the original input again, which is not relevant to the question. A correct response would have been \"51\" as per the expected output.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  33%|      | 33/100 [00:24<01:23,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: 'I\\'d score the model\\'s response a 0 out of 100. The model\\'s response is simply \"Cities\", which does not even attempt to create a data visualization from the table as instructed. A correct response would be a visual representation of the temperature data, such as a bar chart or line graph, with the cities on the x-axis and temperatures on the y-axis.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  56%|    | 56/100 [00:36<00:42,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: 'I\\'d score the model\\'s response a 0 out of 100. The expected output is a rewritten instruction that simplifies and clarifies the original text, whereas the model simply left the input blank (represented by \"...\" dots). This does not meet the requirements of rewriting the instructions in a simpler and easier-to-understand manner.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  67%|   | 67/100 [00:43<00:36,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: 'I would score the model\\'s response a 0 out of 100. The model\\'s response is significantly incorrect, as the BPM of \"Fading Away\" by Jaws of Love is actually 127 bpm, not 7 bpm. This level of inaccuracy suggests that the model did not understand the task or provide an accurate answer.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  76%|  | 76/100 [00:50<00:30,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as 0 out of 100.\\n\\nThe model's response is completely unrelated to the task and instruction provided. The input asks for the calculation of the area of a circle with a given radius, but the model's response is simply a repetition of the same parameter multiple times, without any attempt to calculate or provide the correct answer.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  87%| | 87/100 [00:59<00:20,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response a 20 out of 100.\\n\\nThe model's response is not even close to what the instruction is asking for. The expected output is a numerical value (6) that represents the square root of 36, but the model's response is a series of zeros with no clear connection to the original question. This lack of relevance and understanding of the task makes it difficult to determine the correct answer.\\n\\nTo achieve a higher score, the model should provide an accurate and relevant response that addresses the instruction correctly. In this case, the model should have provided the numerical value 6 as the square root of 36.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|| 100/100 [01:05<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 92 of 100\n",
      "Average: 21.35, Max: 80, Min: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
