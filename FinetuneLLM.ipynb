{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-tune LLM to follow instructions\n",
   "id": "b6e02cf5201e2731"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load open weights",
   "id": "91e1b02b97bac08c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:25:45.454408Z",
     "start_time": "2025-06-15T08:25:44.675861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import GPT2Model, GPT_CONFIG_124M, complete_text, generate_text_simple, tensor_to_text, text_to_tensor\n",
    "\n",
    "GPT_CONFIG_124M.update({\"qkv_bias\": True})\n",
    "model = GPT2Model(GPT_CONFIG_124M)\n"
   ],
   "id": "423a8cab44a48e29",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:11:56.833964Z",
     "start_time": "2025-06-15T02:11:56.819066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ],
   "id": "891e084dd7eeb90a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768])\n",
      "pos_emb.weight torch.Size([1024, 768])\n",
      "blocks.0.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_Q.bias torch.Size([768])\n",
      "blocks.0.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_K.bias torch.Size([768])\n",
      "blocks.0.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_V.bias torch.Size([768])\n",
      "blocks.0.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.0.attn.out_proj.bias torch.Size([768])\n",
      "blocks.0.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.0.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.0.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.0.ff.layers.2.bias torch.Size([768])\n",
      "blocks.0.ln1.weight torch.Size([768])\n",
      "blocks.0.ln1.bias torch.Size([768])\n",
      "blocks.0.ln2.weight torch.Size([768])\n",
      "blocks.0.ln2.bias torch.Size([768])\n",
      "blocks.1.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_Q.bias torch.Size([768])\n",
      "blocks.1.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_K.bias torch.Size([768])\n",
      "blocks.1.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_V.bias torch.Size([768])\n",
      "blocks.1.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.1.attn.out_proj.bias torch.Size([768])\n",
      "blocks.1.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.1.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.1.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.1.ff.layers.2.bias torch.Size([768])\n",
      "blocks.1.ln1.weight torch.Size([768])\n",
      "blocks.1.ln1.bias torch.Size([768])\n",
      "blocks.1.ln2.weight torch.Size([768])\n",
      "blocks.1.ln2.bias torch.Size([768])\n",
      "blocks.2.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_Q.bias torch.Size([768])\n",
      "blocks.2.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_K.bias torch.Size([768])\n",
      "blocks.2.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_V.bias torch.Size([768])\n",
      "blocks.2.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.2.attn.out_proj.bias torch.Size([768])\n",
      "blocks.2.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.2.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.2.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.2.ff.layers.2.bias torch.Size([768])\n",
      "blocks.2.ln1.weight torch.Size([768])\n",
      "blocks.2.ln1.bias torch.Size([768])\n",
      "blocks.2.ln2.weight torch.Size([768])\n",
      "blocks.2.ln2.bias torch.Size([768])\n",
      "blocks.3.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_Q.bias torch.Size([768])\n",
      "blocks.3.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_K.bias torch.Size([768])\n",
      "blocks.3.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_V.bias torch.Size([768])\n",
      "blocks.3.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.3.attn.out_proj.bias torch.Size([768])\n",
      "blocks.3.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.3.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.3.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.3.ff.layers.2.bias torch.Size([768])\n",
      "blocks.3.ln1.weight torch.Size([768])\n",
      "blocks.3.ln1.bias torch.Size([768])\n",
      "blocks.3.ln2.weight torch.Size([768])\n",
      "blocks.3.ln2.bias torch.Size([768])\n",
      "blocks.4.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_Q.bias torch.Size([768])\n",
      "blocks.4.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_K.bias torch.Size([768])\n",
      "blocks.4.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_V.bias torch.Size([768])\n",
      "blocks.4.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.4.attn.out_proj.bias torch.Size([768])\n",
      "blocks.4.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.4.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.4.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.4.ff.layers.2.bias torch.Size([768])\n",
      "blocks.4.ln1.weight torch.Size([768])\n",
      "blocks.4.ln1.bias torch.Size([768])\n",
      "blocks.4.ln2.weight torch.Size([768])\n",
      "blocks.4.ln2.bias torch.Size([768])\n",
      "blocks.5.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_Q.bias torch.Size([768])\n",
      "blocks.5.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_K.bias torch.Size([768])\n",
      "blocks.5.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_V.bias torch.Size([768])\n",
      "blocks.5.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.5.attn.out_proj.bias torch.Size([768])\n",
      "blocks.5.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.5.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.5.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.5.ff.layers.2.bias torch.Size([768])\n",
      "blocks.5.ln1.weight torch.Size([768])\n",
      "blocks.5.ln1.bias torch.Size([768])\n",
      "blocks.5.ln2.weight torch.Size([768])\n",
      "blocks.5.ln2.bias torch.Size([768])\n",
      "blocks.6.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_Q.bias torch.Size([768])\n",
      "blocks.6.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_K.bias torch.Size([768])\n",
      "blocks.6.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_V.bias torch.Size([768])\n",
      "blocks.6.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.6.attn.out_proj.bias torch.Size([768])\n",
      "blocks.6.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.6.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.6.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.6.ff.layers.2.bias torch.Size([768])\n",
      "blocks.6.ln1.weight torch.Size([768])\n",
      "blocks.6.ln1.bias torch.Size([768])\n",
      "blocks.6.ln2.weight torch.Size([768])\n",
      "blocks.6.ln2.bias torch.Size([768])\n",
      "blocks.7.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_Q.bias torch.Size([768])\n",
      "blocks.7.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_K.bias torch.Size([768])\n",
      "blocks.7.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_V.bias torch.Size([768])\n",
      "blocks.7.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.7.attn.out_proj.bias torch.Size([768])\n",
      "blocks.7.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.7.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.7.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.7.ff.layers.2.bias torch.Size([768])\n",
      "blocks.7.ln1.weight torch.Size([768])\n",
      "blocks.7.ln1.bias torch.Size([768])\n",
      "blocks.7.ln2.weight torch.Size([768])\n",
      "blocks.7.ln2.bias torch.Size([768])\n",
      "blocks.8.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_Q.bias torch.Size([768])\n",
      "blocks.8.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_K.bias torch.Size([768])\n",
      "blocks.8.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_V.bias torch.Size([768])\n",
      "blocks.8.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.8.attn.out_proj.bias torch.Size([768])\n",
      "blocks.8.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.8.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.8.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.8.ff.layers.2.bias torch.Size([768])\n",
      "blocks.8.ln1.weight torch.Size([768])\n",
      "blocks.8.ln1.bias torch.Size([768])\n",
      "blocks.8.ln2.weight torch.Size([768])\n",
      "blocks.8.ln2.bias torch.Size([768])\n",
      "blocks.9.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_Q.bias torch.Size([768])\n",
      "blocks.9.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_K.bias torch.Size([768])\n",
      "blocks.9.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_V.bias torch.Size([768])\n",
      "blocks.9.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.9.attn.out_proj.bias torch.Size([768])\n",
      "blocks.9.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.9.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.9.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.9.ff.layers.2.bias torch.Size([768])\n",
      "blocks.9.ln1.weight torch.Size([768])\n",
      "blocks.9.ln1.bias torch.Size([768])\n",
      "blocks.9.ln2.weight torch.Size([768])\n",
      "blocks.9.ln2.bias torch.Size([768])\n",
      "blocks.10.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_Q.bias torch.Size([768])\n",
      "blocks.10.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_K.bias torch.Size([768])\n",
      "blocks.10.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_V.bias torch.Size([768])\n",
      "blocks.10.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.10.attn.out_proj.bias torch.Size([768])\n",
      "blocks.10.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.10.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.10.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.10.ff.layers.2.bias torch.Size([768])\n",
      "blocks.10.ln1.weight torch.Size([768])\n",
      "blocks.10.ln1.bias torch.Size([768])\n",
      "blocks.10.ln2.weight torch.Size([768])\n",
      "blocks.10.ln2.bias torch.Size([768])\n",
      "blocks.11.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_Q.bias torch.Size([768])\n",
      "blocks.11.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_K.bias torch.Size([768])\n",
      "blocks.11.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_V.bias torch.Size([768])\n",
      "blocks.11.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.11.attn.out_proj.bias torch.Size([768])\n",
      "blocks.11.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.11.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.11.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.11.ff.layers.2.bias torch.Size([768])\n",
      "blocks.11.ln1.weight torch.Size([768])\n",
      "blocks.11.ln1.bias torch.Size([768])\n",
      "blocks.11.ln2.weight torch.Size([768])\n",
      "blocks.11.ln2.bias torch.Size([768])\n",
      "final_norm.weight torch.Size([768])\n",
      "final_norm.bias torch.Size([768])\n",
      "out_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:25:50.948739Z",
     "start_time": "2025-06-15T08:25:50.277821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)"
   ],
   "id": "49648386cc30dc35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start ofucc Matth Names sankleg sprayimize inflicting ShallUTC ))) spill Main insanely mph\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download GPT2 from OpenAI",
   "id": "2ba0d473bfd76ff9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:15:17.049393Z",
     "start_time": "2025-06-15T02:15:17.036015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def download_file(url, destination, backup_url=None):\n",
    "    def _attempt_download(download_url):\n",
    "        with urllib.request.urlopen(download_url) as response:\n",
    "            total_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "            if os.path.exists(destination) and os.path.getsize(destination) == total_size:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return True\n",
    "\n",
    "            with tqdm(total=total_size, unit=\"iB\", unit_scale=True, desc=os.path.basename(download_url)) as pbar, \\\n",
    "                 open(destination, \"wb\") as f:\n",
    "                for chunk in iter(lambda: response.read(1024), b\"\"):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "            return True\n",
    "\n",
    "    try:\n",
    "        if _attempt_download(url):\n",
    "            return\n",
    "    except (urllib.error.HTTPError, urllib.error.URLError):\n",
    "        if backup_url:\n",
    "            print(f\"Primary URL failed. Trying backup URL: {backup_url}\")\n",
    "            try:\n",
    "                if _attempt_download(backup_url):\n",
    "                    return\n",
    "            except (urllib.error.HTTPError, urllib.error.URLError):\n",
    "                pass\n",
    "        print(f\"Failed to download from primary URL ({url})\"\n",
    "              + (f\" and backup URL ({backup_url})\" if backup_url else \"\") + \".\\n\"\n",
    "              \"Check your internet connection or the file availability.\\n\"\n",
    "              \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    allowed_sizes = {\"124M\", \"355M\", \"774M\", \"1558M\"}\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size must be one of {allowed_sizes}\")\n",
    "\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    base_url = f\"https://openaipublic.blob.core.windows.net/gpt-2/models/{model_size}/\"\n",
    "    backup_url = f\"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2/{model_size}/\"\n",
    "\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    for fname in filenames:\n",
    "        dst = os.path.join(model_dir, fname)\n",
    "        if os.path.exists(dst):\n",
    "            print(f\"Already exists: {fname}, skipping download.\")\n",
    "            continue\n",
    "        primary = urljoin(base_url, fname)\n",
    "        backup = urljoin(backup_url, fname)\n",
    "        print(f\"Downloading {fname} ...\")\n",
    "        download_file(primary, dst, backup)\n",
    "\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    with open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        settings = json.load(f)\n",
    "\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "    return settings, params\n"
   ],
   "id": "f29d5d3c3d0402a0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:25:59.855760Z",
     "start_time": "2025-06-15T08:25:59.332644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Params:\", params.keys())"
   ],
   "id": "28d0828f52dfce6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: checkpoint, skipping download.\n",
      "Already exists: encoder.json, skipping download.\n",
      "Already exists: hparams.json, skipping download.\n",
      "Already exists: model.ckpt.data-00000-of-00001, skipping download.\n",
      "Already exists: model.ckpt.index, skipping download.\n",
      "Already exists: model.ckpt.meta, skipping download.\n",
      "Already exists: vocab.bpe, skipping download.\n",
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Params: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:26:02.734869Z",
     "start_time": "2025-06-15T08:26:02.688666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def assign_(left, right):\n",
    "    if right is None:\n",
    "        raise ValueError(\"'right' cannot be None\")\n",
    "    right_tensor = torch.as_tensor(right, dtype=left.dtype, device=left.device)\n",
    "    if right_tensor.numel() == 0:\n",
    "        raise ValueError(\"'right' cannot be Empty\")\n",
    "    if left.shape != right_tensor.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {left.shape} vs {right_tensor.shape}\")\n",
    "    with torch.no_grad():\n",
    "        left.copy_(right_tensor)\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    assign_(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    assign_(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b, (block, pblock) in enumerate(zip(gpt.blocks, params[\"blocks\"])):\n",
    "        # Attention QKV\n",
    "        qw, kw, vw = np.split(pblock[\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
    "        qb, kb, vb = np.split(pblock[\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
    "        assign_(block.attn.W_Q.weight, qw.T)\n",
    "        assign_(block.attn.W_K.weight, kw.T)\n",
    "        assign_(block.attn.W_V.weight, vw.T)\n",
    "        assign_(block.attn.W_Q.bias, qb)\n",
    "        assign_(block.attn.W_K.bias, kb)\n",
    "        assign_(block.attn.W_V.bias, vb)\n",
    "\n",
    "        # Attention output projection\n",
    "        assign_(block.attn.out_proj.weight, pblock[\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        assign_(block.attn.out_proj.bias,   pblock[\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # Feedforward\n",
    "        assign_(block.ff.layers[0].weight, pblock[\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        assign_(block.ff.layers[0].bias,   pblock[\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        assign_(block.ff.layers[2].weight, pblock[\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        assign_(block.ff.layers[2].bias,   pblock[\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # LayerNorms\n",
    "        assign_(block.ln1.weight, pblock[\"ln_1\"][\"g\"])\n",
    "        assign_(block.ln1.bias, pblock[\"ln_1\"][\"b\"])\n",
    "        assign_(block.ln2.weight, pblock[\"ln_2\"][\"g\"])\n",
    "        assign_(block.ln2.bias, pblock[\"ln_2\"][\"b\"])\n",
    "\n",
    "    assign_(gpt.final_norm.weight, params[\"g\"])\n",
    "    assign_(gpt.final_norm.bias, params[\"b\"])\n",
    "    assign_(gpt.out_head.weight,  params[\"wte\"])\n"
   ],
   "id": "e83e9805c9622997",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:26:06.417612Z",
     "start_time": "2025-06-15T08:26:06.272488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_weights_into_gpt(model, params)\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n"
   ],
   "id": "7696090694c7c0c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:23:59.896307Z",
     "start_time": "2025-06-15T02:23:59.824877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.mean().item(), param.std().item())\n"
   ],
   "id": "6ae4be9bb4b68198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768]) 0.00037981756031513214 0.14369554817676544\n",
      "pos_emb.weight torch.Size([1024, 768]) -0.0006787165184505284 0.1226913258433342\n",
      "blocks.0.attn.W_Q.weight torch.Size([768, 768]) 0.00015374351642094553 0.2386905699968338\n",
      "blocks.0.attn.W_Q.bias torch.Size([768]) -0.007821076549589634 0.3427544832229614\n",
      "blocks.0.attn.W_K.weight torch.Size([768, 768]) 1.2351122677500825e-05 0.2432965785264969\n",
      "blocks.0.attn.W_K.bias torch.Size([768]) 0.0048723239451646805 0.18297071754932404\n",
      "blocks.0.attn.W_V.weight torch.Size([768, 768]) -5.968316145299468e-06 0.05811797454953194\n",
      "blocks.0.attn.W_V.bias torch.Size([768]) 0.0008267878438346088 0.04772818833589554\n",
      "blocks.0.attn.out_proj.weight torch.Size([768, 768]) -0.0001613790518604219 0.1474614143371582\n",
      "blocks.0.attn.out_proj.bias torch.Size([768]) -0.00691022165119648 0.2589662969112396\n",
      "blocks.0.ff.layers.0.weight torch.Size([3072, 768]) -0.0007485305541194975 0.14116929471492767\n",
      "blocks.0.ff.layers.0.bias torch.Size([3072]) -0.0931621864438057 0.13235801458358765\n",
      "blocks.0.ff.layers.2.weight torch.Size([768, 3072]) 8.009047633095179e-06 0.0879654809832573\n",
      "blocks.0.ff.layers.2.bias torch.Size([768]) -0.0004230523481965065 0.10169976204633713\n",
      "blocks.0.ln1.weight torch.Size([768]) 0.18035894632339478 0.04131494462490082\n",
      "blocks.0.ln1.bias torch.Size([768]) -0.006593453232198954 0.03580174222588539\n",
      "blocks.0.ln2.weight torch.Size([768]) 0.8678296208381653 0.48494789004325867\n",
      "blocks.0.ln2.bias torch.Size([768]) 0.00920353177934885 0.07009701430797577\n",
      "blocks.1.attn.W_Q.weight torch.Size([768, 768]) -0.00015265395632013679 0.15103811025619507\n",
      "blocks.1.attn.W_Q.bias torch.Size([768]) 0.001711765886284411 0.3485022187232971\n",
      "blocks.1.attn.W_K.weight torch.Size([768, 768]) 0.00023460258671548218 0.15902258455753326\n",
      "blocks.1.attn.W_K.bias torch.Size([768]) 0.0008940294501371682 0.08274678885936737\n",
      "blocks.1.attn.W_V.weight torch.Size([768, 768]) 2.3152324502007104e-06 0.10367371886968613\n",
      "blocks.1.attn.W_V.bias torch.Size([768]) -0.00020504526037257165 0.07381219416856766\n",
      "blocks.1.attn.out_proj.weight torch.Size([768, 768]) -8.275873551610857e-05 0.10191785544157028\n",
      "blocks.1.attn.out_proj.bias torch.Size([768]) -0.0010727141052484512 0.104808010160923\n",
      "blocks.1.ff.layers.0.weight torch.Size([3072, 768]) 0.0006420306744985282 0.1307205706834793\n",
      "blocks.1.ff.layers.0.bias torch.Size([3072]) -0.07219841331243515 0.09491100162267685\n",
      "blocks.1.ff.layers.2.weight torch.Size([768, 3072]) 9.794025390874594e-05 0.08719106763601303\n",
      "blocks.1.ff.layers.2.bias torch.Size([768]) 0.0002505724842194468 0.10042908787727356\n",
      "blocks.1.ln1.weight torch.Size([768]) 0.22284089028835297 0.05130758509039879\n",
      "blocks.1.ln1.bias torch.Size([768]) -0.0050234864465892315 0.052436452358961105\n",
      "blocks.1.ln2.weight torch.Size([768]) 0.24269406497478485 0.03164536505937576\n",
      "blocks.1.ln2.bias torch.Size([768]) -0.004073990508913994 0.03919508308172226\n",
      "blocks.2.attn.W_Q.weight torch.Size([768, 768]) 0.0001179432583739981 0.18932299315929413\n",
      "blocks.2.attn.W_Q.bias torch.Size([768]) -0.006064689252525568 0.2589069902896881\n",
      "blocks.2.attn.W_K.weight torch.Size([768, 768]) -4.986231942893937e-05 0.15181933343410492\n",
      "blocks.2.attn.W_K.bias torch.Size([768]) -0.003441582666710019 0.10127473622560501\n",
      "blocks.2.attn.W_V.weight torch.Size([768, 768]) 0.00017838712665252388 0.10502831637859344\n",
      "blocks.2.attn.W_V.bias torch.Size([768]) -0.0021237407345324755 0.0655340775847435\n",
      "blocks.2.attn.out_proj.weight torch.Size([768, 768]) -3.299467061879113e-05 0.08103547245264053\n",
      "blocks.2.attn.out_proj.bias torch.Size([768]) 0.003375591477379203 0.1451117843389511\n",
      "blocks.2.ff.layers.0.weight torch.Size([3072, 768]) -0.005061259493231773 0.13352689146995544\n",
      "blocks.2.ff.layers.0.bias torch.Size([3072]) -0.0928223729133606 0.1066708043217659\n",
      "blocks.2.ff.layers.2.weight torch.Size([768, 3072]) 0.00019714866357389838 0.09308750927448273\n",
      "blocks.2.ff.layers.2.bias torch.Size([768]) 0.002819357207044959 0.11242914199829102\n",
      "blocks.2.ln1.weight torch.Size([768]) 0.24077002704143524 0.07527267932891846\n",
      "blocks.2.ln1.bias torch.Size([768]) -0.00036007841117680073 0.07055915892124176\n",
      "blocks.2.ln2.weight torch.Size([768]) 0.2925874888896942 0.04540559649467468\n",
      "blocks.2.ln2.bias torch.Size([768]) 0.006347864866256714 0.043932873755693436\n",
      "blocks.3.attn.W_Q.weight torch.Size([768, 768]) -0.00021896824182476848 0.16474692523479462\n",
      "blocks.3.attn.W_Q.bias torch.Size([768]) -0.0038416890893131495 0.2126476913690567\n",
      "blocks.3.attn.W_K.weight torch.Size([768, 768]) -5.696068546967581e-05 0.1537768691778183\n",
      "blocks.3.attn.W_K.bias torch.Size([768]) -0.0004758095892611891 0.10474759340286255\n",
      "blocks.3.attn.W_V.weight torch.Size([768, 768]) 0.00020247689099051058 0.09766855090856552\n",
      "blocks.3.attn.W_V.bias torch.Size([768]) 0.0016517918556928635 0.0643523558974266\n",
      "blocks.3.attn.out_proj.weight torch.Size([768, 768]) 3.374144580448046e-05 0.0841251015663147\n",
      "blocks.3.attn.out_proj.bias torch.Size([768]) -0.0015612887218594551 0.10791037976741791\n",
      "blocks.3.ff.layers.0.weight torch.Size([3072, 768]) -0.00595002481713891 0.12953029572963715\n",
      "blocks.3.ff.layers.0.bias torch.Size([3072]) -0.09253177046775818 0.08565898984670639\n",
      "blocks.3.ff.layers.2.weight torch.Size([768, 3072]) 0.00017645648040343076 0.09180603921413422\n",
      "blocks.3.ff.layers.2.bias torch.Size([768]) 0.0021020257845520973 0.11511888355016708\n",
      "blocks.3.ln1.weight torch.Size([768]) 0.3010978400707245 0.05351252108812332\n",
      "blocks.3.ln1.bias torch.Size([768]) 0.0054475064389407635 0.07015811651945114\n",
      "blocks.3.ln2.weight torch.Size([768]) 0.30650773644447327 0.05265355482697487\n",
      "blocks.3.ln2.bias torch.Size([768]) 0.00996524840593338 0.04357629269361496\n",
      "blocks.4.attn.W_Q.weight torch.Size([768, 768]) 0.0002935132652055472 0.1704353392124176\n",
      "blocks.4.attn.W_Q.bias torch.Size([768]) 0.012315389700233936 0.20646966993808746\n",
      "blocks.4.attn.W_K.weight torch.Size([768, 768]) 5.515472003025934e-05 0.15752458572387695\n",
      "blocks.4.attn.W_K.bias torch.Size([768]) 0.004667005501687527 0.3554832935333252\n",
      "blocks.4.attn.W_V.weight torch.Size([768, 768]) 0.00010751151421573013 0.10232267528772354\n",
      "blocks.4.attn.W_V.bias torch.Size([768]) -0.0013783341273665428 0.05650889128446579\n",
      "blocks.4.attn.out_proj.weight torch.Size([768, 768]) -1.1396015906939283e-05 0.09297914057970047\n",
      "blocks.4.attn.out_proj.bias torch.Size([768]) -0.0009013282251544297 0.10049892961978912\n",
      "blocks.4.ff.layers.0.weight torch.Size([3072, 768]) -0.0032636644318699837 0.12971320748329163\n",
      "blocks.4.ff.layers.0.bias torch.Size([3072]) -0.08612614870071411 0.09320200234651566\n",
      "blocks.4.ff.layers.2.weight torch.Size([768, 3072]) 0.0001799796591512859 0.09099913388490677\n",
      "blocks.4.ff.layers.2.bias torch.Size([768]) 0.0016059394693002105 0.13685975968837738\n",
      "blocks.4.ln1.weight torch.Size([768]) 0.31934547424316406 0.04739116132259369\n",
      "blocks.4.ln1.bias torch.Size([768]) 0.007916287519037724 0.06749274581670761\n",
      "blocks.4.ln2.weight torch.Size([768]) 0.2725818455219269 0.043896984308958054\n",
      "blocks.4.ln2.bias torch.Size([768]) 0.0009596580639481544 0.02688399702310562\n",
      "blocks.5.attn.W_Q.weight torch.Size([768, 768]) -0.00026773728313855827 0.14125582575798035\n",
      "blocks.5.attn.W_Q.bias torch.Size([768]) -0.0016259821131825447 0.12543262541294098\n",
      "blocks.5.attn.W_K.weight torch.Size([768, 768]) 4.6552144340239465e-05 0.13614720106124878\n",
      "blocks.5.attn.W_K.bias torch.Size([768]) 0.006209613289684057 0.10714870691299438\n",
      "blocks.5.attn.W_V.weight torch.Size([768, 768]) -0.00010422924242448062 0.10330777615308762\n",
      "blocks.5.attn.W_V.bias torch.Size([768]) -0.001447124988771975 0.04811704158782959\n",
      "blocks.5.attn.out_proj.weight torch.Size([768, 768]) 1.0988791473209858e-05 0.09377486258745193\n",
      "blocks.5.attn.out_proj.bias torch.Size([768]) -0.0011684768833220005 0.11080432683229446\n",
      "blocks.5.ff.layers.0.weight torch.Size([3072, 768]) -0.004193877335637808 0.1267070472240448\n",
      "blocks.5.ff.layers.0.bias torch.Size([3072]) -0.08502542972564697 0.08900804817676544\n",
      "blocks.5.ff.layers.2.weight torch.Size([768, 3072]) 0.00011598864512052387 0.09735694527626038\n",
      "blocks.5.ff.layers.2.bias torch.Size([768]) 0.0009532846161164343 0.1064532995223999\n",
      "blocks.5.ln1.weight torch.Size([768]) 0.3731194734573364 0.04502682015299797\n",
      "blocks.5.ln1.bias torch.Size([768]) 0.011876348406076431 0.04906607046723366\n",
      "blocks.5.ln2.weight torch.Size([768]) 0.27900201082229614 0.05147692933678627\n",
      "blocks.5.ln2.bias torch.Size([768]) 0.00815197080373764 0.03279997780919075\n",
      "blocks.6.attn.W_Q.weight torch.Size([768, 768]) 0.00019029114628210664 0.13397741317749023\n",
      "blocks.6.attn.W_Q.bias torch.Size([768]) 0.004683490376919508 0.18410006165504456\n",
      "blocks.6.attn.W_K.weight torch.Size([768, 768]) -7.406622171401978e-05 0.12758222222328186\n",
      "blocks.6.attn.W_K.bias torch.Size([768]) 0.0015144060598686337 0.1012103334069252\n",
      "blocks.6.attn.W_V.weight torch.Size([768, 768]) 0.00021866592578589916 0.11854671686887741\n",
      "blocks.6.attn.W_V.bias torch.Size([768]) -0.0007340701413340867 0.035576995462179184\n",
      "blocks.6.attn.out_proj.weight torch.Size([768, 768]) 3.6974248359911144e-05 0.11368992179632187\n",
      "blocks.6.attn.out_proj.bias torch.Size([768]) -0.0004363872576504946 0.10605379194021225\n",
      "blocks.6.ff.layers.0.weight torch.Size([3072, 768]) -0.0028191537130624056 0.12635649740695953\n",
      "blocks.6.ff.layers.0.bias torch.Size([3072]) -0.08570227026939392 0.09056127071380615\n",
      "blocks.6.ff.layers.2.weight torch.Size([768, 3072]) 9.54796705627814e-05 0.10733181238174438\n",
      "blocks.6.ff.layers.2.bias torch.Size([768]) 0.0015628753462806344 0.12105625122785568\n",
      "blocks.6.ln1.weight torch.Size([768]) 0.3455983102321625 0.0441710501909256\n",
      "blocks.6.ln1.bias torch.Size([768]) 0.011822459287941456 0.0662064254283905\n",
      "blocks.6.ln2.weight torch.Size([768]) 0.2594689726829529 0.047400206327438354\n",
      "blocks.6.ln2.bias torch.Size([768]) 0.004331209696829319 0.03382179141044617\n",
      "blocks.7.attn.W_Q.weight torch.Size([768, 768]) -0.00045816207421012223 0.1364603042602539\n",
      "blocks.7.attn.W_Q.bias torch.Size([768]) -0.008822117000818253 0.2147248536348343\n",
      "blocks.7.attn.W_K.weight torch.Size([768, 768]) 0.00013387270155362785 0.13045501708984375\n",
      "blocks.7.attn.W_K.bias torch.Size([768]) -0.0036485891323536634 0.09873808920383453\n",
      "blocks.7.attn.W_V.weight torch.Size([768, 768]) 5.1211449317634106e-05 0.1194656491279602\n",
      "blocks.7.attn.W_V.bias torch.Size([768]) -0.00037803445593453944 0.036869797855615616\n",
      "blocks.7.attn.out_proj.weight torch.Size([768, 768]) 2.3895683625596575e-05 0.11391738802194595\n",
      "blocks.7.attn.out_proj.bias torch.Size([768]) -0.00013363065954763442 0.14512065052986145\n",
      "blocks.7.ff.layers.0.weight torch.Size([3072, 768]) -0.00352298840880394 0.12642338871955872\n",
      "blocks.7.ff.layers.0.bias torch.Size([3072]) -0.08847203105688095 0.09064304083585739\n",
      "blocks.7.ff.layers.2.weight torch.Size([768, 3072]) 8.648945367895067e-05 0.1187349334359169\n",
      "blocks.7.ff.layers.2.bias torch.Size([768]) 0.0011925119906663895 0.12881210446357727\n",
      "blocks.7.ln1.weight torch.Size([768]) 0.3565715253353119 0.04378596320748329\n",
      "blocks.7.ln1.bias torch.Size([768]) 0.01434354204684496 0.05914687365293503\n",
      "blocks.7.ln2.weight torch.Size([768]) 0.2560140788555145 0.04705559089779854\n",
      "blocks.7.ln2.bias torch.Size([768]) 0.009183691814541817 0.04571011662483215\n",
      "blocks.8.attn.W_Q.weight torch.Size([768, 768]) -0.0003004284226335585 0.13009199500083923\n",
      "blocks.8.attn.W_Q.bias torch.Size([768]) -0.013181586749851704 0.20185819268226624\n",
      "blocks.8.attn.W_K.weight torch.Size([768, 768]) 0.00017389189451932907 0.12435027956962585\n",
      "blocks.8.attn.W_K.bias torch.Size([768]) -0.0026047879364341497 0.09828340262174606\n",
      "blocks.8.attn.W_V.weight torch.Size([768, 768]) -0.00038428150583058596 0.12628209590911865\n",
      "blocks.8.attn.W_V.bias torch.Size([768]) -0.0008028498850762844 0.036681145429611206\n",
      "blocks.8.attn.out_proj.weight torch.Size([768, 768]) 8.123623047140427e-06 0.12236364185810089\n",
      "blocks.8.attn.out_proj.bias torch.Size([768]) 0.0010857944143936038 0.14007924497127533\n",
      "blocks.8.ff.layers.0.weight torch.Size([3072, 768]) -0.0020665344782173634 0.12728072702884674\n",
      "blocks.8.ff.layers.0.bias torch.Size([3072]) -0.08505804091691971 0.0935441330075264\n",
      "blocks.8.ff.layers.2.weight torch.Size([768, 3072]) 4.6921471948735416e-05 0.13540396094322205\n",
      "blocks.8.ff.layers.2.bias torch.Size([768]) 0.0011560862185433507 0.12735813856124878\n",
      "blocks.8.ln1.weight torch.Size([768]) 0.3352259397506714 0.044541217386722565\n",
      "blocks.8.ln1.bias torch.Size([768]) 0.013251811265945435 0.06870879977941513\n",
      "blocks.8.ln2.weight torch.Size([768]) 0.2566564977169037 0.04127686098217964\n",
      "blocks.8.ln2.bias torch.Size([768]) 0.000384395505534485 0.049368783831596375\n",
      "blocks.9.attn.W_Q.weight torch.Size([768, 768]) 0.00044967501889914274 0.12296558916568756\n",
      "blocks.9.attn.W_Q.bias torch.Size([768]) 0.008131361566483974 0.22075878083705902\n",
      "blocks.9.attn.W_K.weight torch.Size([768, 768]) -0.0003476667625363916 0.11892230808734894\n",
      "blocks.9.attn.W_K.bias torch.Size([768]) -0.007281634956598282 0.09623975306749344\n",
      "blocks.9.attn.W_V.weight torch.Size([768, 768]) -0.00031157408375293016 0.13612067699432373\n",
      "blocks.9.attn.W_V.bias torch.Size([768]) 6.7658256739377975e-06 0.03432562202215195\n",
      "blocks.9.attn.out_proj.weight torch.Size([768, 768]) -2.7884121664101258e-05 0.13681966066360474\n",
      "blocks.9.attn.out_proj.bias torch.Size([768]) 0.0021340707316994667 0.20949006080627441\n",
      "blocks.9.ff.layers.0.weight torch.Size([3072, 768]) -0.0027408814057707787 0.12761937081813812\n",
      "blocks.9.ff.layers.0.bias torch.Size([3072]) -0.08366744965314865 0.09236326813697815\n",
      "blocks.9.ff.layers.2.weight torch.Size([768, 3072]) 3.4792548831319436e-05 0.1558738499879837\n",
      "blocks.9.ff.layers.2.bias torch.Size([768]) 0.0007137329666875303 0.15918298065662384\n",
      "blocks.9.ln1.weight torch.Size([768]) 0.3575561046600342 0.04693679139018059\n",
      "blocks.9.ln1.bias torch.Size([768]) 0.01590331830084324 0.06386822462081909\n",
      "blocks.9.ln2.weight torch.Size([768]) 0.26497504115104675 0.041519638150930405\n",
      "blocks.9.ln2.bias torch.Size([768]) 0.006400738377124071 0.04571041837334633\n",
      "blocks.10.attn.W_Q.weight torch.Size([768, 768]) 0.00031771004432812333 0.11865982413291931\n",
      "blocks.10.attn.W_Q.bias torch.Size([768]) 0.008911840617656708 0.2265850007534027\n",
      "blocks.10.attn.W_K.weight torch.Size([768, 768]) -0.00012976815924048424 0.11467549949884415\n",
      "blocks.10.attn.W_K.bias torch.Size([768]) -0.002668120665475726 0.09832144528627396\n",
      "blocks.10.attn.W_V.weight torch.Size([768, 768]) 8.956639067037031e-05 0.14459584653377533\n",
      "blocks.10.attn.W_V.bias torch.Size([768]) -0.0014112890930846334 0.04762532189488411\n",
      "blocks.10.attn.out_proj.weight torch.Size([768, 768]) -8.798572821433481e-07 0.14662745594978333\n",
      "blocks.10.attn.out_proj.bias torch.Size([768]) 0.0020241064485162497 0.2322089970111847\n",
      "blocks.10.ff.layers.0.weight torch.Size([3072, 768]) -0.0032080465462058783 0.12764813005924225\n",
      "blocks.10.ff.layers.0.bias torch.Size([3072]) -0.07652498036623001 0.09122857451438904\n",
      "blocks.10.ff.layers.2.weight torch.Size([768, 3072]) 6.105272404965945e-06 0.17814528942108154\n",
      "blocks.10.ff.layers.2.bias torch.Size([768]) 0.0016591990133747458 0.19404050707817078\n",
      "blocks.10.ln1.weight torch.Size([768]) 0.37820783257484436 0.055699631571769714\n",
      "blocks.10.ln1.bias torch.Size([768]) 0.018612800166010857 0.05506131052970886\n",
      "blocks.10.ln2.weight torch.Size([768]) 0.2896941602230072 0.05117756500840187\n",
      "blocks.10.ln2.bias torch.Size([768]) 0.021159043535590172 0.04487457126379013\n",
      "blocks.11.attn.W_Q.weight torch.Size([768, 768]) -1.1366326361894608e-05 0.10982047021389008\n",
      "blocks.11.attn.W_Q.bias torch.Size([768]) 0.0005513962241820991 0.18319359421730042\n",
      "blocks.11.attn.W_K.weight torch.Size([768, 768]) -4.2440758988959715e-05 0.10549236834049225\n",
      "blocks.11.attn.W_K.bias torch.Size([768]) 0.0015315081691369414 0.08636081963777542\n",
      "blocks.11.attn.W_V.weight torch.Size([768, 768]) 0.0002158462448278442 0.16225944459438324\n",
      "blocks.11.attn.W_V.bias torch.Size([768]) 0.00011266752699157223 0.05450312793254852\n",
      "blocks.11.attn.out_proj.weight torch.Size([768, 768]) -5.3778097935719416e-05 0.1819266527891159\n",
      "blocks.11.attn.out_proj.bias torch.Size([768]) -0.021505361422896385 0.46894726157188416\n",
      "blocks.11.ff.layers.0.weight torch.Size([3072, 768]) -0.001846416387706995 0.13000451028347015\n",
      "blocks.11.ff.layers.0.bias torch.Size([3072]) -0.06411425024271011 0.0930408164858818\n",
      "blocks.11.ff.layers.2.weight torch.Size([768, 3072]) -0.00043532054405659437 0.19821906089782715\n",
      "blocks.11.ff.layers.2.bias torch.Size([768]) 0.000971626432146877 0.10824692994356155\n",
      "blocks.11.ln1.weight torch.Size([768]) 0.4786931276321411 0.06516604125499725\n",
      "blocks.11.ln1.bias torch.Size([768]) 0.023284194990992546 0.05674212425947189\n",
      "blocks.11.ln2.weight torch.Size([768]) 0.5041061043739319 0.09001091122627258\n",
      "blocks.11.ln2.bias torch.Size([768]) 0.009192791767418385 0.03916310518980026\n",
      "final_norm.weight torch.Size([768]) 1.5078086853027344 1.3910776376724243\n",
      "final_norm.bias torch.Size([768]) -0.003138466738164425 0.4196469485759735\n",
      "out_head.weight torch.Size([50257, 768]) 0.00037981756031513214 0.14369554817676544\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:49:36.416965Z",
     "start_time": "2025-06-15T03:49:35.406847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import complete_text\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)"
   ],
   "id": "a6c89718f5289428",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of the game, and then the game ends.\n",
      "\n",
      "The game is a\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:26:12.152899Z",
     "start_time": "2025-06-15T08:26:11.323346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "torch.manual_seed(123)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(\"at the start of\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", tensor_to_text(token_ids, tokenizer))"
   ],
   "id": "84204d1eb2f82b26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of an international series of events. You don't have to worry about who has\n"
     ]
    }
   ],
   "execution_count": 177
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Instruction Finetuning",
   "id": "d988216ca3645214"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:16:10.339298Z",
     "start_time": "2025-06-15T07:16:06.573976Z"
    }
   },
   "cell_type": "code",
   "source": "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
   "id": "3a43e589337beead",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-15 15:16:07--  https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 30.100.0.25, 30.100.0.26, 30.100.0.23, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|30.100.0.25|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 22773992 (22M) [text/plain]\r\n",
      "Saving to: alpaca_data.json\r\n",
      "\r\n",
      "alpaca_data.json    100%[===================>]  21.72M  21.9MB/s    in 1.0s    \r\n",
      "\r\n",
      "2025-06-15 15:16:10 (21.9 MB/s) - alpaca_data.json saved [22773992/22773992]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:09:14.364279Z",
     "start_time": "2025-06-15T09:09:14.190164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"alpaca_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#just take first 1000\n",
    "data = data[:1000]\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(data)"
   ],
   "id": "2b051862bc4b6205",
   "outputs": [],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:09:18.434337Z",
     "start_time": "2025-06-15T09:09:18.383945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_input(entry):\n",
    "    instruction = entry.get(\"instruction\", \"\").strip()\n",
    "    input_section = entry.get(\"input\", \"\").strip()\n",
    "\n",
    "    parts = [\n",
    "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n",
    "        \"\\n\\n### Instruction:\\n\" + instruction,\n",
    "    ]\n",
    "\n",
    "    if input_section:\n",
    "        parts.append(\"\\n\\n### Input:\\n\" + input_section)\n",
    "\n",
    "    return \"\".join(parts)\n"
   ],
   "id": "d3baaa33104e8709",
   "outputs": [],
   "execution_count": 213
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:22:47.775567Z",
     "start_time": "2025-06-15T07:22:47.740263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ],
   "id": "8e2ea8dda1848a37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Edit the following sentence to make it more concise.\n",
      "\n",
      "### Input:\n",
      "He ran to the bus stop in order to catch the bus that was due to arrive in five minutes.\n",
      "\n",
      "### Response:\n",
      "He ran to the bus stop, due to arrive in five minutes.\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:22:51.062923Z",
     "start_time": "2025-06-15T07:22:51.025334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = len(data)\n",
    "train_data = data[:int(n * 0.85)]\n",
    "test_data = data[int(n * 0.85):int(n * 0.95)]\n",
    "val_data = data[int(n * 0.95):]"
   ],
   "id": "2866573989336046",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:23:05.114700Z",
     "start_time": "2025-06-15T07:23:05.081727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ],
   "id": "1790ba80fbfdd587",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 850\n",
      "Validation set length: 50\n",
      "Test set length: 100\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:43:43.013600Z",
     "start_time": "2025-06-15T07:43:42.895540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(train_dataset))\n",
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ],
   "id": "dd4c34cccb5194d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n",
      "Train loader:\n",
      "torch.Size([8, 110]) torch.Size([8, 110])\n",
      "torch.Size([8, 146]) torch.Size([8, 146])\n",
      "torch.Size([8, 141]) torch.Size([8, 141])\n",
      "torch.Size([8, 147]) torch.Size([8, 147])\n",
      "torch.Size([8, 107]) torch.Size([8, 107])\n",
      "torch.Size([8, 249]) torch.Size([8, 249])\n",
      "torch.Size([8, 231]) torch.Size([8, 231])\n",
      "torch.Size([8, 199]) torch.Size([8, 199])\n",
      "torch.Size([8, 185]) torch.Size([8, 185])\n",
      "torch.Size([8, 130]) torch.Size([8, 130])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 147]) torch.Size([8, 147])\n",
      "torch.Size([8, 259]) torch.Size([8, 259])\n",
      "torch.Size([8, 109]) torch.Size([8, 109])\n",
      "torch.Size([8, 201]) torch.Size([8, 201])\n",
      "torch.Size([8, 199]) torch.Size([8, 199])\n",
      "torch.Size([8, 201]) torch.Size([8, 201])\n",
      "torch.Size([8, 307]) torch.Size([8, 307])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 191]) torch.Size([8, 191])\n",
      "torch.Size([8, 155]) torch.Size([8, 155])\n",
      "torch.Size([8, 292]) torch.Size([8, 292])\n",
      "torch.Size([8, 111]) torch.Size([8, 111])\n",
      "torch.Size([8, 122]) torch.Size([8, 122])\n",
      "torch.Size([8, 273]) torch.Size([8, 273])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 208]) torch.Size([8, 208])\n",
      "torch.Size([8, 160]) torch.Size([8, 160])\n",
      "torch.Size([8, 326]) torch.Size([8, 326])\n",
      "torch.Size([8, 135]) torch.Size([8, 135])\n",
      "torch.Size([8, 220]) torch.Size([8, 220])\n",
      "torch.Size([8, 200]) torch.Size([8, 200])\n",
      "torch.Size([8, 182]) torch.Size([8, 182])\n",
      "torch.Size([8, 172]) torch.Size([8, 172])\n",
      "torch.Size([8, 131]) torch.Size([8, 131])\n",
      "torch.Size([8, 166]) torch.Size([8, 166])\n",
      "torch.Size([8, 175]) torch.Size([8, 175])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 280]) torch.Size([8, 280])\n",
      "torch.Size([8, 145]) torch.Size([8, 145])\n",
      "torch.Size([8, 113]) torch.Size([8, 113])\n",
      "torch.Size([8, 223]) torch.Size([8, 223])\n",
      "torch.Size([8, 229]) torch.Size([8, 229])\n",
      "torch.Size([8, 310]) torch.Size([8, 310])\n",
      "torch.Size([8, 164]) torch.Size([8, 164])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 203]) torch.Size([8, 203])\n",
      "torch.Size([8, 209]) torch.Size([8, 209])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 184]) torch.Size([8, 184])\n",
      "torch.Size([8, 133]) torch.Size([8, 133])\n",
      "torch.Size([8, 197]) torch.Size([8, 197])\n",
      "torch.Size([8, 213]) torch.Size([8, 213])\n",
      "torch.Size([8, 192]) torch.Size([8, 192])\n",
      "torch.Size([8, 118]) torch.Size([8, 118])\n",
      "torch.Size([8, 177]) torch.Size([8, 177])\n",
      "torch.Size([8, 281]) torch.Size([8, 281])\n",
      "torch.Size([8, 106]) torch.Size([8, 106])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 427]) torch.Size([8, 427])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 115]) torch.Size([8, 115])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 114]) torch.Size([8, 114])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 126]) torch.Size([8, 126])\n",
      "torch.Size([8, 225]) torch.Size([8, 225])\n",
      "torch.Size([8, 164]) torch.Size([8, 164])\n",
      "torch.Size([8, 138]) torch.Size([8, 138])\n",
      "torch.Size([8, 257]) torch.Size([8, 257])\n",
      "torch.Size([8, 342]) torch.Size([8, 342])\n",
      "torch.Size([8, 137]) torch.Size([8, 137])\n",
      "torch.Size([8, 185]) torch.Size([8, 185])\n",
      "torch.Size([8, 265]) torch.Size([8, 265])\n",
      "torch.Size([8, 279]) torch.Size([8, 279])\n",
      "torch.Size([8, 114]) torch.Size([8, 114])\n",
      "torch.Size([8, 168]) torch.Size([8, 168])\n",
      "torch.Size([8, 274]) torch.Size([8, 274])\n",
      "torch.Size([8, 127]) torch.Size([8, 127])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 160]) torch.Size([8, 160])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 132]) torch.Size([8, 132])\n",
      "torch.Size([8, 168]) torch.Size([8, 168])\n",
      "torch.Size([8, 198]) torch.Size([8, 198])\n",
      "torch.Size([8, 233]) torch.Size([8, 233])\n",
      "torch.Size([8, 178]) torch.Size([8, 178])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 182]) torch.Size([8, 182])\n",
      "torch.Size([8, 230]) torch.Size([8, 230])\n",
      "torch.Size([8, 169]) torch.Size([8, 169])\n",
      "torch.Size([8, 426]) torch.Size([8, 426])\n",
      "torch.Size([8, 179]) torch.Size([8, 179])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 190]) torch.Size([8, 190])\n",
      "torch.Size([8, 269]) torch.Size([8, 269])\n",
      "torch.Size([8, 271]) torch.Size([8, 271])\n",
      "torch.Size([8, 206]) torch.Size([8, 206])\n",
      "torch.Size([8, 124]) torch.Size([8, 124])\n",
      "torch.Size([8, 344]) torch.Size([8, 344])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 231]) torch.Size([8, 231])\n",
      "torch.Size([8, 119]) torch.Size([8, 119])\n",
      "torch.Size([8, 126]) torch.Size([8, 126])\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:59:50.829037Z",
     "start_time": "2025-06-15T07:59:50.730241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from functools import partial\n",
    "\n",
    "device = \"cpu\"  # or \"cuda\" if available\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(\n",
    "                format_input(entry) + f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            )\n",
    "            for entry in data\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_texts[idx]\n",
    "\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    max_len = min(\n",
    "        max(len(seq) + 1 for seq in batch),\n",
    "        allowed_max_length or float('inf')\n",
    "    )\n",
    "\n",
    "    input_tensors, label_tensors = [], []\n",
    "\n",
    "    for seq in batch:\n",
    "        seq = seq + [pad_token_id]\n",
    "        padded = seq + [pad_token_id] * (max_len - len(seq))\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1], dtype=torch.long)\n",
    "        labels = torch.tensor(padded[1:], dtype=torch.long)\n",
    "\n",
    "        # Mask padding in labels except the first one\n",
    "        pad_mask = (labels == pad_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(pad_mask) > 1:\n",
    "            labels[pad_mask[1:]] = ignore_index\n",
    "\n",
    "        input_tensors.append(inputs)\n",
    "        label_tensors.append(labels)\n",
    "\n",
    "    return (\n",
    "        torch.stack(input_tensors).to(device),\n",
    "        torch.stack(label_tensors).to(device)\n",
    "    )\n",
    "\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")\n"
   ],
   "id": "7370d205bf9e7f88",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:10:19.252354Z",
     "start_time": "2025-06-15T09:10:19.173679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=True,drop_last=True,num_workers=0)\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=False,drop_last=False,num_workers=0)\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=False,drop_last=False,num_workers=0)"
   ],
   "id": "75e168056c043dbb",
   "outputs": [],
   "execution_count": 214
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:11:22.162138Z",
     "start_time": "2025-06-15T08:11:22.102899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)\n",
    "print(\"inputs: \",inputs[0])\n",
    "print(\"targets: \",targets[0])"
   ],
   "id": "fe4bac319d2bb9a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader:\n",
      "torch.Size([8, 225]) torch.Size([8, 225])\n",
      "torch.Size([8, 190]) torch.Size([8, 190])\n",
      "torch.Size([8, 280]) torch.Size([8, 280])\n",
      "torch.Size([8, 130]) torch.Size([8, 130])\n",
      "torch.Size([8, 209]) torch.Size([8, 209])\n",
      "torch.Size([8, 240]) torch.Size([8, 240])\n",
      "torch.Size([8, 201]) torch.Size([8, 201])\n",
      "torch.Size([8, 162]) torch.Size([8, 162])\n",
      "torch.Size([8, 152]) torch.Size([8, 152])\n",
      "torch.Size([8, 115]) torch.Size([8, 115])\n",
      "torch.Size([8, 426]) torch.Size([8, 426])\n",
      "torch.Size([8, 178]) torch.Size([8, 178])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 265]) torch.Size([8, 265])\n",
      "torch.Size([8, 192]) torch.Size([8, 192])\n",
      "torch.Size([8, 129]) torch.Size([8, 129])\n",
      "torch.Size([8, 185]) torch.Size([8, 185])\n",
      "torch.Size([8, 191]) torch.Size([8, 191])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 271]) torch.Size([8, 271])\n",
      "torch.Size([8, 279]) torch.Size([8, 279])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 208]) torch.Size([8, 208])\n",
      "torch.Size([8, 168]) torch.Size([8, 168])\n",
      "torch.Size([8, 204]) torch.Size([8, 204])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 344]) torch.Size([8, 344])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 123]) torch.Size([8, 123])\n",
      "torch.Size([8, 274]) torch.Size([8, 274])\n",
      "torch.Size([8, 273]) torch.Size([8, 273])\n",
      "torch.Size([8, 196]) torch.Size([8, 196])\n",
      "torch.Size([8, 210]) torch.Size([8, 210])\n",
      "torch.Size([8, 143]) torch.Size([8, 143])\n",
      "torch.Size([8, 167]) torch.Size([8, 167])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 191]) torch.Size([8, 191])\n",
      "torch.Size([8, 269]) torch.Size([8, 269])\n",
      "torch.Size([8, 160]) torch.Size([8, 160])\n",
      "torch.Size([8, 223]) torch.Size([8, 223])\n",
      "torch.Size([8, 194]) torch.Size([8, 194])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 166]) torch.Size([8, 166])\n",
      "torch.Size([8, 281]) torch.Size([8, 281])\n",
      "torch.Size([8, 182]) torch.Size([8, 182])\n",
      "torch.Size([8, 218]) torch.Size([8, 218])\n",
      "torch.Size([8, 229]) torch.Size([8, 229])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 138]) torch.Size([8, 138])\n",
      "torch.Size([8, 259]) torch.Size([8, 259])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 326]) torch.Size([8, 326])\n",
      "torch.Size([8, 138]) torch.Size([8, 138])\n",
      "torch.Size([8, 220]) torch.Size([8, 220])\n",
      "torch.Size([8, 203]) torch.Size([8, 203])\n",
      "torch.Size([8, 147]) torch.Size([8, 147])\n",
      "torch.Size([8, 151]) torch.Size([8, 151])\n",
      "torch.Size([8, 184]) torch.Size([8, 184])\n",
      "torch.Size([8, 117]) torch.Size([8, 117])\n",
      "torch.Size([8, 169]) torch.Size([8, 169])\n",
      "torch.Size([8, 206]) torch.Size([8, 206])\n",
      "torch.Size([8, 144]) torch.Size([8, 144])\n",
      "torch.Size([8, 176]) torch.Size([8, 176])\n",
      "torch.Size([8, 307]) torch.Size([8, 307])\n",
      "torch.Size([8, 129]) torch.Size([8, 129])\n",
      "torch.Size([8, 427]) torch.Size([8, 427])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 201]) torch.Size([8, 201])\n",
      "torch.Size([8, 233]) torch.Size([8, 233])\n",
      "torch.Size([8, 168]) torch.Size([8, 168])\n",
      "torch.Size([8, 310]) torch.Size([8, 310])\n",
      "torch.Size([8, 116]) torch.Size([8, 116])\n",
      "torch.Size([8, 198]) torch.Size([8, 198])\n",
      "torch.Size([8, 231]) torch.Size([8, 231])\n",
      "torch.Size([8, 342]) torch.Size([8, 342])\n",
      "torch.Size([8, 155]) torch.Size([8, 155])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 292]) torch.Size([8, 292])\n",
      "torch.Size([8, 115]) torch.Size([8, 115])\n",
      "torch.Size([8, 133]) torch.Size([8, 133])\n",
      "torch.Size([8, 257]) torch.Size([8, 257])\n",
      "torch.Size([8, 143]) torch.Size([8, 143])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 144]) torch.Size([8, 144])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 136]) torch.Size([8, 136])\n",
      "torch.Size([8, 172]) torch.Size([8, 172])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 145]) torch.Size([8, 145])\n",
      "torch.Size([8, 134]) torch.Size([8, 134])\n",
      "torch.Size([8, 191]) torch.Size([8, 191])\n",
      "torch.Size([8, 136]) torch.Size([8, 136])\n",
      "torch.Size([8, 140]) torch.Size([8, 140])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 132]) torch.Size([8, 132])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 99]) torch.Size([8, 99])\n",
      "torch.Size([8, 153]) torch.Size([8, 153])\n",
      "torch.Size([8, 177]) torch.Size([8, 177])\n",
      "torch.Size([8, 161]) torch.Size([8, 161])\n",
      "torch.Size([8, 199]) torch.Size([8, 199])\n",
      "torch.Size([8, 231]) torch.Size([8, 231])\n",
      "torch.Size([8, 144]) torch.Size([8, 144])\n",
      "torch.Size([8, 133]) torch.Size([8, 133])\n",
      "inputs:  tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 16594,   257,  6770,   286,   366, 38611,\n",
      "        25444,  1911,   198,   198, 21017, 18261,    25,   198, 27248, 25444,\n",
      "          318,   257,  4590, 12857,  3788,  4166,   416, 21771,   326,   318,\n",
      "          973,   284,  2987,  4263,   393,  2251,  3048,   416, 29349,   290,\n",
      "        19771,  3354,   286,   262,  4683,  4875,  2939,    13, 29153, 13536,\n",
      "         2985,   284,  4532,   262, 23755,    11,  3124,    11, 12019,    11,\n",
      "          290, 11743,   286,   281,  2939,    11,   355,   880,   355,   284,\n",
      "         2251,  2420,    11,  2251,   513,    35,  5563,    11,   751,  4875,\n",
      "         3048,   290,  4174, 16628,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256])\n",
      "targets:  tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 16594,   257,  6770,   286,   366, 38611, 25444,\n",
      "         1911,   198,   198, 21017, 18261,    25,   198, 27248, 25444,   318,\n",
      "          257,  4590, 12857,  3788,  4166,   416, 21771,   326,   318,   973,\n",
      "          284,  2987,  4263,   393,  2251,  3048,   416, 29349,   290, 19771,\n",
      "         3354,   286,   262,  4683,  4875,  2939,    13, 29153, 13536,  2985,\n",
      "          284,  4532,   262, 23755,    11,  3124,    11, 12019,    11,   290,\n",
      "        11743,   286,   281,  2939,    11,   355,   880,   355,   284,  2251,\n",
      "         2420,    11,  2251,   513,    35,  5563,    11,   751,  4875,  3048,\n",
      "          290,  4174, 16628,    13, 50256,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100])\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:10:25.763439Z",
     "start_time": "2025-06-15T09:10:25.029720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=1024,\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = tensor_to_text(token_ids, tokenizer)\n",
    "print(generated_text)"
   ],
   "id": "3520f02e1e624290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Re-write the following sentence to use a different verb\n",
      "\n",
      "### Input:\n",
      "I will read the book tomorrow\n",
      "\n",
      "### Response:\n",
      "Tomorrow is tomorrow.\n"
     ]
    }
   ],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:10:37.442859Z",
     "start_time": "2025-06-15T09:10:32.568854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import loss_loader\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "id": "bda02c08bc5dcfa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9082719683647156\n",
      "Validation loss: 1.7060818195343017\n"
     ]
    }
   ],
   "execution_count": 216
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train as normal",
   "id": "dccf9e269d59d3f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:25:10.285155Z",
     "start_time": "2025-06-15T09:12:00.519376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time\n",
    "from gpt2_v2 import train_model_simple, build_tokenizer\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_num_threads(12)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# FineTune the model\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=format_input(val_data[0]),\n",
    "    tokenizer=build_tokenizer()\n",
    ")\n",
    "\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f\"Training completed in {elapsed:.2f} minutes.\")\n"
   ],
   "id": "a297eb6b0a9efce3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000005): Train loss 1.260, Val loss 1.836, Tokens seen: 5208\n",
      "Ep 1 (Step 000010): Train loss 1.253, Val loss 1.853, Tokens seen: 13160\n",
      "Ep 1 (Step 000015): Train loss 1.218, Val loss 1.867, Tokens seen: 20328\n",
      "Ep 1 (Step 000020): Train loss 1.360, Val loss 1.871, Tokens seen: 28776\n",
      "Ep 1 (Step 000025): Train loss 1.240, Val loss 1.897, Tokens seen: 36400\n",
      "Ep 1 (Step 000030): Train loss 1.135, Val loss 1.875, Tokens seen: 44400\n",
      "Ep 1 (Step 000035): Train loss 1.251, Val loss 1.905, Tokens seen: 51640\n",
      "Ep 1 (Step 000040): Train loss 1.255, Val loss 1.917, Tokens seen: 59032\n",
      "Ep 1 (Step 000045): Train loss 1.269, Val loss 1.908, Tokens seen: 67344\n",
      "Ep 1 (Step 000050): Train loss 1.211, Val loss 1.911, Tokens seen: 74984\n",
      "Ep 1 (Step 000055): Train loss 1.381, Val loss 1.904, Tokens seen: 81808\n",
      "Ep 1 (Step 000060): Train loss 1.223, Val loss 1.912, Tokens seen: 91120\n",
      "Ep 1 (Step 000065): Train loss 1.165, Val loss 1.915, Tokens seen: 96880\n",
      "Ep 1 (Step 000070): Train loss 1.301, Val loss 1.925, Tokens seen: 104160\n",
      "Ep 1 (Step 000075): Train loss 1.270, Val loss 1.925, Tokens seen: 113824\n",
      "Ep 1 (Step 000080): Train loss 1.388, Val loss 1.914, Tokens seen: 120904\n",
      "Ep 1 (Step 000085): Train loss 1.221, Val loss 1.917, Tokens seen: 127224\n",
      "Ep 1 (Step 000090): Train loss 1.270, Val loss 1.892, Tokens seen: 134736\n",
      "Ep 1 (Step 000095): Train loss 1.251, Val loss 1.876, Tokens seen: 144072\n",
      "Ep 1 (Step 000100): Train loss 1.337, Val loss 1.886, Tokens seen: 152552\n",
      "Ep 1 (Step 000105): Train loss 1.303, Val loss 1.859, Tokens seen: 160736\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Re-write the following sentence to use a different verb\n",
      "\n",
      "### Input:\n",
      "I will read the book tomorrow\n",
      "\n",
      "### Response:\n",
      "Tomorrow is tomorrow.<|endoftext|>The following article is an opinion article written\n",
      " Checkpoint saved: checkpoints/checkpoint_epoch1.pth\n",
      "Ep 2 (Step 000110): Train loss 1.239, Val loss 1.862, Tokens seen: 167456\n",
      "Ep 2 (Step 000115): Train loss 1.335, Val loss 1.863, Tokens seen: 174848\n",
      "Ep 2 (Step 000120): Train loss 1.174, Val loss 1.874, Tokens seen: 182032\n",
      "Ep 2 (Step 000125): Train loss 1.269, Val loss 1.875, Tokens seen: 188920\n",
      "Ep 2 (Step 000130): Train loss 1.258, Val loss 1.899, Tokens seen: 194624\n",
      "Ep 2 (Step 000135): Train loss 1.263, Val loss 1.895, Tokens seen: 203032\n",
      "Ep 2 (Step 000140): Train loss 1.338, Val loss 1.921, Tokens seen: 211480\n",
      "Ep 2 (Step 000145): Train loss 1.275, Val loss 1.935, Tokens seen: 219552\n",
      "Ep 2 (Step 000150): Train loss 1.282, Val loss 1.901, Tokens seen: 226232\n",
      "Ep 2 (Step 000155): Train loss 1.289, Val loss 1.908, Tokens seen: 236208\n",
      "Ep 2 (Step 000160): Train loss 1.149, Val loss 1.912, Tokens seen: 242680\n",
      "Ep 2 (Step 000165): Train loss 1.283, Val loss 1.923, Tokens seen: 250152\n",
      "Ep 2 (Step 000170): Train loss 1.167, Val loss 1.912, Tokens seen: 257568\n",
      "Ep 2 (Step 000175): Train loss 1.218, Val loss 1.925, Tokens seen: 269352\n",
      "Ep 2 (Step 000180): Train loss 1.226, Val loss 1.932, Tokens seen: 275824\n",
      "Ep 2 (Step 000185): Train loss 1.242, Val loss 1.958, Tokens seen: 283984\n",
      "Ep 2 (Step 000190): Train loss 1.188, Val loss 1.944, Tokens seen: 291208\n",
      "Ep 2 (Step 000195): Train loss 1.251, Val loss 1.930, Tokens seen: 298056\n",
      "Ep 2 (Step 000200): Train loss 1.282, Val loss 1.910, Tokens seen: 304256\n",
      "Ep 2 (Step 000205): Train loss 1.241, Val loss 1.935, Tokens seen: 313488\n",
      "Ep 2 (Step 000210): Train loss 1.114, Val loss 1.903, Tokens seen: 321464\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Re-write the following sentence to use a different verb\n",
      "\n",
      "### Input:\n",
      "I will read the book tomorrow\n",
      "\n",
      "### Response:\n",
      "Tomorrow is tomorrow.<|endoftext|>The following article is an opinion article written\n",
      " Checkpoint saved: checkpoints/checkpoint_epoch2.pth\n",
      " Training complete. Final model saved: checkpoints/final_model.pth\n",
      "Training completed in 13.16 minutes.\n"
     ]
    }
   ],
   "execution_count": 217
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:25:17.157304Z",
     "start_time": "2025-06-15T09:25:16.972914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ],
   "id": "fe49e1158904c239",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:26:48.056964Z",
     "start_time": "2025-06-15T09:26:46.378604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[1])\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=1024,\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = tensor_to_text(token_ids, tokenizer)\n",
    "print(generated_text)"
   ],
   "id": "1818eb1a3d9ca2c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Propose a suitable title for the below article\n",
      "\n",
      "### Input:\n",
      "This article discusses the current state of the music industry and how technology has had an impact on its evolution.\n",
      "\n",
      "### Response:\n",
      "The current state of the music industry and how technology has had an impact on its evolution is described in the article.\n"
     ]
    }
   ],
   "execution_count": 221
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save model",
   "id": "ffe50f3c0d3864f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:34:40.857986Z",
     "start_time": "2025-06-15T09:32:36.904855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def generate_response(entry, model):\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=text_to_tensor(input_text, tokenizer),\n",
    "        max_new_tokens=35,\n",
    "        context_size=1024,\n",
    "        eos_id=50256,\n",
    "    )\n",
    "    generated_text = tensor_to_text(token_ids, tokenizer)\n",
    "    response = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "    return response\n",
    "\n",
    "# Generate and attach responses\n",
    "for entry in tqdm(test_data, desc=\"Generating responses\"):\n",
    "    entry[\"model_response\"] = generate_response(entry, model)\n",
    "\n",
    "# Save to file\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as f:\n",
    "    json.dump(test_data, f, indent=4)\n"
   ],
   "id": "97b67f7b32b44749",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|| 100/100 [02:03<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:38:54.211213Z",
     "start_time": "2025-06-15T09:38:54.163078Z"
    }
   },
   "cell_type": "code",
   "source": "print(test_data[0])",
   "id": "3f9cb2c8c0a22d6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Design a logo for a child care center.', 'input': '', 'output': 'The logo should be warm, playful and inviting. It should utilize a font that is easy to read and appealing to children. I would suggest a combination of vibrant colors, such as yellow, blue and green. It could include an image of a happy kid or an illustration of a sun and clouds. The logo should also convey a sense of security and safety, as the center is aiming to provide a safe, nurturing environment for kids.', 'model_response': 'The logo should be simple and simple, with a simple design and a simple message. It should be simple and simple, with a simple message and'}\n"
     ]
    }
   ],
   "execution_count": 228
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:40:09.032681Z",
     "start_time": "2025-06-15T09:40:06.755629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_name = \"gpt2-124M-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ],
   "id": "2e5f0b4a2aae83e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-124M-sft.pth\n"
     ]
    }
   ],
   "execution_count": 229
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate the model",
   "id": "2d16eb445d3fd5da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:45:22.296359Z",
     "start_time": "2025-06-15T09:45:22.153252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import psutil\n",
    "\n",
    "def is_process_running(name_substr: str) -> bool:\n",
    "    \"\"\"Check if any running process contains the given substring in its name.\"\"\"\n",
    "    return any(name_substr.lower() in (proc.info[\"name\"] or \"\").lower()\n",
    "               for proc in psutil.process_iter([\"name\"]))\n",
    "\n",
    "if not is_process_running(\"ollama\"):\n",
    "    raise RuntimeError(\" Ollama not running. Please launch it before proceeding.\")\n",
    "\n",
    "print(\" Ollama is running.\")\n"
   ],
   "id": "56c837af437f0965",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ollama is running.\n"
     ]
    }
   ],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:49:32.572991Z",
     "start_time": "2025-06-15T09:49:32.480140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "    prompt: str,\n",
    "    model: str = \"llama3\",\n",
    "    url: str = \"http://localhost:11434/api/chat\",\n",
    "    seed: int = 123,\n",
    "    temperature: float = 0.0,\n",
    "    num_ctx: int = 2048\n",
    ") -> str:\n",
    "    \"\"\"Send a prompt to a local chat model and return the generated response.\"\"\"\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"options\": {\n",
    "            \"seed\": seed,\n",
    "            \"temperature\": temperature,\n",
    "            \"num_ctx\": num_ctx\n",
    "        }\n",
    "    }\n",
    "\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=json.dumps(data).encode(\"utf-8\"),\n",
    "        method=\"POST\",\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    response_text = []\n",
    "\n",
    "    try:\n",
    "        with urllib.request.urlopen(request) as response:\n",
    "            for line in response:\n",
    "                line = line.decode(\"utf-8\").strip()\n",
    "                if line:\n",
    "                    message_chunk = json.loads(line)\n",
    "                    content = message_chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    response_text.append(content)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to query model: {e}\")\n",
    "\n",
    "    return \"\".join(response_text)\n"
   ],
   "id": "576103fbe032757",
   "outputs": [],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:50:17.141779Z",
     "start_time": "2025-06-15T09:49:59.651359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = query_model(\"What do Llamas eat?\", \"llama3\")\n",
    "print(result)"
   ],
   "id": "8034456625eb72ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:55:33.008610Z",
     "start_time": "2025-06-15T09:55:32.912473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_model_scores(data, response_key=\"model_response\", model=\"llama3\"):\n",
    "    \"\"\"Generate integer scores (0100) for model responses using LLM evaluation.\"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for entry in tqdm(data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            \"Given the input below, the correct output, and the model's response, \"\n",
    "            \"score the model's response on a scale from 0 to 100, where 100 is the best.\\n\\n\"\n",
    "            f\"### Input:\\n{format_input(entry)}\\n\\n\"\n",
    "            f\"### Expected Output:\\n{entry['output']}\\n\\n\"\n",
    "            f\"### Model Response:\\n{entry.get(response_key, '').strip()}\\n\\n\"\n",
    "            \"### Respond with the integer number only.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score_str = query_model(prompt, model=model).strip()\n",
    "            score = int(score_str)\n",
    "            scores.append(score)\n",
    "        except ValueError:\n",
    "            print(f\"[Warning] Invalid score format: {score_str!r}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Scoring failed for entry: {e}\")\n",
    "\n",
    "    return scores\n"
   ],
   "id": "6ef00a7087af6e6c",
   "outputs": [],
   "execution_count": 238
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:56:31.209869Z",
     "start_time": "2025-06-15T09:55:35.260610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scores = generate_model_scores(test_data)\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ],
   "id": "a437d382509a0058",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  44%|     | 44/100 [00:22<00:36,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response a 0 out of 100. The expected output was 117, but the model responded with 5, which is incorrect.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  47%|     | 47/100 [00:25<01:04,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response a 0 out of 100.\\n\\nThe model's response is completely unrelated to the original input and instruction. The expected output was a rearranged list in descending order, but the model provided a new list that doesn't even match the original numbers or instruction. This indicates a complete misunderstanding of the task and lack of effort to provide a relevant response.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  50%|     | 50/100 [00:29<01:06,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I would score the model's response a 0 out of 100 because it does not attempt to complete the task as instructed. The expected output is the sum of the sides of the triangle, which is 18 cm, but the model simply repeats the input information without performing any calculation or providing the correct answer.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  57%|    | 57/100 [00:34<00:49,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: 'I\\'d score the model\\'s response a 0 out of 100.\\n\\nThe reason is that the model\\'s response is not a correct completion of the task. The instruction asks for a direct conversion from words to digits, but the model\\'s response is a Python function definition, which is not a numerical output. The expected output was \"48\", but the model\\'s response does not provide that.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  85%| | 85/100 [00:49<00:16,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Invalid score format: \"I'd score the model's response as follows:\\n\\n* The model partially completes the request by providing the correct calculation formula, but it doesn't provide the actual answer.\\n* The expected output is a simple numerical value (100 square meters), which the model fails to deliver.\\n\\nScore: 40/100\\n\\nThe model gets some credit for attempting to complete the request, but it falls short of providing the desired output.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|| 100/100 [00:55<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 95 of 100\n",
      "Average score: 26.81\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 239
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
