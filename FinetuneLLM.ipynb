{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-tune LLM to follow instructions\n",
   "id": "b6e02cf5201e2731"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load open weights",
   "id": "91e1b02b97bac08c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:25:45.454408Z",
     "start_time": "2025-06-15T08:25:44.675861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wget\n",
    "\n",
    "from gpt2_v2 import GPT2Model, GPT_CONFIG_124M, complete_text, generate_text_simple, tensor_to_text, text_to_tensor\n",
    "\n",
    "GPT_CONFIG_124M.update({\"qkv_bias\": True})\n",
    "model = GPT2Model(GPT_CONFIG_124M)\n"
   ],
   "id": "423a8cab44a48e29",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:11:56.833964Z",
     "start_time": "2025-06-15T02:11:56.819066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ],
   "id": "891e084dd7eeb90a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768])\n",
      "pos_emb.weight torch.Size([1024, 768])\n",
      "blocks.0.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_Q.bias torch.Size([768])\n",
      "blocks.0.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_K.bias torch.Size([768])\n",
      "blocks.0.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.0.attn.W_V.bias torch.Size([768])\n",
      "blocks.0.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.0.attn.out_proj.bias torch.Size([768])\n",
      "blocks.0.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.0.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.0.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.0.ff.layers.2.bias torch.Size([768])\n",
      "blocks.0.ln1.weight torch.Size([768])\n",
      "blocks.0.ln1.bias torch.Size([768])\n",
      "blocks.0.ln2.weight torch.Size([768])\n",
      "blocks.0.ln2.bias torch.Size([768])\n",
      "blocks.1.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_Q.bias torch.Size([768])\n",
      "blocks.1.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_K.bias torch.Size([768])\n",
      "blocks.1.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.1.attn.W_V.bias torch.Size([768])\n",
      "blocks.1.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.1.attn.out_proj.bias torch.Size([768])\n",
      "blocks.1.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.1.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.1.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.1.ff.layers.2.bias torch.Size([768])\n",
      "blocks.1.ln1.weight torch.Size([768])\n",
      "blocks.1.ln1.bias torch.Size([768])\n",
      "blocks.1.ln2.weight torch.Size([768])\n",
      "blocks.1.ln2.bias torch.Size([768])\n",
      "blocks.2.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_Q.bias torch.Size([768])\n",
      "blocks.2.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_K.bias torch.Size([768])\n",
      "blocks.2.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.2.attn.W_V.bias torch.Size([768])\n",
      "blocks.2.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.2.attn.out_proj.bias torch.Size([768])\n",
      "blocks.2.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.2.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.2.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.2.ff.layers.2.bias torch.Size([768])\n",
      "blocks.2.ln1.weight torch.Size([768])\n",
      "blocks.2.ln1.bias torch.Size([768])\n",
      "blocks.2.ln2.weight torch.Size([768])\n",
      "blocks.2.ln2.bias torch.Size([768])\n",
      "blocks.3.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_Q.bias torch.Size([768])\n",
      "blocks.3.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_K.bias torch.Size([768])\n",
      "blocks.3.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.3.attn.W_V.bias torch.Size([768])\n",
      "blocks.3.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.3.attn.out_proj.bias torch.Size([768])\n",
      "blocks.3.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.3.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.3.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.3.ff.layers.2.bias torch.Size([768])\n",
      "blocks.3.ln1.weight torch.Size([768])\n",
      "blocks.3.ln1.bias torch.Size([768])\n",
      "blocks.3.ln2.weight torch.Size([768])\n",
      "blocks.3.ln2.bias torch.Size([768])\n",
      "blocks.4.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_Q.bias torch.Size([768])\n",
      "blocks.4.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_K.bias torch.Size([768])\n",
      "blocks.4.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.4.attn.W_V.bias torch.Size([768])\n",
      "blocks.4.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.4.attn.out_proj.bias torch.Size([768])\n",
      "blocks.4.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.4.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.4.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.4.ff.layers.2.bias torch.Size([768])\n",
      "blocks.4.ln1.weight torch.Size([768])\n",
      "blocks.4.ln1.bias torch.Size([768])\n",
      "blocks.4.ln2.weight torch.Size([768])\n",
      "blocks.4.ln2.bias torch.Size([768])\n",
      "blocks.5.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_Q.bias torch.Size([768])\n",
      "blocks.5.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_K.bias torch.Size([768])\n",
      "blocks.5.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.5.attn.W_V.bias torch.Size([768])\n",
      "blocks.5.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.5.attn.out_proj.bias torch.Size([768])\n",
      "blocks.5.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.5.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.5.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.5.ff.layers.2.bias torch.Size([768])\n",
      "blocks.5.ln1.weight torch.Size([768])\n",
      "blocks.5.ln1.bias torch.Size([768])\n",
      "blocks.5.ln2.weight torch.Size([768])\n",
      "blocks.5.ln2.bias torch.Size([768])\n",
      "blocks.6.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_Q.bias torch.Size([768])\n",
      "blocks.6.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_K.bias torch.Size([768])\n",
      "blocks.6.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.6.attn.W_V.bias torch.Size([768])\n",
      "blocks.6.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.6.attn.out_proj.bias torch.Size([768])\n",
      "blocks.6.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.6.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.6.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.6.ff.layers.2.bias torch.Size([768])\n",
      "blocks.6.ln1.weight torch.Size([768])\n",
      "blocks.6.ln1.bias torch.Size([768])\n",
      "blocks.6.ln2.weight torch.Size([768])\n",
      "blocks.6.ln2.bias torch.Size([768])\n",
      "blocks.7.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_Q.bias torch.Size([768])\n",
      "blocks.7.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_K.bias torch.Size([768])\n",
      "blocks.7.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.7.attn.W_V.bias torch.Size([768])\n",
      "blocks.7.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.7.attn.out_proj.bias torch.Size([768])\n",
      "blocks.7.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.7.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.7.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.7.ff.layers.2.bias torch.Size([768])\n",
      "blocks.7.ln1.weight torch.Size([768])\n",
      "blocks.7.ln1.bias torch.Size([768])\n",
      "blocks.7.ln2.weight torch.Size([768])\n",
      "blocks.7.ln2.bias torch.Size([768])\n",
      "blocks.8.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_Q.bias torch.Size([768])\n",
      "blocks.8.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_K.bias torch.Size([768])\n",
      "blocks.8.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.8.attn.W_V.bias torch.Size([768])\n",
      "blocks.8.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.8.attn.out_proj.bias torch.Size([768])\n",
      "blocks.8.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.8.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.8.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.8.ff.layers.2.bias torch.Size([768])\n",
      "blocks.8.ln1.weight torch.Size([768])\n",
      "blocks.8.ln1.bias torch.Size([768])\n",
      "blocks.8.ln2.weight torch.Size([768])\n",
      "blocks.8.ln2.bias torch.Size([768])\n",
      "blocks.9.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_Q.bias torch.Size([768])\n",
      "blocks.9.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_K.bias torch.Size([768])\n",
      "blocks.9.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.9.attn.W_V.bias torch.Size([768])\n",
      "blocks.9.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.9.attn.out_proj.bias torch.Size([768])\n",
      "blocks.9.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.9.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.9.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.9.ff.layers.2.bias torch.Size([768])\n",
      "blocks.9.ln1.weight torch.Size([768])\n",
      "blocks.9.ln1.bias torch.Size([768])\n",
      "blocks.9.ln2.weight torch.Size([768])\n",
      "blocks.9.ln2.bias torch.Size([768])\n",
      "blocks.10.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_Q.bias torch.Size([768])\n",
      "blocks.10.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_K.bias torch.Size([768])\n",
      "blocks.10.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.10.attn.W_V.bias torch.Size([768])\n",
      "blocks.10.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.10.attn.out_proj.bias torch.Size([768])\n",
      "blocks.10.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.10.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.10.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.10.ff.layers.2.bias torch.Size([768])\n",
      "blocks.10.ln1.weight torch.Size([768])\n",
      "blocks.10.ln1.bias torch.Size([768])\n",
      "blocks.10.ln2.weight torch.Size([768])\n",
      "blocks.10.ln2.bias torch.Size([768])\n",
      "blocks.11.attn.W_Q.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_Q.bias torch.Size([768])\n",
      "blocks.11.attn.W_K.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_K.bias torch.Size([768])\n",
      "blocks.11.attn.W_V.weight torch.Size([768, 768])\n",
      "blocks.11.attn.W_V.bias torch.Size([768])\n",
      "blocks.11.attn.out_proj.weight torch.Size([768, 768])\n",
      "blocks.11.attn.out_proj.bias torch.Size([768])\n",
      "blocks.11.ff.layers.0.weight torch.Size([3072, 768])\n",
      "blocks.11.ff.layers.0.bias torch.Size([3072])\n",
      "blocks.11.ff.layers.2.weight torch.Size([768, 3072])\n",
      "blocks.11.ff.layers.2.bias torch.Size([768])\n",
      "blocks.11.ln1.weight torch.Size([768])\n",
      "blocks.11.ln1.bias torch.Size([768])\n",
      "blocks.11.ln2.weight torch.Size([768])\n",
      "blocks.11.ln2.bias torch.Size([768])\n",
      "final_norm.weight torch.Size([768])\n",
      "final_norm.bias torch.Size([768])\n",
      "out_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:25:50.948739Z",
     "start_time": "2025-06-15T08:25:50.277821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)"
   ],
   "id": "49648386cc30dc35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start ofucc Matth Names sankleg sprayimize inflicting ShallUTC ))) spill Main insanely mph\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download GPT2 from OpenAI",
   "id": "2ba0d473bfd76ff9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:15:17.049393Z",
     "start_time": "2025-06-15T02:15:17.036015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def download_file(url, destination, backup_url=None):\n",
    "    def _attempt_download(download_url):\n",
    "        with urllib.request.urlopen(download_url) as response:\n",
    "            total_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "            if os.path.exists(destination) and os.path.getsize(destination) == total_size:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return True\n",
    "\n",
    "            with tqdm(total=total_size, unit=\"iB\", unit_scale=True, desc=os.path.basename(download_url)) as pbar, \\\n",
    "                 open(destination, \"wb\") as f:\n",
    "                for chunk in iter(lambda: response.read(1024), b\"\"):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "            return True\n",
    "\n",
    "    try:\n",
    "        if _attempt_download(url):\n",
    "            return\n",
    "    except (urllib.error.HTTPError, urllib.error.URLError):\n",
    "        if backup_url:\n",
    "            print(f\"Primary URL failed. Trying backup URL: {backup_url}\")\n",
    "            try:\n",
    "                if _attempt_download(backup_url):\n",
    "                    return\n",
    "            except (urllib.error.HTTPError, urllib.error.URLError):\n",
    "                pass\n",
    "        print(f\"Failed to download from primary URL ({url})\"\n",
    "              + (f\" and backup URL ({backup_url})\" if backup_url else \"\") + \".\\n\"\n",
    "              \"Check your internet connection or the file availability.\\n\"\n",
    "              \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    allowed_sizes = {\"124M\", \"355M\", \"774M\", \"1558M\"}\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size must be one of {allowed_sizes}\")\n",
    "\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    base_url = f\"https://openaipublic.blob.core.windows.net/gpt-2/models/{model_size}/\"\n",
    "    backup_url = f\"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2/{model_size}/\"\n",
    "\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    for fname in filenames:\n",
    "        dst = os.path.join(model_dir, fname)\n",
    "        if os.path.exists(dst):\n",
    "            print(f\"Already exists: {fname}, skipping download.\")\n",
    "            continue\n",
    "        primary = urljoin(base_url, fname)\n",
    "        backup = urljoin(backup_url, fname)\n",
    "        print(f\"Downloading {fname} ...\")\n",
    "        download_file(primary, dst, backup)\n",
    "\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    with open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        settings = json.load(f)\n",
    "\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "    return settings, params\n"
   ],
   "id": "f29d5d3c3d0402a0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:25:59.855760Z",
     "start_time": "2025-06-15T08:25:59.332644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Params:\", params.keys())"
   ],
   "id": "28d0828f52dfce6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: checkpoint, skipping download.\n",
      "Already exists: encoder.json, skipping download.\n",
      "Already exists: hparams.json, skipping download.\n",
      "Already exists: model.ckpt.data-00000-of-00001, skipping download.\n",
      "Already exists: model.ckpt.index, skipping download.\n",
      "Already exists: model.ckpt.meta, skipping download.\n",
      "Already exists: vocab.bpe, skipping download.\n",
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Params: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:26:02.734869Z",
     "start_time": "2025-06-15T08:26:02.688666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def assign_(left, right):\n",
    "    if right is None:\n",
    "        raise ValueError(\"'right' cannot be None\")\n",
    "    right_tensor = torch.as_tensor(right, dtype=left.dtype, device=left.device)\n",
    "    if right_tensor.numel() == 0:\n",
    "        raise ValueError(\"'right' cannot be Empty\")\n",
    "    if left.shape != right_tensor.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {left.shape} vs {right_tensor.shape}\")\n",
    "    with torch.no_grad():\n",
    "        left.copy_(right_tensor)\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    assign_(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    assign_(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b, (block, pblock) in enumerate(zip(gpt.blocks, params[\"blocks\"])):\n",
    "        # Attention QKV\n",
    "        qw, kw, vw = np.split(pblock[\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
    "        qb, kb, vb = np.split(pblock[\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
    "        assign_(block.attn.W_Q.weight, qw.T)\n",
    "        assign_(block.attn.W_K.weight, kw.T)\n",
    "        assign_(block.attn.W_V.weight, vw.T)\n",
    "        assign_(block.attn.W_Q.bias, qb)\n",
    "        assign_(block.attn.W_K.bias, kb)\n",
    "        assign_(block.attn.W_V.bias, vb)\n",
    "\n",
    "        # Attention output projection\n",
    "        assign_(block.attn.out_proj.weight, pblock[\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        assign_(block.attn.out_proj.bias,   pblock[\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # Feedforward\n",
    "        assign_(block.ff.layers[0].weight, pblock[\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        assign_(block.ff.layers[0].bias,   pblock[\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        assign_(block.ff.layers[2].weight, pblock[\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        assign_(block.ff.layers[2].bias,   pblock[\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # LayerNorms\n",
    "        assign_(block.ln1.weight, pblock[\"ln_1\"][\"g\"])\n",
    "        assign_(block.ln1.bias, pblock[\"ln_1\"][\"b\"])\n",
    "        assign_(block.ln2.weight, pblock[\"ln_2\"][\"g\"])\n",
    "        assign_(block.ln2.bias, pblock[\"ln_2\"][\"b\"])\n",
    "\n",
    "    assign_(gpt.final_norm.weight, params[\"g\"])\n",
    "    assign_(gpt.final_norm.bias, params[\"b\"])\n",
    "    assign_(gpt.out_head.weight,  params[\"wte\"])\n"
   ],
   "id": "e83e9805c9622997",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:26:06.417612Z",
     "start_time": "2025-06-15T08:26:06.272488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_weights_into_gpt(model, params)\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n"
   ],
   "id": "7696090694c7c0c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T02:23:59.896307Z",
     "start_time": "2025-06-15T02:23:59.824877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.mean().item(), param.std().item())\n"
   ],
   "id": "6ae4be9bb4b68198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768]) 0.00037981756031513214 0.14369554817676544\n",
      "pos_emb.weight torch.Size([1024, 768]) -0.0006787165184505284 0.1226913258433342\n",
      "blocks.0.attn.W_Q.weight torch.Size([768, 768]) 0.00015374351642094553 0.2386905699968338\n",
      "blocks.0.attn.W_Q.bias torch.Size([768]) -0.007821076549589634 0.3427544832229614\n",
      "blocks.0.attn.W_K.weight torch.Size([768, 768]) 1.2351122677500825e-05 0.2432965785264969\n",
      "blocks.0.attn.W_K.bias torch.Size([768]) 0.0048723239451646805 0.18297071754932404\n",
      "blocks.0.attn.W_V.weight torch.Size([768, 768]) -5.968316145299468e-06 0.05811797454953194\n",
      "blocks.0.attn.W_V.bias torch.Size([768]) 0.0008267878438346088 0.04772818833589554\n",
      "blocks.0.attn.out_proj.weight torch.Size([768, 768]) -0.0001613790518604219 0.1474614143371582\n",
      "blocks.0.attn.out_proj.bias torch.Size([768]) -0.00691022165119648 0.2589662969112396\n",
      "blocks.0.ff.layers.0.weight torch.Size([3072, 768]) -0.0007485305541194975 0.14116929471492767\n",
      "blocks.0.ff.layers.0.bias torch.Size([3072]) -0.0931621864438057 0.13235801458358765\n",
      "blocks.0.ff.layers.2.weight torch.Size([768, 3072]) 8.009047633095179e-06 0.0879654809832573\n",
      "blocks.0.ff.layers.2.bias torch.Size([768]) -0.0004230523481965065 0.10169976204633713\n",
      "blocks.0.ln1.weight torch.Size([768]) 0.18035894632339478 0.04131494462490082\n",
      "blocks.0.ln1.bias torch.Size([768]) -0.006593453232198954 0.03580174222588539\n",
      "blocks.0.ln2.weight torch.Size([768]) 0.8678296208381653 0.48494789004325867\n",
      "blocks.0.ln2.bias torch.Size([768]) 0.00920353177934885 0.07009701430797577\n",
      "blocks.1.attn.W_Q.weight torch.Size([768, 768]) -0.00015265395632013679 0.15103811025619507\n",
      "blocks.1.attn.W_Q.bias torch.Size([768]) 0.001711765886284411 0.3485022187232971\n",
      "blocks.1.attn.W_K.weight torch.Size([768, 768]) 0.00023460258671548218 0.15902258455753326\n",
      "blocks.1.attn.W_K.bias torch.Size([768]) 0.0008940294501371682 0.08274678885936737\n",
      "blocks.1.attn.W_V.weight torch.Size([768, 768]) 2.3152324502007104e-06 0.10367371886968613\n",
      "blocks.1.attn.W_V.bias torch.Size([768]) -0.00020504526037257165 0.07381219416856766\n",
      "blocks.1.attn.out_proj.weight torch.Size([768, 768]) -8.275873551610857e-05 0.10191785544157028\n",
      "blocks.1.attn.out_proj.bias torch.Size([768]) -0.0010727141052484512 0.104808010160923\n",
      "blocks.1.ff.layers.0.weight torch.Size([3072, 768]) 0.0006420306744985282 0.1307205706834793\n",
      "blocks.1.ff.layers.0.bias torch.Size([3072]) -0.07219841331243515 0.09491100162267685\n",
      "blocks.1.ff.layers.2.weight torch.Size([768, 3072]) 9.794025390874594e-05 0.08719106763601303\n",
      "blocks.1.ff.layers.2.bias torch.Size([768]) 0.0002505724842194468 0.10042908787727356\n",
      "blocks.1.ln1.weight torch.Size([768]) 0.22284089028835297 0.05130758509039879\n",
      "blocks.1.ln1.bias torch.Size([768]) -0.0050234864465892315 0.052436452358961105\n",
      "blocks.1.ln2.weight torch.Size([768]) 0.24269406497478485 0.03164536505937576\n",
      "blocks.1.ln2.bias torch.Size([768]) -0.004073990508913994 0.03919508308172226\n",
      "blocks.2.attn.W_Q.weight torch.Size([768, 768]) 0.0001179432583739981 0.18932299315929413\n",
      "blocks.2.attn.W_Q.bias torch.Size([768]) -0.006064689252525568 0.2589069902896881\n",
      "blocks.2.attn.W_K.weight torch.Size([768, 768]) -4.986231942893937e-05 0.15181933343410492\n",
      "blocks.2.attn.W_K.bias torch.Size([768]) -0.003441582666710019 0.10127473622560501\n",
      "blocks.2.attn.W_V.weight torch.Size([768, 768]) 0.00017838712665252388 0.10502831637859344\n",
      "blocks.2.attn.W_V.bias torch.Size([768]) -0.0021237407345324755 0.0655340775847435\n",
      "blocks.2.attn.out_proj.weight torch.Size([768, 768]) -3.299467061879113e-05 0.08103547245264053\n",
      "blocks.2.attn.out_proj.bias torch.Size([768]) 0.003375591477379203 0.1451117843389511\n",
      "blocks.2.ff.layers.0.weight torch.Size([3072, 768]) -0.005061259493231773 0.13352689146995544\n",
      "blocks.2.ff.layers.0.bias torch.Size([3072]) -0.0928223729133606 0.1066708043217659\n",
      "blocks.2.ff.layers.2.weight torch.Size([768, 3072]) 0.00019714866357389838 0.09308750927448273\n",
      "blocks.2.ff.layers.2.bias torch.Size([768]) 0.002819357207044959 0.11242914199829102\n",
      "blocks.2.ln1.weight torch.Size([768]) 0.24077002704143524 0.07527267932891846\n",
      "blocks.2.ln1.bias torch.Size([768]) -0.00036007841117680073 0.07055915892124176\n",
      "blocks.2.ln2.weight torch.Size([768]) 0.2925874888896942 0.04540559649467468\n",
      "blocks.2.ln2.bias torch.Size([768]) 0.006347864866256714 0.043932873755693436\n",
      "blocks.3.attn.W_Q.weight torch.Size([768, 768]) -0.00021896824182476848 0.16474692523479462\n",
      "blocks.3.attn.W_Q.bias torch.Size([768]) -0.0038416890893131495 0.2126476913690567\n",
      "blocks.3.attn.W_K.weight torch.Size([768, 768]) -5.696068546967581e-05 0.1537768691778183\n",
      "blocks.3.attn.W_K.bias torch.Size([768]) -0.0004758095892611891 0.10474759340286255\n",
      "blocks.3.attn.W_V.weight torch.Size([768, 768]) 0.00020247689099051058 0.09766855090856552\n",
      "blocks.3.attn.W_V.bias torch.Size([768]) 0.0016517918556928635 0.0643523558974266\n",
      "blocks.3.attn.out_proj.weight torch.Size([768, 768]) 3.374144580448046e-05 0.0841251015663147\n",
      "blocks.3.attn.out_proj.bias torch.Size([768]) -0.0015612887218594551 0.10791037976741791\n",
      "blocks.3.ff.layers.0.weight torch.Size([3072, 768]) -0.00595002481713891 0.12953029572963715\n",
      "blocks.3.ff.layers.0.bias torch.Size([3072]) -0.09253177046775818 0.08565898984670639\n",
      "blocks.3.ff.layers.2.weight torch.Size([768, 3072]) 0.00017645648040343076 0.09180603921413422\n",
      "blocks.3.ff.layers.2.bias torch.Size([768]) 0.0021020257845520973 0.11511888355016708\n",
      "blocks.3.ln1.weight torch.Size([768]) 0.3010978400707245 0.05351252108812332\n",
      "blocks.3.ln1.bias torch.Size([768]) 0.0054475064389407635 0.07015811651945114\n",
      "blocks.3.ln2.weight torch.Size([768]) 0.30650773644447327 0.05265355482697487\n",
      "blocks.3.ln2.bias torch.Size([768]) 0.00996524840593338 0.04357629269361496\n",
      "blocks.4.attn.W_Q.weight torch.Size([768, 768]) 0.0002935132652055472 0.1704353392124176\n",
      "blocks.4.attn.W_Q.bias torch.Size([768]) 0.012315389700233936 0.20646966993808746\n",
      "blocks.4.attn.W_K.weight torch.Size([768, 768]) 5.515472003025934e-05 0.15752458572387695\n",
      "blocks.4.attn.W_K.bias torch.Size([768]) 0.004667005501687527 0.3554832935333252\n",
      "blocks.4.attn.W_V.weight torch.Size([768, 768]) 0.00010751151421573013 0.10232267528772354\n",
      "blocks.4.attn.W_V.bias torch.Size([768]) -0.0013783341273665428 0.05650889128446579\n",
      "blocks.4.attn.out_proj.weight torch.Size([768, 768]) -1.1396015906939283e-05 0.09297914057970047\n",
      "blocks.4.attn.out_proj.bias torch.Size([768]) -0.0009013282251544297 0.10049892961978912\n",
      "blocks.4.ff.layers.0.weight torch.Size([3072, 768]) -0.0032636644318699837 0.12971320748329163\n",
      "blocks.4.ff.layers.0.bias torch.Size([3072]) -0.08612614870071411 0.09320200234651566\n",
      "blocks.4.ff.layers.2.weight torch.Size([768, 3072]) 0.0001799796591512859 0.09099913388490677\n",
      "blocks.4.ff.layers.2.bias torch.Size([768]) 0.0016059394693002105 0.13685975968837738\n",
      "blocks.4.ln1.weight torch.Size([768]) 0.31934547424316406 0.04739116132259369\n",
      "blocks.4.ln1.bias torch.Size([768]) 0.007916287519037724 0.06749274581670761\n",
      "blocks.4.ln2.weight torch.Size([768]) 0.2725818455219269 0.043896984308958054\n",
      "blocks.4.ln2.bias torch.Size([768]) 0.0009596580639481544 0.02688399702310562\n",
      "blocks.5.attn.W_Q.weight torch.Size([768, 768]) -0.00026773728313855827 0.14125582575798035\n",
      "blocks.5.attn.W_Q.bias torch.Size([768]) -0.0016259821131825447 0.12543262541294098\n",
      "blocks.5.attn.W_K.weight torch.Size([768, 768]) 4.6552144340239465e-05 0.13614720106124878\n",
      "blocks.5.attn.W_K.bias torch.Size([768]) 0.006209613289684057 0.10714870691299438\n",
      "blocks.5.attn.W_V.weight torch.Size([768, 768]) -0.00010422924242448062 0.10330777615308762\n",
      "blocks.5.attn.W_V.bias torch.Size([768]) -0.001447124988771975 0.04811704158782959\n",
      "blocks.5.attn.out_proj.weight torch.Size([768, 768]) 1.0988791473209858e-05 0.09377486258745193\n",
      "blocks.5.attn.out_proj.bias torch.Size([768]) -0.0011684768833220005 0.11080432683229446\n",
      "blocks.5.ff.layers.0.weight torch.Size([3072, 768]) -0.004193877335637808 0.1267070472240448\n",
      "blocks.5.ff.layers.0.bias torch.Size([3072]) -0.08502542972564697 0.08900804817676544\n",
      "blocks.5.ff.layers.2.weight torch.Size([768, 3072]) 0.00011598864512052387 0.09735694527626038\n",
      "blocks.5.ff.layers.2.bias torch.Size([768]) 0.0009532846161164343 0.1064532995223999\n",
      "blocks.5.ln1.weight torch.Size([768]) 0.3731194734573364 0.04502682015299797\n",
      "blocks.5.ln1.bias torch.Size([768]) 0.011876348406076431 0.04906607046723366\n",
      "blocks.5.ln2.weight torch.Size([768]) 0.27900201082229614 0.05147692933678627\n",
      "blocks.5.ln2.bias torch.Size([768]) 0.00815197080373764 0.03279997780919075\n",
      "blocks.6.attn.W_Q.weight torch.Size([768, 768]) 0.00019029114628210664 0.13397741317749023\n",
      "blocks.6.attn.W_Q.bias torch.Size([768]) 0.004683490376919508 0.18410006165504456\n",
      "blocks.6.attn.W_K.weight torch.Size([768, 768]) -7.406622171401978e-05 0.12758222222328186\n",
      "blocks.6.attn.W_K.bias torch.Size([768]) 0.0015144060598686337 0.1012103334069252\n",
      "blocks.6.attn.W_V.weight torch.Size([768, 768]) 0.00021866592578589916 0.11854671686887741\n",
      "blocks.6.attn.W_V.bias torch.Size([768]) -0.0007340701413340867 0.035576995462179184\n",
      "blocks.6.attn.out_proj.weight torch.Size([768, 768]) 3.6974248359911144e-05 0.11368992179632187\n",
      "blocks.6.attn.out_proj.bias torch.Size([768]) -0.0004363872576504946 0.10605379194021225\n",
      "blocks.6.ff.layers.0.weight torch.Size([3072, 768]) -0.0028191537130624056 0.12635649740695953\n",
      "blocks.6.ff.layers.0.bias torch.Size([3072]) -0.08570227026939392 0.09056127071380615\n",
      "blocks.6.ff.layers.2.weight torch.Size([768, 3072]) 9.54796705627814e-05 0.10733181238174438\n",
      "blocks.6.ff.layers.2.bias torch.Size([768]) 0.0015628753462806344 0.12105625122785568\n",
      "blocks.6.ln1.weight torch.Size([768]) 0.3455983102321625 0.0441710501909256\n",
      "blocks.6.ln1.bias torch.Size([768]) 0.011822459287941456 0.0662064254283905\n",
      "blocks.6.ln2.weight torch.Size([768]) 0.2594689726829529 0.047400206327438354\n",
      "blocks.6.ln2.bias torch.Size([768]) 0.004331209696829319 0.03382179141044617\n",
      "blocks.7.attn.W_Q.weight torch.Size([768, 768]) -0.00045816207421012223 0.1364603042602539\n",
      "blocks.7.attn.W_Q.bias torch.Size([768]) -0.008822117000818253 0.2147248536348343\n",
      "blocks.7.attn.W_K.weight torch.Size([768, 768]) 0.00013387270155362785 0.13045501708984375\n",
      "blocks.7.attn.W_K.bias torch.Size([768]) -0.0036485891323536634 0.09873808920383453\n",
      "blocks.7.attn.W_V.weight torch.Size([768, 768]) 5.1211449317634106e-05 0.1194656491279602\n",
      "blocks.7.attn.W_V.bias torch.Size([768]) -0.00037803445593453944 0.036869797855615616\n",
      "blocks.7.attn.out_proj.weight torch.Size([768, 768]) 2.3895683625596575e-05 0.11391738802194595\n",
      "blocks.7.attn.out_proj.bias torch.Size([768]) -0.00013363065954763442 0.14512065052986145\n",
      "blocks.7.ff.layers.0.weight torch.Size([3072, 768]) -0.00352298840880394 0.12642338871955872\n",
      "blocks.7.ff.layers.0.bias torch.Size([3072]) -0.08847203105688095 0.09064304083585739\n",
      "blocks.7.ff.layers.2.weight torch.Size([768, 3072]) 8.648945367895067e-05 0.1187349334359169\n",
      "blocks.7.ff.layers.2.bias torch.Size([768]) 0.0011925119906663895 0.12881210446357727\n",
      "blocks.7.ln1.weight torch.Size([768]) 0.3565715253353119 0.04378596320748329\n",
      "blocks.7.ln1.bias torch.Size([768]) 0.01434354204684496 0.05914687365293503\n",
      "blocks.7.ln2.weight torch.Size([768]) 0.2560140788555145 0.04705559089779854\n",
      "blocks.7.ln2.bias torch.Size([768]) 0.009183691814541817 0.04571011662483215\n",
      "blocks.8.attn.W_Q.weight torch.Size([768, 768]) -0.0003004284226335585 0.13009199500083923\n",
      "blocks.8.attn.W_Q.bias torch.Size([768]) -0.013181586749851704 0.20185819268226624\n",
      "blocks.8.attn.W_K.weight torch.Size([768, 768]) 0.00017389189451932907 0.12435027956962585\n",
      "blocks.8.attn.W_K.bias torch.Size([768]) -0.0026047879364341497 0.09828340262174606\n",
      "blocks.8.attn.W_V.weight torch.Size([768, 768]) -0.00038428150583058596 0.12628209590911865\n",
      "blocks.8.attn.W_V.bias torch.Size([768]) -0.0008028498850762844 0.036681145429611206\n",
      "blocks.8.attn.out_proj.weight torch.Size([768, 768]) 8.123623047140427e-06 0.12236364185810089\n",
      "blocks.8.attn.out_proj.bias torch.Size([768]) 0.0010857944143936038 0.14007924497127533\n",
      "blocks.8.ff.layers.0.weight torch.Size([3072, 768]) -0.0020665344782173634 0.12728072702884674\n",
      "blocks.8.ff.layers.0.bias torch.Size([3072]) -0.08505804091691971 0.0935441330075264\n",
      "blocks.8.ff.layers.2.weight torch.Size([768, 3072]) 4.6921471948735416e-05 0.13540396094322205\n",
      "blocks.8.ff.layers.2.bias torch.Size([768]) 0.0011560862185433507 0.12735813856124878\n",
      "blocks.8.ln1.weight torch.Size([768]) 0.3352259397506714 0.044541217386722565\n",
      "blocks.8.ln1.bias torch.Size([768]) 0.013251811265945435 0.06870879977941513\n",
      "blocks.8.ln2.weight torch.Size([768]) 0.2566564977169037 0.04127686098217964\n",
      "blocks.8.ln2.bias torch.Size([768]) 0.000384395505534485 0.049368783831596375\n",
      "blocks.9.attn.W_Q.weight torch.Size([768, 768]) 0.00044967501889914274 0.12296558916568756\n",
      "blocks.9.attn.W_Q.bias torch.Size([768]) 0.008131361566483974 0.22075878083705902\n",
      "blocks.9.attn.W_K.weight torch.Size([768, 768]) -0.0003476667625363916 0.11892230808734894\n",
      "blocks.9.attn.W_K.bias torch.Size([768]) -0.007281634956598282 0.09623975306749344\n",
      "blocks.9.attn.W_V.weight torch.Size([768, 768]) -0.00031157408375293016 0.13612067699432373\n",
      "blocks.9.attn.W_V.bias torch.Size([768]) 6.7658256739377975e-06 0.03432562202215195\n",
      "blocks.9.attn.out_proj.weight torch.Size([768, 768]) -2.7884121664101258e-05 0.13681966066360474\n",
      "blocks.9.attn.out_proj.bias torch.Size([768]) 0.0021340707316994667 0.20949006080627441\n",
      "blocks.9.ff.layers.0.weight torch.Size([3072, 768]) -0.0027408814057707787 0.12761937081813812\n",
      "blocks.9.ff.layers.0.bias torch.Size([3072]) -0.08366744965314865 0.09236326813697815\n",
      "blocks.9.ff.layers.2.weight torch.Size([768, 3072]) 3.4792548831319436e-05 0.1558738499879837\n",
      "blocks.9.ff.layers.2.bias torch.Size([768]) 0.0007137329666875303 0.15918298065662384\n",
      "blocks.9.ln1.weight torch.Size([768]) 0.3575561046600342 0.04693679139018059\n",
      "blocks.9.ln1.bias torch.Size([768]) 0.01590331830084324 0.06386822462081909\n",
      "blocks.9.ln2.weight torch.Size([768]) 0.26497504115104675 0.041519638150930405\n",
      "blocks.9.ln2.bias torch.Size([768]) 0.006400738377124071 0.04571041837334633\n",
      "blocks.10.attn.W_Q.weight torch.Size([768, 768]) 0.00031771004432812333 0.11865982413291931\n",
      "blocks.10.attn.W_Q.bias torch.Size([768]) 0.008911840617656708 0.2265850007534027\n",
      "blocks.10.attn.W_K.weight torch.Size([768, 768]) -0.00012976815924048424 0.11467549949884415\n",
      "blocks.10.attn.W_K.bias torch.Size([768]) -0.002668120665475726 0.09832144528627396\n",
      "blocks.10.attn.W_V.weight torch.Size([768, 768]) 8.956639067037031e-05 0.14459584653377533\n",
      "blocks.10.attn.W_V.bias torch.Size([768]) -0.0014112890930846334 0.04762532189488411\n",
      "blocks.10.attn.out_proj.weight torch.Size([768, 768]) -8.798572821433481e-07 0.14662745594978333\n",
      "blocks.10.attn.out_proj.bias torch.Size([768]) 0.0020241064485162497 0.2322089970111847\n",
      "blocks.10.ff.layers.0.weight torch.Size([3072, 768]) -0.0032080465462058783 0.12764813005924225\n",
      "blocks.10.ff.layers.0.bias torch.Size([3072]) -0.07652498036623001 0.09122857451438904\n",
      "blocks.10.ff.layers.2.weight torch.Size([768, 3072]) 6.105272404965945e-06 0.17814528942108154\n",
      "blocks.10.ff.layers.2.bias torch.Size([768]) 0.0016591990133747458 0.19404050707817078\n",
      "blocks.10.ln1.weight torch.Size([768]) 0.37820783257484436 0.055699631571769714\n",
      "blocks.10.ln1.bias torch.Size([768]) 0.018612800166010857 0.05506131052970886\n",
      "blocks.10.ln2.weight torch.Size([768]) 0.2896941602230072 0.05117756500840187\n",
      "blocks.10.ln2.bias torch.Size([768]) 0.021159043535590172 0.04487457126379013\n",
      "blocks.11.attn.W_Q.weight torch.Size([768, 768]) -1.1366326361894608e-05 0.10982047021389008\n",
      "blocks.11.attn.W_Q.bias torch.Size([768]) 0.0005513962241820991 0.18319359421730042\n",
      "blocks.11.attn.W_K.weight torch.Size([768, 768]) -4.2440758988959715e-05 0.10549236834049225\n",
      "blocks.11.attn.W_K.bias torch.Size([768]) 0.0015315081691369414 0.08636081963777542\n",
      "blocks.11.attn.W_V.weight torch.Size([768, 768]) 0.0002158462448278442 0.16225944459438324\n",
      "blocks.11.attn.W_V.bias torch.Size([768]) 0.00011266752699157223 0.05450312793254852\n",
      "blocks.11.attn.out_proj.weight torch.Size([768, 768]) -5.3778097935719416e-05 0.1819266527891159\n",
      "blocks.11.attn.out_proj.bias torch.Size([768]) -0.021505361422896385 0.46894726157188416\n",
      "blocks.11.ff.layers.0.weight torch.Size([3072, 768]) -0.001846416387706995 0.13000451028347015\n",
      "blocks.11.ff.layers.0.bias torch.Size([3072]) -0.06411425024271011 0.0930408164858818\n",
      "blocks.11.ff.layers.2.weight torch.Size([768, 3072]) -0.00043532054405659437 0.19821906089782715\n",
      "blocks.11.ff.layers.2.bias torch.Size([768]) 0.000971626432146877 0.10824692994356155\n",
      "blocks.11.ln1.weight torch.Size([768]) 0.4786931276321411 0.06516604125499725\n",
      "blocks.11.ln1.bias torch.Size([768]) 0.023284194990992546 0.05674212425947189\n",
      "blocks.11.ln2.weight torch.Size([768]) 0.5041061043739319 0.09001091122627258\n",
      "blocks.11.ln2.bias torch.Size([768]) 0.009192791767418385 0.03916310518980026\n",
      "final_norm.weight torch.Size([768]) 1.5078086853027344 1.3910776376724243\n",
      "final_norm.bias torch.Size([768]) -0.003138466738164425 0.4196469485759735\n",
      "out_head.weight torch.Size([50257, 768]) 0.00037981756031513214 0.14369554817676544\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T03:49:36.416965Z",
     "start_time": "2025-06-15T03:49:35.406847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import complete_text\n",
    "result = complete_text(\"at the start of\", model,15)\n",
    "print(\"Output text:\\n\", result)"
   ],
   "id": "a6c89718f5289428",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of the game, and then the game ends.\n",
      "\n",
      "The game is a\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:26:12.152899Z",
     "start_time": "2025-06-15T08:26:11.323346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(\"at the start of\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", tensor_to_text(token_ids, tokenizer))"
   ],
   "id": "84204d1eb2f82b26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " at the start of an international series of events. You don't have to worry about who has\n"
     ]
    }
   ],
   "execution_count": 177
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Instruction Finetuning",
   "id": "d988216ca3645214"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:16:10.339298Z",
     "start_time": "2025-06-15T07:16:06.573976Z"
    }
   },
   "cell_type": "code",
   "source": "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
   "id": "3a43e589337beead",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-15 15:16:07--  https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 30.100.0.25, 30.100.0.26, 30.100.0.23, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|30.100.0.25|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 22773992 (22M) [text/plain]\r\n",
      "Saving to: alpaca_data.json\r\n",
      "\r\n",
      "alpaca_data.json    100%[===================>]  21.72M  21.9MB/s    in 1.0s    \r\n",
      "\r\n",
      "2025-06-15 15:16:10 (21.9 MB/s) - alpaca_data.json saved [22773992/22773992]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:22:42.853311Z",
     "start_time": "2025-06-15T07:22:42.674599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"alpaca_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#just take first 1000\n",
    "data = data[:1000]\n"
   ],
   "id": "2b051862bc4b6205",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:19:31.544495Z",
     "start_time": "2025-06-15T07:19:31.456231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_input(entry):\n",
    "    instruction = entry.get(\"instruction\", \"\").strip()\n",
    "    input_section = entry.get(\"input\", \"\").strip()\n",
    "\n",
    "    parts = [\n",
    "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n",
    "        \"\\n\\n### Instruction:\\n\" + instruction,\n",
    "    ]\n",
    "\n",
    "    if input_section:\n",
    "        parts.append(\"\\n\\n### Input:\\n\" + input_section)\n",
    "\n",
    "    return \"\".join(parts)\n"
   ],
   "id": "d3baaa33104e8709",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:22:47.775567Z",
     "start_time": "2025-06-15T07:22:47.740263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ],
   "id": "8e2ea8dda1848a37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Edit the following sentence to make it more concise.\n",
      "\n",
      "### Input:\n",
      "He ran to the bus stop in order to catch the bus that was due to arrive in five minutes.\n",
      "\n",
      "### Response:\n",
      "He ran to the bus stop, due to arrive in five minutes.\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:22:51.062923Z",
     "start_time": "2025-06-15T07:22:51.025334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = len(data)\n",
    "train_data = data[:int(n * 0.85)]\n",
    "test_data = data[int(n * 0.85):int(n * 0.95)]\n",
    "val_data = data[int(n * 0.95):]"
   ],
   "id": "2866573989336046",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:23:05.114700Z",
     "start_time": "2025-06-15T07:23:05.081727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ],
   "id": "1790ba80fbfdd587",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 850\n",
      "Validation set length: 50\n",
      "Test set length: 100\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:43:43.013600Z",
     "start_time": "2025-06-15T07:43:42.895540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(train_dataset))\n",
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ],
   "id": "dd4c34cccb5194d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n",
      "Train loader:\n",
      "torch.Size([8, 110]) torch.Size([8, 110])\n",
      "torch.Size([8, 146]) torch.Size([8, 146])\n",
      "torch.Size([8, 141]) torch.Size([8, 141])\n",
      "torch.Size([8, 147]) torch.Size([8, 147])\n",
      "torch.Size([8, 107]) torch.Size([8, 107])\n",
      "torch.Size([8, 249]) torch.Size([8, 249])\n",
      "torch.Size([8, 231]) torch.Size([8, 231])\n",
      "torch.Size([8, 199]) torch.Size([8, 199])\n",
      "torch.Size([8, 185]) torch.Size([8, 185])\n",
      "torch.Size([8, 130]) torch.Size([8, 130])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 147]) torch.Size([8, 147])\n",
      "torch.Size([8, 259]) torch.Size([8, 259])\n",
      "torch.Size([8, 109]) torch.Size([8, 109])\n",
      "torch.Size([8, 201]) torch.Size([8, 201])\n",
      "torch.Size([8, 199]) torch.Size([8, 199])\n",
      "torch.Size([8, 201]) torch.Size([8, 201])\n",
      "torch.Size([8, 307]) torch.Size([8, 307])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 191]) torch.Size([8, 191])\n",
      "torch.Size([8, 155]) torch.Size([8, 155])\n",
      "torch.Size([8, 292]) torch.Size([8, 292])\n",
      "torch.Size([8, 111]) torch.Size([8, 111])\n",
      "torch.Size([8, 122]) torch.Size([8, 122])\n",
      "torch.Size([8, 273]) torch.Size([8, 273])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 208]) torch.Size([8, 208])\n",
      "torch.Size([8, 160]) torch.Size([8, 160])\n",
      "torch.Size([8, 326]) torch.Size([8, 326])\n",
      "torch.Size([8, 135]) torch.Size([8, 135])\n",
      "torch.Size([8, 220]) torch.Size([8, 220])\n",
      "torch.Size([8, 200]) torch.Size([8, 200])\n",
      "torch.Size([8, 182]) torch.Size([8, 182])\n",
      "torch.Size([8, 172]) torch.Size([8, 172])\n",
      "torch.Size([8, 131]) torch.Size([8, 131])\n",
      "torch.Size([8, 166]) torch.Size([8, 166])\n",
      "torch.Size([8, 175]) torch.Size([8, 175])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 280]) torch.Size([8, 280])\n",
      "torch.Size([8, 145]) torch.Size([8, 145])\n",
      "torch.Size([8, 113]) torch.Size([8, 113])\n",
      "torch.Size([8, 223]) torch.Size([8, 223])\n",
      "torch.Size([8, 229]) torch.Size([8, 229])\n",
      "torch.Size([8, 310]) torch.Size([8, 310])\n",
      "torch.Size([8, 164]) torch.Size([8, 164])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 203]) torch.Size([8, 203])\n",
      "torch.Size([8, 209]) torch.Size([8, 209])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 184]) torch.Size([8, 184])\n",
      "torch.Size([8, 133]) torch.Size([8, 133])\n",
      "torch.Size([8, 197]) torch.Size([8, 197])\n",
      "torch.Size([8, 213]) torch.Size([8, 213])\n",
      "torch.Size([8, 192]) torch.Size([8, 192])\n",
      "torch.Size([8, 118]) torch.Size([8, 118])\n",
      "torch.Size([8, 177]) torch.Size([8, 177])\n",
      "torch.Size([8, 281]) torch.Size([8, 281])\n",
      "torch.Size([8, 106]) torch.Size([8, 106])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 427]) torch.Size([8, 427])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 115]) torch.Size([8, 115])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 114]) torch.Size([8, 114])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 126]) torch.Size([8, 126])\n",
      "torch.Size([8, 225]) torch.Size([8, 225])\n",
      "torch.Size([8, 164]) torch.Size([8, 164])\n",
      "torch.Size([8, 138]) torch.Size([8, 138])\n",
      "torch.Size([8, 257]) torch.Size([8, 257])\n",
      "torch.Size([8, 342]) torch.Size([8, 342])\n",
      "torch.Size([8, 137]) torch.Size([8, 137])\n",
      "torch.Size([8, 185]) torch.Size([8, 185])\n",
      "torch.Size([8, 265]) torch.Size([8, 265])\n",
      "torch.Size([8, 279]) torch.Size([8, 279])\n",
      "torch.Size([8, 114]) torch.Size([8, 114])\n",
      "torch.Size([8, 168]) torch.Size([8, 168])\n",
      "torch.Size([8, 274]) torch.Size([8, 274])\n",
      "torch.Size([8, 127]) torch.Size([8, 127])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 160]) torch.Size([8, 160])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 132]) torch.Size([8, 132])\n",
      "torch.Size([8, 168]) torch.Size([8, 168])\n",
      "torch.Size([8, 198]) torch.Size([8, 198])\n",
      "torch.Size([8, 233]) torch.Size([8, 233])\n",
      "torch.Size([8, 178]) torch.Size([8, 178])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 182]) torch.Size([8, 182])\n",
      "torch.Size([8, 230]) torch.Size([8, 230])\n",
      "torch.Size([8, 169]) torch.Size([8, 169])\n",
      "torch.Size([8, 426]) torch.Size([8, 426])\n",
      "torch.Size([8, 179]) torch.Size([8, 179])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 190]) torch.Size([8, 190])\n",
      "torch.Size([8, 269]) torch.Size([8, 269])\n",
      "torch.Size([8, 271]) torch.Size([8, 271])\n",
      "torch.Size([8, 206]) torch.Size([8, 206])\n",
      "torch.Size([8, 124]) torch.Size([8, 124])\n",
      "torch.Size([8, 344]) torch.Size([8, 344])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 231]) torch.Size([8, 231])\n",
      "torch.Size([8, 119]) torch.Size([8, 119])\n",
      "torch.Size([8, 126]) torch.Size([8, 126])\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:59:50.829037Z",
     "start_time": "2025-06-15T07:59:50.730241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from functools import partial\n",
    "\n",
    "device = \"cpu\"  # or \"cuda\" if available\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(\n",
    "                format_input(entry) + f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            )\n",
    "            for entry in data\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_texts[idx]\n",
    "\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    max_len = min(\n",
    "        max(len(seq) + 1 for seq in batch),\n",
    "        allowed_max_length or float('inf')\n",
    "    )\n",
    "\n",
    "    input_tensors, label_tensors = [], []\n",
    "\n",
    "    for seq in batch:\n",
    "        seq = seq + [pad_token_id]\n",
    "        padded = seq + [pad_token_id] * (max_len - len(seq))\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1], dtype=torch.long)\n",
    "        labels = torch.tensor(padded[1:], dtype=torch.long)\n",
    "\n",
    "        # Mask padding in labels except the first one\n",
    "        pad_mask = (labels == pad_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(pad_mask) > 1:\n",
    "            labels[pad_mask[1:]] = ignore_index\n",
    "\n",
    "        input_tensors.append(inputs)\n",
    "        label_tensors.append(labels)\n",
    "\n",
    "    return (\n",
    "        torch.stack(input_tensors).to(device),\n",
    "        torch.stack(label_tensors).to(device)\n",
    "    )\n",
    "\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")\n"
   ],
   "id": "7370d205bf9e7f88",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:09:55.760562Z",
     "start_time": "2025-06-15T08:09:55.695824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=True,drop_last=True,num_workers=0)\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=False,drop_last=False,num_workers=0)\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=customized_collate_fn, shuffle=False,drop_last=False,num_workers=0)"
   ],
   "id": "75e168056c043dbb",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:11:22.162138Z",
     "start_time": "2025-06-15T08:11:22.102899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)\n",
    "print(\"inputs: \",inputs[0])\n",
    "print(\"targets: \",targets[0])"
   ],
   "id": "fe4bac319d2bb9a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader:\n",
      "torch.Size([8, 225]) torch.Size([8, 225])\n",
      "torch.Size([8, 190]) torch.Size([8, 190])\n",
      "torch.Size([8, 280]) torch.Size([8, 280])\n",
      "torch.Size([8, 130]) torch.Size([8, 130])\n",
      "torch.Size([8, 209]) torch.Size([8, 209])\n",
      "torch.Size([8, 240]) torch.Size([8, 240])\n",
      "torch.Size([8, 201]) torch.Size([8, 201])\n",
      "torch.Size([8, 162]) torch.Size([8, 162])\n",
      "torch.Size([8, 152]) torch.Size([8, 152])\n",
      "torch.Size([8, 115]) torch.Size([8, 115])\n",
      "torch.Size([8, 426]) torch.Size([8, 426])\n",
      "torch.Size([8, 178]) torch.Size([8, 178])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 265]) torch.Size([8, 265])\n",
      "torch.Size([8, 192]) torch.Size([8, 192])\n",
      "torch.Size([8, 129]) torch.Size([8, 129])\n",
      "torch.Size([8, 185]) torch.Size([8, 185])\n",
      "torch.Size([8, 191]) torch.Size([8, 191])\n",
      "torch.Size([8, 180]) torch.Size([8, 180])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 271]) torch.Size([8, 271])\n",
      "torch.Size([8, 279]) torch.Size([8, 279])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 208]) torch.Size([8, 208])\n",
      "torch.Size([8, 168]) torch.Size([8, 168])\n",
      "torch.Size([8, 204]) torch.Size([8, 204])\n",
      "torch.Size([8, 202]) torch.Size([8, 202])\n",
      "torch.Size([8, 344]) torch.Size([8, 344])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 123]) torch.Size([8, 123])\n",
      "torch.Size([8, 274]) torch.Size([8, 274])\n",
      "torch.Size([8, 273]) torch.Size([8, 273])\n",
      "torch.Size([8, 196]) torch.Size([8, 196])\n",
      "torch.Size([8, 210]) torch.Size([8, 210])\n",
      "torch.Size([8, 143]) torch.Size([8, 143])\n",
      "torch.Size([8, 167]) torch.Size([8, 167])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 191]) torch.Size([8, 191])\n",
      "torch.Size([8, 269]) torch.Size([8, 269])\n",
      "torch.Size([8, 160]) torch.Size([8, 160])\n",
      "torch.Size([8, 223]) torch.Size([8, 223])\n",
      "torch.Size([8, 194]) torch.Size([8, 194])\n",
      "torch.Size([8, 148]) torch.Size([8, 148])\n",
      "torch.Size([8, 166]) torch.Size([8, 166])\n",
      "torch.Size([8, 281]) torch.Size([8, 281])\n",
      "torch.Size([8, 182]) torch.Size([8, 182])\n",
      "torch.Size([8, 218]) torch.Size([8, 218])\n",
      "torch.Size([8, 229]) torch.Size([8, 229])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 138]) torch.Size([8, 138])\n",
      "torch.Size([8, 259]) torch.Size([8, 259])\n",
      "torch.Size([8, 157]) torch.Size([8, 157])\n",
      "torch.Size([8, 326]) torch.Size([8, 326])\n",
      "torch.Size([8, 138]) torch.Size([8, 138])\n",
      "torch.Size([8, 220]) torch.Size([8, 220])\n",
      "torch.Size([8, 203]) torch.Size([8, 203])\n",
      "torch.Size([8, 147]) torch.Size([8, 147])\n",
      "torch.Size([8, 151]) torch.Size([8, 151])\n",
      "torch.Size([8, 184]) torch.Size([8, 184])\n",
      "torch.Size([8, 117]) torch.Size([8, 117])\n",
      "torch.Size([8, 169]) torch.Size([8, 169])\n",
      "torch.Size([8, 206]) torch.Size([8, 206])\n",
      "torch.Size([8, 144]) torch.Size([8, 144])\n",
      "torch.Size([8, 176]) torch.Size([8, 176])\n",
      "torch.Size([8, 307]) torch.Size([8, 307])\n",
      "torch.Size([8, 129]) torch.Size([8, 129])\n",
      "torch.Size([8, 427]) torch.Size([8, 427])\n",
      "torch.Size([8, 158]) torch.Size([8, 158])\n",
      "torch.Size([8, 201]) torch.Size([8, 201])\n",
      "torch.Size([8, 233]) torch.Size([8, 233])\n",
      "torch.Size([8, 168]) torch.Size([8, 168])\n",
      "torch.Size([8, 310]) torch.Size([8, 310])\n",
      "torch.Size([8, 116]) torch.Size([8, 116])\n",
      "torch.Size([8, 198]) torch.Size([8, 198])\n",
      "torch.Size([8, 231]) torch.Size([8, 231])\n",
      "torch.Size([8, 342]) torch.Size([8, 342])\n",
      "torch.Size([8, 155]) torch.Size([8, 155])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 292]) torch.Size([8, 292])\n",
      "torch.Size([8, 115]) torch.Size([8, 115])\n",
      "torch.Size([8, 133]) torch.Size([8, 133])\n",
      "torch.Size([8, 257]) torch.Size([8, 257])\n",
      "torch.Size([8, 143]) torch.Size([8, 143])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 144]) torch.Size([8, 144])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 136]) torch.Size([8, 136])\n",
      "torch.Size([8, 172]) torch.Size([8, 172])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 171]) torch.Size([8, 171])\n",
      "torch.Size([8, 145]) torch.Size([8, 145])\n",
      "torch.Size([8, 134]) torch.Size([8, 134])\n",
      "torch.Size([8, 191]) torch.Size([8, 191])\n",
      "torch.Size([8, 136]) torch.Size([8, 136])\n",
      "torch.Size([8, 140]) torch.Size([8, 140])\n",
      "torch.Size([8, 173]) torch.Size([8, 173])\n",
      "torch.Size([8, 132]) torch.Size([8, 132])\n",
      "torch.Size([8, 163]) torch.Size([8, 163])\n",
      "torch.Size([8, 99]) torch.Size([8, 99])\n",
      "torch.Size([8, 153]) torch.Size([8, 153])\n",
      "torch.Size([8, 177]) torch.Size([8, 177])\n",
      "torch.Size([8, 161]) torch.Size([8, 161])\n",
      "torch.Size([8, 199]) torch.Size([8, 199])\n",
      "torch.Size([8, 231]) torch.Size([8, 231])\n",
      "torch.Size([8, 144]) torch.Size([8, 144])\n",
      "torch.Size([8, 133]) torch.Size([8, 133])\n",
      "inputs:  tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 16594,   257,  6770,   286,   366, 38611,\n",
      "        25444,  1911,   198,   198, 21017, 18261,    25,   198, 27248, 25444,\n",
      "          318,   257,  4590, 12857,  3788,  4166,   416, 21771,   326,   318,\n",
      "          973,   284,  2987,  4263,   393,  2251,  3048,   416, 29349,   290,\n",
      "        19771,  3354,   286,   262,  4683,  4875,  2939,    13, 29153, 13536,\n",
      "         2985,   284,  4532,   262, 23755,    11,  3124,    11, 12019,    11,\n",
      "          290, 11743,   286,   281,  2939,    11,   355,   880,   355,   284,\n",
      "         2251,  2420,    11,  2251,   513,    35,  5563,    11,   751,  4875,\n",
      "         3048,   290,  4174, 16628,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256])\n",
      "targets:  tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 16594,   257,  6770,   286,   366, 38611, 25444,\n",
      "         1911,   198,   198, 21017, 18261,    25,   198, 27248, 25444,   318,\n",
      "          257,  4590, 12857,  3788,  4166,   416, 21771,   326,   318,   973,\n",
      "          284,  2987,  4263,   393,  2251,  3048,   416, 29349,   290, 19771,\n",
      "         3354,   286,   262,  4683,  4875,  2939,    13, 29153, 13536,  2985,\n",
      "          284,  4532,   262, 23755,    11,  3124,    11, 12019,    11,   290,\n",
      "        11743,   286,   281,  2939,    11,   355,   880,   355,   284,  2251,\n",
      "         2420,    11,  2251,   513,    35,  5563,    11,   751,  4875,  3048,\n",
      "          290,  4174, 16628,    13, 50256,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100])\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:51:08.697599Z",
     "start_time": "2025-06-15T08:51:08.231287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=1024,\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = tensor_to_text(token_ids, tokenizer)\n",
    "print(generated_text)"
   ],
   "id": "3520f02e1e624290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Re-write the following sentence to use a different verb\n",
      "\n",
      "### Input:\n",
      "I will read the book tomorrow\n",
      "\n",
      "### Response:\n",
      "Tomorrow is tomorrow.\n"
     ]
    }
   ],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:18:31.115805Z",
     "start_time": "2025-06-15T08:18:26.036183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import loss_loader\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "id": "bda02c08bc5dcfa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.7368107795715333\n",
      "Validation loss: 3.2687312602996825\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train as normal",
   "id": "dccf9e269d59d3f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:48:07.638624Z",
     "start_time": "2025-06-15T08:35:22.169172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time\n",
    "from gpt2_v2 import train_model_simple, build_tokenizer\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_num_threads(12)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# FineTune the model\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=format_input(val_data[0]),\n",
    "    tokenizer=build_tokenizer()\n",
    ")\n",
    "\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f\"Training completed in {elapsed:.2f} minutes.\")\n"
   ],
   "id": "a297eb6b0a9efce3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000005): Train loss 1.679, Val loss 1.842, Tokens seen: 5208\n",
      "Ep 1 (Step 000010): Train loss 1.618, Val loss 1.833, Tokens seen: 13160\n",
      "Ep 1 (Step 000015): Train loss 1.583, Val loss 1.848, Tokens seen: 20328\n",
      "Ep 1 (Step 000020): Train loss 1.734, Val loss 1.842, Tokens seen: 28776\n",
      "Ep 1 (Step 000025): Train loss 1.608, Val loss 1.852, Tokens seen: 36400\n",
      "Ep 1 (Step 000030): Train loss 1.438, Val loss 1.823, Tokens seen: 44400\n",
      "Ep 1 (Step 000035): Train loss 1.629, Val loss 1.854, Tokens seen: 51640\n",
      "Ep 1 (Step 000040): Train loss 1.586, Val loss 1.860, Tokens seen: 59032\n",
      "Ep 1 (Step 000045): Train loss 1.584, Val loss 1.852, Tokens seen: 67344\n",
      "Ep 1 (Step 000050): Train loss 1.548, Val loss 1.856, Tokens seen: 74984\n",
      "Ep 1 (Step 000055): Train loss 1.712, Val loss 1.836, Tokens seen: 81808\n",
      "Ep 1 (Step 000060): Train loss 1.511, Val loss 1.845, Tokens seen: 91120\n",
      "Ep 1 (Step 000065): Train loss 1.451, Val loss 1.850, Tokens seen: 96880\n",
      "Ep 1 (Step 000070): Train loss 1.559, Val loss 1.831, Tokens seen: 104160\n",
      "Ep 1 (Step 000075): Train loss 1.524, Val loss 1.835, Tokens seen: 113824\n",
      "Ep 1 (Step 000080): Train loss 1.670, Val loss 1.821, Tokens seen: 120904\n",
      "Ep 1 (Step 000085): Train loss 1.456, Val loss 1.801, Tokens seen: 127224\n",
      "Ep 1 (Step 000090): Train loss 1.539, Val loss 1.800, Tokens seen: 134736\n",
      "Ep 1 (Step 000095): Train loss 1.494, Val loss 1.794, Tokens seen: 144072\n",
      "Ep 1 (Step 000100): Train loss 1.608, Val loss 1.806, Tokens seen: 152552\n",
      "Ep 1 (Step 000105): Train loss 1.550, Val loss 1.793, Tokens seen: 160736\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Re-write the following sentence to use a different verb\n",
      "\n",
      "### Input:\n",
      "I will read the book tomorrow\n",
      "\n",
      "### Response:\n",
      "Tomorrow is tomorrow.<|endoftext|>The following article is an opinion article written\n",
      " Checkpoint saved: checkpoints/checkpoint_epoch1.pth\n",
      "Ep 2 (Step 000110): Train loss 1.466, Val loss 1.800, Tokens seen: 167456\n",
      "Ep 2 (Step 000115): Train loss 1.567, Val loss 1.796, Tokens seen: 174848\n",
      "Ep 2 (Step 000120): Train loss 1.375, Val loss 1.802, Tokens seen: 182032\n",
      "Ep 2 (Step 000125): Train loss 1.475, Val loss 1.799, Tokens seen: 188920\n",
      "Ep 2 (Step 000130): Train loss 1.435, Val loss 1.815, Tokens seen: 194624\n",
      "Ep 2 (Step 000135): Train loss 1.422, Val loss 1.805, Tokens seen: 203032\n",
      "Ep 2 (Step 000140): Train loss 1.490, Val loss 1.813, Tokens seen: 211480\n",
      "Ep 2 (Step 000145): Train loss 1.433, Val loss 1.815, Tokens seen: 219552\n",
      "Ep 2 (Step 000150): Train loss 1.429, Val loss 1.792, Tokens seen: 226232\n",
      "Ep 2 (Step 000155): Train loss 1.455, Val loss 1.796, Tokens seen: 236208\n",
      "Ep 2 (Step 000160): Train loss 1.260, Val loss 1.792, Tokens seen: 242680\n",
      "Ep 2 (Step 000165): Train loss 1.386, Val loss 1.806, Tokens seen: 250152\n",
      "Ep 2 (Step 000170): Train loss 1.278, Val loss 1.799, Tokens seen: 257568\n",
      "Ep 2 (Step 000175): Train loss 1.348, Val loss 1.805, Tokens seen: 269352\n",
      "Ep 2 (Step 000180): Train loss 1.313, Val loss 1.807, Tokens seen: 275824\n",
      "Ep 2 (Step 000185): Train loss 1.355, Val loss 1.826, Tokens seen: 283984\n",
      "Ep 2 (Step 000190): Train loss 1.290, Val loss 1.804, Tokens seen: 291208\n",
      "Ep 2 (Step 000195): Train loss 1.339, Val loss 1.809, Tokens seen: 298056\n",
      "Ep 2 (Step 000200): Train loss 1.361, Val loss 1.782, Tokens seen: 304256\n",
      "Ep 2 (Step 000205): Train loss 1.304, Val loss 1.817, Tokens seen: 313488\n",
      "Ep 2 (Step 000210): Train loss 1.186, Val loss 1.808, Tokens seen: 321464\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Re-write the following sentence to use a different verb\n",
      "\n",
      "### Input:\n",
      "I will read the book tomorrow\n",
      "\n",
      "### Response:\n",
      "Tomorrow is tomorrow.<|endoftext|>The following article is an opinion article written\n",
      " Checkpoint saved: checkpoints/checkpoint_epoch2.pth\n",
      " Training complete. Final model saved: checkpoints/final_model.pth\n",
      "Training completed in 12.75 minutes.\n"
     ]
    }
   ],
   "execution_count": 180
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:59:49.629534Z",
     "start_time": "2025-06-15T08:59:49.318892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt2_v2 import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ],
   "id": "fe49e1158904c239",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T09:01:06.645586Z",
     "start_time": "2025-06-15T09:01:05.335564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[1])\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_tensor(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=1024,\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = tensor_to_text(token_ids, tokenizer)\n",
    "print(generated_text)"
   ],
   "id": "1818eb1a3d9ca2c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Propose a suitable title for the below article\n",
      "\n",
      "### Input:\n",
      "This article discusses the current state of the music industry and how technology has had an impact on its evolution.\n",
      "\n",
      "### Response:\n",
      "The current state of the music industry and how technology has had an impact on its evolution.\n"
     ]
    }
   ],
   "execution_count": 210
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
