{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Preference Dataset",
   "id": "ed9b7c67aa46cc35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:47:02.147713Z",
     "start_time": "2025-07-18T09:47:02.133255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import amp\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device (use MPS for Apple Silicon if available, else CPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "92f71d56245fe997",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:47:02.166173Z",
     "start_time": "2025-07-18T09:47:02.154153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset from local disk if exists, otherwise download and sample\n",
    "local_path = \"./orca_dpo_sample_1000\"\n",
    "\n",
    "if os.path.exists(local_path):\n",
    "    print(\"Loading dataset from local disk...\")\n",
    "    dataset = load_from_disk(local_path)\n",
    "else:\n",
    "    print(\"Downloading dataset from HuggingFace...\")\n",
    "    full_dataset = load_dataset(\"Intel/orca_dpo_pairs\", split=\"train\")\n",
    "    sampled_dataset = full_dataset.shuffle(seed=42).select(range(1000))\n",
    "    train_size = int(0.8 * len(sampled_dataset))\n",
    "    val_size = int(0.1 * len(sampled_dataset))\n",
    "    train_dataset = sampled_dataset.select(range(train_size))\n",
    "    val_dataset = sampled_dataset.select(range(train_size, train_size + val_size))\n",
    "    test_dataset = sampled_dataset.select(range(train_size + val_size, len(sampled_dataset)))\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset,\n",
    "        \"test\": test_dataset\n",
    "    })\n",
    "    dataset.save_to_disk(local_path)\n",
    "    print(f\"Dataset saved locally at: {local_path}\")\n",
    "\n",
    "print(f\"Train size: {len(dataset['train'])}\")\n",
    "print(f\"Validation size: {len(dataset['validation'])}\")\n",
    "print(f\"Test size: {len(dataset['test'])}\")\n",
    "print(\"\\nExample sample:\")\n",
    "print(dataset[\"train\"][0])"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from local disk...\n",
      "Train size: 800\n",
      "Validation size: 100\n",
      "Test size: 100\n",
      "\n",
      "Example sample:\n",
      "{'system': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.', 'question': 'This is some data: CBS PLAY-BY-PLAY Chris Schenkel (first half) and Ray Scott (second half); 1962 NETWORK CBS.\\n\\nGenerate a detailed description of this data', 'chosen': \"Okay, imagine you are watching a fun game on TV with your family. In this case, the game happened in 1962. Now, on TV, there are people who talk to us and tell us what is happening in the game. They help us understand the game better, just like how I'm helping you understand things right now.\\n\\nIn this data, there are two people who talked about the game in 1962. The first person, Chris Schenkel, talked about the game in the first half. The second person, Ray Scott, talked about the game in the second half. Both of them worked for a big TV company called CBS. So, this sentence is just telling us who talked about the game on TV and when they did it.\", 'rejected': ' OH MY GOSH, YOU WANT TO KNOW ABOUT THIS SUPER COOL DATA?! üòç\\n\\nOkay, so let me tell you all about it! ü§ì\\n\\nThis data is like a special treasure that tells us about something called \"CBS PLAY-BY-PLAY\" and two super cool people who helped make it happen! üéâ\\n\\nThe first person\\'s name is Chris Schenkel, and he was the play-by-play guy for the first half of the thingy. üèàüëç\\n\\nThe second person\\'s name is Ray Scott, and he was the play-by-play guy for the second half of the thingy. üèàüëç\\n\\nBut wait, there\\'s more! üò± This whole thing happened in 1962, which is like, a really long time ago! ü§Ø\\n\\nAnd the best part? üéâ It all happened on a special network called CBS! üì∫üëç\\n\\nSo, in other words, this data is like a special memory book that tells us about two cool dudes who helped make some amazing sports stuff happen on TV a long time ago! üòÑ\\n\\nIsn\\'t that just the most amazing thing you\\'ve ever heard?! üòç'}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Model",
   "id": "dd716c1ad12f7053"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:47:08.240201Z",
     "start_time": "2025-07-18T09:47:02.172255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"openai-community/gpt2-medium\"\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device.type == \"mps\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device.type == \"mps\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "8640040c286d45dd",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:47:08.251376Z",
     "start_time": "2025-07-18T09:47:08.248998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess data: format and tokenize\n",
    "def format_dpo_data(example):\n",
    "    # Map 'question' to 'prompt' and include 'system' in the prompt for context\n",
    "    prompt = f\"{example['system']}\\n\\n{example['question']}\"\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": example[\"chosen\"],\n",
    "        \"rejected\": example[\"rejected\"]\n",
    "    }"
   ],
   "id": "87bcc7b6c2b29f10",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:47:08.258595Z",
     "start_time": "2025-07-18T09:47:08.256185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_data(example):\n",
    "    prompt_ids = tokenizer(example[\"prompt\"],padding=True, truncation=True, max_length=256)[\"input_ids\"]\n",
    "    chosen_ids = tokenizer(example[\"chosen\"], padding=True, truncation=True, max_length=512)[\"input_ids\"]\n",
    "    rejected_ids = tokenizer(example[\"rejected\"], padding=True, truncation=True, max_length=512)[\"input_ids\"]\n",
    "    return {\n",
    "        \"prompt_ids\": prompt_ids,\n",
    "        \"chosen_ids\": chosen_ids,\n",
    "        \"rejected_ids\": rejected_ids\n",
    "    }\n"
   ],
   "id": "38c6e318b1ff6ddb",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:47:08.411756Z",
     "start_time": "2025-07-18T09:47:08.263676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = dataset.map(format_dpo_data)\n",
    "print(f\"first training example formatted: {dataset['train'][0]}\")\n",
    "\n",
    "dataset = dataset.map(tokenize_data)\n",
    "print(f\"first training example tokenized: {dataset['train'][0]}\")\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"prompt_ids\", \"chosen_ids\", \"rejected_ids\"])\n",
    "print(f\"first training example torch: {dataset['train'][0]}\")\n"
   ],
   "id": "5e666434bc5a922e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first training example formatted: {'system': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.', 'question': 'This is some data: CBS PLAY-BY-PLAY Chris Schenkel (first half) and Ray Scott (second half); 1962 NETWORK CBS.\\n\\nGenerate a detailed description of this data', 'chosen': \"Okay, imagine you are watching a fun game on TV with your family. In this case, the game happened in 1962. Now, on TV, there are people who talk to us and tell us what is happening in the game. They help us understand the game better, just like how I'm helping you understand things right now.\\n\\nIn this data, there are two people who talked about the game in 1962. The first person, Chris Schenkel, talked about the game in the first half. The second person, Ray Scott, talked about the game in the second half. Both of them worked for a big TV company called CBS. So, this sentence is just telling us who talked about the game on TV and when they did it.\", 'rejected': ' OH MY GOSH, YOU WANT TO KNOW ABOUT THIS SUPER COOL DATA?! üòç\\n\\nOkay, so let me tell you all about it! ü§ì\\n\\nThis data is like a special treasure that tells us about something called \"CBS PLAY-BY-PLAY\" and two super cool people who helped make it happen! üéâ\\n\\nThe first person\\'s name is Chris Schenkel, and he was the play-by-play guy for the first half of the thingy. üèàüëç\\n\\nThe second person\\'s name is Ray Scott, and he was the play-by-play guy for the second half of the thingy. üèàüëç\\n\\nBut wait, there\\'s more! üò± This whole thing happened in 1962, which is like, a really long time ago! ü§Ø\\n\\nAnd the best part? üéâ It all happened on a special network called CBS! üì∫üëç\\n\\nSo, in other words, this data is like a special memory book that tells us about two cool dudes who helped make some amazing sports stuff happen on TV a long time ago! üòÑ\\n\\nIsn\\'t that just the most amazing thing you\\'ve ever heard?! üòç', 'prompt': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\\n\\nThis is some data: CBS PLAY-BY-PLAY Chris Schenkel (first half) and Ray Scott (second half); 1962 NETWORK CBS.\\n\\nGenerate a detailed description of this data'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 1044.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first training example tokenized: {'system': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.', 'question': 'This is some data: CBS PLAY-BY-PLAY Chris Schenkel (first half) and Ray Scott (second half); 1962 NETWORK CBS.\\n\\nGenerate a detailed description of this data', 'chosen': \"Okay, imagine you are watching a fun game on TV with your family. In this case, the game happened in 1962. Now, on TV, there are people who talk to us and tell us what is happening in the game. They help us understand the game better, just like how I'm helping you understand things right now.\\n\\nIn this data, there are two people who talked about the game in 1962. The first person, Chris Schenkel, talked about the game in the first half. The second person, Ray Scott, talked about the game in the second half. Both of them worked for a big TV company called CBS. So, this sentence is just telling us who talked about the game on TV and when they did it.\", 'rejected': ' OH MY GOSH, YOU WANT TO KNOW ABOUT THIS SUPER COOL DATA?! üòç\\n\\nOkay, so let me tell you all about it! ü§ì\\n\\nThis data is like a special treasure that tells us about something called \"CBS PLAY-BY-PLAY\" and two super cool people who helped make it happen! üéâ\\n\\nThe first person\\'s name is Chris Schenkel, and he was the play-by-play guy for the first half of the thingy. üèàüëç\\n\\nThe second person\\'s name is Ray Scott, and he was the play-by-play guy for the second half of the thingy. üèàüëç\\n\\nBut wait, there\\'s more! üò± This whole thing happened in 1962, which is like, a really long time ago! ü§Ø\\n\\nAnd the best part? üéâ It all happened on a special network called CBS! üì∫üëç\\n\\nSo, in other words, this data is like a special memory book that tells us about two cool dudes who helped make some amazing sports stuff happen on TV a long time ago! üòÑ\\n\\nIsn\\'t that just the most amazing thing you\\'ve ever heard?! üòç', 'prompt': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\\n\\nThis is some data: CBS PLAY-BY-PLAY Chris Schenkel (first half) and Ray Scott (second half); 1962 NETWORK CBS.\\n\\nGenerate a detailed description of this data', 'prompt_ids': [1639, 389, 257, 7613, 8796, 11, 508, 1464, 2148, 7468, 13, 11382, 588, 345, 389, 18877, 284, 257, 1936, 614, 1468, 13, 198, 198, 1212, 318, 617, 1366, 25, 11133, 28180, 12, 17513, 12, 31519, 5180, 1446, 831, 7750, 357, 11085, 2063, 8, 290, 7760, 4746, 357, 12227, 2063, 1776, 20033, 49791, 11133, 13, 198, 198, 8645, 378, 257, 6496, 6764, 286, 428, 1366], 'chosen_ids': [16454, 11, 5967, 345, 389, 4964, 257, 1257, 983, 319, 3195, 351, 534, 1641, 13, 554, 428, 1339, 11, 262, 983, 3022, 287, 20033, 13, 2735, 11, 319, 3195, 11, 612, 389, 661, 508, 1561, 284, 514, 290, 1560, 514, 644, 318, 5836, 287, 262, 983, 13, 1119, 1037, 514, 1833, 262, 983, 1365, 11, 655, 588, 703, 314, 1101, 5742, 345, 1833, 1243, 826, 783, 13, 198, 198, 818, 428, 1366, 11, 612, 389, 734, 661, 508, 6619, 546, 262, 983, 287, 20033, 13, 383, 717, 1048, 11, 5180, 1446, 831, 7750, 11, 6619, 546, 262, 983, 287, 262, 717, 2063, 13, 383, 1218, 1048, 11, 7760, 4746, 11, 6619, 546, 262, 983, 287, 262, 1218, 2063, 13, 5747, 286, 606, 3111, 329, 257, 1263, 3195, 1664, 1444, 11133, 13, 1406, 11, 428, 6827, 318, 655, 5149, 514, 508, 6619, 546, 262, 983, 319, 3195, 290, 618, 484, 750, 340, 13], 'rejected_ids': [18723, 17615, 402, 45704, 11, 7013, 41300, 5390, 35876, 33478, 12680, 33088, 7375, 3535, 42865, 12248, 30325, 235, 198, 198, 16454, 11, 523, 1309, 502, 1560, 345, 477, 546, 340, 0, 12520, 97, 241, 198, 198, 1212, 1366, 318, 588, 257, 2041, 14068, 326, 4952, 514, 546, 1223, 1444, 366, 22923, 28180, 12, 17513, 12, 31519, 1, 290, 734, 2208, 3608, 661, 508, 4193, 787, 340, 1645, 0, 12520, 236, 231, 198, 198, 464, 717, 1048, 338, 1438, 318, 5180, 1446, 831, 7750, 11, 290, 339, 373, 262, 711, 12, 1525, 12, 1759, 3516, 329, 262, 717, 2063, 286, 262, 1517, 88, 13, 12520, 237, 230, 41840, 235, 198, 198, 464, 1218, 1048, 338, 1438, 318, 7760, 4746, 11, 290, 339, 373, 262, 711, 12, 1525, 12, 1759, 3516, 329, 262, 1218, 2063, 286, 262, 1517, 88, 13, 12520, 237, 230, 41840, 235, 198, 198, 1537, 4043, 11, 612, 338, 517, 0, 30325, 109, 770, 2187, 1517, 3022, 287, 20033, 11, 543, 318, 588, 11, 257, 1107, 890, 640, 2084, 0, 12520, 97, 107, 198, 198, 1870, 262, 1266, 636, 30, 12520, 236, 231, 632, 477, 3022, 319, 257, 2041, 3127, 1444, 11133, 0, 12520, 241, 118, 41840, 235, 198, 198, 2396, 11, 287, 584, 2456, 11, 428, 1366, 318, 588, 257, 2041, 4088, 1492, 326, 4952, 514, 546, 734, 3608, 34578, 508, 4193, 787, 617, 4998, 5701, 3404, 1645, 319, 3195, 257, 890, 640, 2084, 0, 30325, 226, 198, 198, 41451, 470, 326, 655, 262, 749, 4998, 1517, 345, 1053, 1683, 2982, 12248, 30325, 235]}\n",
      "first training example torch: {'prompt_ids': tensor([ 1639,   389,   257,  7613,  8796,    11,   508,  1464,  2148,  7468,\n",
      "           13, 11382,   588,   345,   389, 18877,   284,   257,  1936,   614,\n",
      "         1468,    13,   198,   198,  1212,   318,   617,  1366,    25, 11133,\n",
      "        28180,    12, 17513,    12, 31519,  5180,  1446,   831,  7750,   357,\n",
      "        11085,  2063,     8,   290,  7760,  4746,   357, 12227,  2063,  1776,\n",
      "        20033, 49791, 11133,    13,   198,   198,  8645,   378,   257,  6496,\n",
      "         6764,   286,   428,  1366]), 'chosen_ids': tensor([16454,    11,  5967,   345,   389,  4964,   257,  1257,   983,   319,\n",
      "         3195,   351,   534,  1641,    13,   554,   428,  1339,    11,   262,\n",
      "          983,  3022,   287, 20033,    13,  2735,    11,   319,  3195,    11,\n",
      "          612,   389,   661,   508,  1561,   284,   514,   290,  1560,   514,\n",
      "          644,   318,  5836,   287,   262,   983,    13,  1119,  1037,   514,\n",
      "         1833,   262,   983,  1365,    11,   655,   588,   703,   314,  1101,\n",
      "         5742,   345,  1833,  1243,   826,   783,    13,   198,   198,   818,\n",
      "          428,  1366,    11,   612,   389,   734,   661,   508,  6619,   546,\n",
      "          262,   983,   287, 20033,    13,   383,   717,  1048,    11,  5180,\n",
      "         1446,   831,  7750,    11,  6619,   546,   262,   983,   287,   262,\n",
      "          717,  2063,    13,   383,  1218,  1048,    11,  7760,  4746,    11,\n",
      "         6619,   546,   262,   983,   287,   262,  1218,  2063,    13,  5747,\n",
      "          286,   606,  3111,   329,   257,  1263,  3195,  1664,  1444, 11133,\n",
      "           13,  1406,    11,   428,  6827,   318,   655,  5149,   514,   508,\n",
      "         6619,   546,   262,   983,   319,  3195,   290,   618,   484,   750,\n",
      "          340,    13]), 'rejected_ids': tensor([18723, 17615,   402, 45704,    11,  7013, 41300,  5390, 35876, 33478,\n",
      "        12680, 33088,  7375,  3535, 42865, 12248, 30325,   235,   198,   198,\n",
      "        16454,    11,   523,  1309,   502,  1560,   345,   477,   546,   340,\n",
      "            0, 12520,    97,   241,   198,   198,  1212,  1366,   318,   588,\n",
      "          257,  2041, 14068,   326,  4952,   514,   546,  1223,  1444,   366,\n",
      "        22923, 28180,    12, 17513,    12, 31519,     1,   290,   734,  2208,\n",
      "         3608,   661,   508,  4193,   787,   340,  1645,     0, 12520,   236,\n",
      "          231,   198,   198,   464,   717,  1048,   338,  1438,   318,  5180,\n",
      "         1446,   831,  7750,    11,   290,   339,   373,   262,   711,    12,\n",
      "         1525,    12,  1759,  3516,   329,   262,   717,  2063,   286,   262,\n",
      "         1517,    88,    13, 12520,   237,   230, 41840,   235,   198,   198,\n",
      "          464,  1218,  1048,   338,  1438,   318,  7760,  4746,    11,   290,\n",
      "          339,   373,   262,   711,    12,  1525,    12,  1759,  3516,   329,\n",
      "          262,  1218,  2063,   286,   262,  1517,    88,    13, 12520,   237,\n",
      "          230, 41840,   235,   198,   198,  1537,  4043,    11,   612,   338,\n",
      "          517,     0, 30325,   109,   770,  2187,  1517,  3022,   287, 20033,\n",
      "           11,   543,   318,   588,    11,   257,  1107,   890,   640,  2084,\n",
      "            0, 12520,    97,   107,   198,   198,  1870,   262,  1266,   636,\n",
      "           30, 12520,   236,   231,   632,   477,  3022,   319,   257,  2041,\n",
      "         3127,  1444, 11133,     0, 12520,   241,   118, 41840,   235,   198,\n",
      "          198,  2396,    11,   287,   584,  2456,    11,   428,  1366,   318,\n",
      "          588,   257,  2041,  4088,  1492,   326,  4952,   514,   546,   734,\n",
      "         3608, 34578,   508,  4193,   787,   617,  4998,  5701,  3404,  1645,\n",
      "          319,  3195,   257,   890,   640,  2084,     0, 30325,   226,   198,\n",
      "          198, 41451,   470,   326,   655,   262,   749,  4998,  1517,   345,\n",
      "         1053,  1683,  2982, 12248, 30325,   235])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:47:08.702165Z",
     "start_time": "2025-07-18T09:47:08.699186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom collate function to ensure consistent tensor shapes\n",
    "def collate_fn(batch):\n",
    "    prompt_ids = [item[\"prompt_ids\"] for item in batch]\n",
    "    chosen_ids = [item[\"chosen_ids\"] for item in batch]\n",
    "    rejected_ids = [item[\"rejected_ids\"] for item in batch]\n",
    "\n",
    "    # Ê≥®ÊÑèÔºö‰Ω†ÈúÄË¶ÅÊèêÂâç‰º†ÂÖ• tokenizer ÂÆû‰æã\n",
    "    prompt_ids = tokenizer.pad({\"input_ids\": prompt_ids}, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    chosen_ids = tokenizer.pad({\"input_ids\": chosen_ids}, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    rejected_ids = tokenizer.pad({\"input_ids\": rejected_ids}, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    return {\n",
    "        \"prompt_ids\": prompt_ids,\n",
    "        \"chosen_ids\": chosen_ids,\n",
    "        \"rejected_ids\": rejected_ids\n",
    "    }\n"
   ],
   "id": "1533fcbfb93664cf",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:47:08.716786Z",
     "start_time": "2025-07-18T09:47:08.714071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataLoader with custom collate function\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset[\"validation\"], batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ],
   "id": "57d999345aaa72d1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DPO loss",
   "id": "c60148cf16501368"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:47:08.731636Z",
     "start_time": "2025-07-18T09:47:08.727257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define DPO loss computation for a single batch\n",
    "def compute_dpo_loss_batch(batch, policy_model, reference_model, beta):\n",
    "    # Move input tensors to device\n",
    "    prompt_ids = batch[\"prompt_ids\"].to(device)\n",
    "    chosen_ids = batch[\"chosen_ids\"].to(device)\n",
    "    rejected_ids = batch[\"rejected_ids\"].to(device)\n",
    "\n",
    "    # Compute attention masks\n",
    "    chosen_mask = (chosen_ids != tokenizer.pad_token_id).to(device)\n",
    "    rejected_mask = (rejected_ids != tokenizer.pad_token_id).to(device)\n",
    "\n",
    "    # Compute logits for chosen and rejected responses\n",
    "    with torch.no_grad():\n",
    "        ref_chosen_outputs = reference_model(chosen_ids, attention_mask=chosen_mask)\n",
    "        ref_rejected_outputs = reference_model(rejected_ids, attention_mask=rejected_mask)\n",
    "    with amp.autocast(device_type=\"mps\" if device.type == \"mps\" else \"cpu\"):\n",
    "        policy_chosen_outputs = policy_model(chosen_ids, attention_mask=chosen_mask)\n",
    "        policy_rejected_outputs = policy_model(rejected_ids, attention_mask=rejected_mask)\n",
    "\n",
    "    # Compute log probabilities\n",
    "    chosen_logprobs = -F.log_softmax(policy_chosen_outputs.logits, dim=-1).mean(dim=-1)\n",
    "    rejected_logprobs = -F.log_softmax(policy_rejected_outputs.logits, dim=-1).mean(dim=-1)\n",
    "    ref_chosen_logprobs = -F.log_softmax(ref_chosen_outputs.logits, dim=-1).mean(dim=-1)\n",
    "    ref_rejected_logprobs = -F.log_softmax(ref_rejected_outputs.logits, dim=-1).mean(dim=-1)\n",
    "\n",
    "    # Compute DPO loss\n",
    "    log_ratio_chosen = chosen_logprobs - ref_chosen_logprobs\n",
    "    log_ratio_rejected = rejected_logprobs - ref_rejected_logprobs\n",
    "    logits = beta * (log_ratio_chosen - log_ratio_rejected)\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "\n",
    "    # Compute rewards for monitoring\n",
    "    chosen_rewards = beta * log_ratio_chosen\n",
    "    rejected_rewards = beta * log_ratio_rejected\n",
    "\n",
    "    return loss, chosen_rewards.mean(), rejected_rewards.mean()"
   ],
   "id": "fa6ac8421b5b8f19",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:47:08.737870Z",
     "start_time": "2025-07-18T09:47:08.734503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Define DPO loss computation for entire DataLoader\n",
    "def compute_dpo_loss_loader(data_loader, policy_model, reference_model, beta, num_batches=None):\n",
    "    # Initialize accumulators\n",
    "    total_loss, total_chosen_rewards, total_rejected_rewards = 0., 0., 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    policy_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n",
    "                batch=batch,\n",
    "                policy_model=policy_model,\n",
    "                reference_model=reference_model,\n",
    "                beta=beta\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            total_chosen_rewards += chosen_rewards.item()\n",
    "            total_rejected_rewards += rejected_rewards.item()\n",
    "\n",
    "    # Compute averages\n",
    "    total_loss /= num_batches\n",
    "    total_chosen_rewards /= num_batches\n",
    "    total_rejected_rewards /= num_batches\n",
    "    return total_loss, total_chosen_rewards, total_rejected_rewards"
   ],
   "id": "f0a92e716d8c3190",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train loop",
   "id": "4eca552738817ad7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:47:11.815084Z",
     "start_time": "2025-07-18T09:47:08.742260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5)\n",
    "num_epochs = 3\n",
    "beta = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    policy_model.train()\n",
    "    total_loss, total_chosen_rewards, total_rejected_rewards = 0., 0., 0.\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        with amp.autocast(device_type=\"mps\" if device.type == \"mps\" else \"cpu\"):\n",
    "            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n",
    "                batch=batch,\n",
    "                policy_model=policy_model,\n",
    "                reference_model=reference_model,\n",
    "                beta=beta\n",
    "            )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_chosen_rewards += chosen_rewards.item()\n",
    "        total_rejected_rewards += rejected_rewards.item()\n",
    "\n",
    "    # Compute average metrics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_chosen_rewards = total_chosen_rewards / len(train_loader)\n",
    "    avg_rejected_rewards = total_rejected_rewards / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}, Chosen Rewards: {avg_chosen_rewards:.4f}, Rejected Rewards: {avg_rejected_rewards:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_chosen_rewards, val_rejected_rewards = compute_dpo_loss_loader(\n",
    "        val_loader, policy_model, reference_model, beta\n",
    "    )\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Chosen Rewards: {val_chosen_rewards:.4f}, Rejected Rewards: {val_rejected_rewards:.4f}\")"
   ],
   "id": "6c953f11f9abba79",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (225) must match the size of tensor b (185) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m amp\u001B[38;5;241m.\u001B[39mautocast(device_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m device\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m---> 13\u001B[0m     loss, chosen_rewards, rejected_rewards \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_dpo_loss_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreference_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreference_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     20\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "Cell \u001B[0;32mIn[20], line 29\u001B[0m, in \u001B[0;36mcompute_dpo_loss_batch\u001B[0;34m(batch, policy_model, reference_model, beta)\u001B[0m\n\u001B[1;32m     27\u001B[0m log_ratio_chosen \u001B[38;5;241m=\u001B[39m chosen_logprobs \u001B[38;5;241m-\u001B[39m ref_chosen_logprobs\n\u001B[1;32m     28\u001B[0m log_ratio_rejected \u001B[38;5;241m=\u001B[39m rejected_logprobs \u001B[38;5;241m-\u001B[39m ref_rejected_logprobs\n\u001B[0;32m---> 29\u001B[0m logits \u001B[38;5;241m=\u001B[39m beta \u001B[38;5;241m*\u001B[39m (\u001B[43mlog_ratio_chosen\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlog_ratio_rejected\u001B[49m)\n\u001B[1;32m     30\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mF\u001B[38;5;241m.\u001B[39mlogsigmoid(logits)\u001B[38;5;241m.\u001B[39mmean()\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Compute rewards for monitoring\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (225) must match the size of tensor b (185) at non-singleton dimension 1"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:56:04.222773Z",
     "start_time": "2025-07-18T09:56:04.216048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ],
   "id": "dc20fb809df81acb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save model and test",
   "id": "4fe11bc7789bdf2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save model and tokenizer\n",
    "policy_model.save_pretrained(\"./gpt2_medium_dpo_final\")\n",
    "tokenizer.save_pretrained(\"./gpt2_medium_dpo_final\")"
   ],
   "id": "2b301804c379f52f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test set evaluation\n",
    "test_loader = DataLoader(dataset[\"test\"], batch_size=2, shuffle=False)\n",
    "test_loss, test_chosen_rewards, test_rejected_rewards = compute_dpo_loss_loader(\n",
    "    test_loader, policy_model, reference_model, beta\n",
    ")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Chosen Rewards: {test_chosen_rewards:.4f}, Rejected Rewards: {test_rejected_rewards:.4f}\")"
   ],
   "id": "eecbe925e1bb1b9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example inference\n",
    "policy_model.eval()\n",
    "prompt = dataset[\"test\"][0][\"prompt\"]\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = policy_model.generate(**inputs, max_length=512)\n",
    "print(\"\\nExample generation:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "id": "d04cfbe7abdea347"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
