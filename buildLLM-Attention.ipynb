{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîç Attention Mechanism (Scaled Dot-Product)\n",
    "\n",
    "The core formula of Attention is:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( Q \\): Query matrix\n",
    "- \\( K \\): Key matrix\n",
    "- \\( V \\): Value matrix\n",
    "- \\( d_k \\): Dimension of the key vectors (used for scaling)\n",
    "\n",
    "### üß† Intuition\n",
    "\n",
    "The attention mechanism lets the model **focus on the most relevant input tokens** when generating each output, based on the similarity between query and key vectors.\n",
    "\n",
    "It computes a weighted sum of all values (V), where the weights are determined by how well each key (K) matches the current query (Q).\n",
    "\n"
   ],
   "id": "59a758b2dc1c4553"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Input Tokens and Random Embeddings",
   "id": "e3b7895f1134e2a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üì¶ Input Embedding Matrix $X$ with Tokens\n",
    "\n",
    "\\[\n",
    "\\begin{array}{cccc|c}\n",
    "\\text{Dim 1} & \\text{Dim 2} & \\text{Dim 3} & \\text{Dim 4} & \\text{Token} \\\\\n",
    "\\hline\n",
    "\\phantom{-}0.3374 & -0.1778 & -0.3035 & -0.5880 & \\text{\"Once\"} \\\\\n",
    "\\phantom{-}1.5810 & \\phantom{-}1.3010 & \\phantom{-}1.2753 & -0.2010 & \\text{\"upon\"} \\\\\n",
    "-0.1606 & -0.4015 & \\phantom{-}0.6957 & -1.8061 & \\text{\"a\"} \\\\\n",
    "-1.1589 & \\phantom{-}0.3255 & -0.6315 & -2.8400 & \\text{\"time\"} \\\\\n",
    "-0.7849 & -1.4096 & -0.4076 & \\phantom{-}0.7953 & \\text{\"there\"}\n",
    "\\end{array}\n",
    "\\]\n"
   ],
   "id": "74f2e6ec523e9a6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.011462Z",
     "start_time": "2025-06-04T02:14:42.977772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokens = [\"Once\", \"upon\", \"a\", \"time\", \"there\"]\n",
    "token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
    "embedding_dim = 4\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(tokens), embedding_dim=embedding_dim)\n",
    "\n",
    "input_indices = torch.tensor([token_to_idx[token] for token in tokens])  # [0,1,2,3,4]\n",
    "X = embedding_layer(input_indices)\n",
    "print(\"shape of input X:\", X.shape)\n",
    "print(X)"
   ],
   "id": "76e44b3939241124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input X: torch.Size([5, 4])\n",
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880],\n",
      "        [ 1.5810,  1.3010,  1.2753, -0.2010],\n",
      "        [-0.1606, -0.4015,  0.6957, -1.8061],\n",
      "        [-1.1589,  0.3255, -0.6315, -2.8400],\n",
      "        [-0.7849, -1.4096, -0.4076,  0.7953]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 340
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Query, Key, and Value Matrices",
   "id": "1693840fd356b8f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.038275Z",
     "start_time": "2025-06-04T02:14:43.028464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_Q = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim), requires_grad=False)\n",
    "W_K = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim), requires_grad=False)\n",
    "W_V = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim), requires_grad=False)\n",
    "\n",
    "print(\"shape of W_Q:\", W_Q.shape)\n",
    "print(\"W_Q:\", W_Q)\n",
    "\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "print(\"shape of Q:\", Q.shape)\n",
    "print(\"Q:\", Q)\n"
   ],
   "id": "317030b3acc175f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of W_Q: torch.Size([4, 4])\n",
      "W_Q: Parameter containing:\n",
      "tensor([[0.2961, 0.5166, 0.2517, 0.6886],\n",
      "        [0.0740, 0.8665, 0.1366, 0.1025],\n",
      "        [0.1841, 0.7264, 0.3153, 0.6871],\n",
      "        [0.0756, 0.1966, 0.3164, 0.4017]])\n",
      "shape of Q: torch.Size([5, 4])\n",
      "Q: tensor([[-0.0136, -0.3159, -0.2211, -0.2307],\n",
      "        [ 0.7839,  2.8310,  0.9140,  2.0175],\n",
      "        [-0.0858, -0.2806, -0.4474, -0.3992],\n",
      "        [-0.6501, -1.3338, -1.3449, -2.3394],\n",
      "        [-0.3515, -1.7666, -0.2669, -0.6454]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 341
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Compute similarity (dot product between Q and K):\n",
    "\n",
    "$$\n",
    "\\text{scores} = Q K^T\n",
    "$$\n"
   ],
   "id": "65cc817f97122421"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.063484Z",
     "start_time": "2025-06-04T02:14:43.059025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scores = Q @ K.T\n",
    "print(\"shape of scores:\", scores.shape)\n",
    "print(\"scores:\", scores)"
   ],
   "id": "a90f194002861335",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of scores: torch.Size([5, 5])\n",
      "scores: tensor([[  0.3101,  -2.0474,   0.7024,   1.8280,   1.0647],\n",
      "        [ -2.5714,  17.4476,  -5.5017, -14.6920,  -9.3044],\n",
      "        [  0.6084,  -2.9632,   1.4480,   3.1775,   1.4642],\n",
      "        [  2.8736, -14.6337,   6.4597,  14.7155,   7.4156],\n",
      "        [  0.9222,  -8.1955,   1.8808,   5.9959,   4.5150]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 342
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "2. **Scale the scores:**\n",
    "\n",
    "$$\n",
    "\\text{scaled\\_scores} = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "$$\n"
   ],
   "id": "459cbf5dd1f0cb64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.148644Z",
     "start_time": "2025-06-04T02:14:43.145513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "attention_scores = scores / math.sqrt(embedding_dim)\n",
    "print(attention_scores)"
   ],
   "id": "d5b48ae51b9a46b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1551, -1.0237,  0.3512,  0.9140,  0.5323],\n",
      "        [-1.2857,  8.7238, -2.7508, -7.3460, -4.6522],\n",
      "        [ 0.3042, -1.4816,  0.7240,  1.5888,  0.7321],\n",
      "        [ 1.4368, -7.3169,  3.2298,  7.3577,  3.7078],\n",
      "        [ 0.4611, -4.0977,  0.9404,  2.9979,  2.2575]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "execution_count": 343
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Apply softmax to obtain attention weights\n",
    "\n",
    "$$\n",
    "\\text{attention\\_weights} = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right)\n",
    "$$"
   ],
   "id": "1da117e9a2bcc599"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.184845Z",
     "start_time": "2025-06-04T02:14:43.179468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x) # e_x = np.exp(x - np.max(x)) is better for stability\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "scores_np = attention_scores.detach().numpy()\n",
    "attention_weights = softmax(scores_np)\n",
    "print(attention_weights)"
   ],
   "id": "3f06d4935aa2a971",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6343656e-01 5.0282925e-02 1.9884653e-01 3.4909919e-01 2.3833480e-01]\n",
      " [4.4966473e-05 9.9994296e-01 1.0389488e-05 1.0493805e-07 1.5518635e-06]\n",
      " [1.2761034e-01 2.1395484e-02 1.9417544e-01 4.6106294e-01 1.9575578e-01]\n",
      " [2.5676459e-03 4.0538399e-07 1.5425654e-02 9.5712799e-01 2.4878288e-02]\n",
      " [4.6963401e-02 4.9190823e-04 7.5843528e-02 5.9360790e-01 2.8309324e-01]]\n"
     ]
    }
   ],
   "execution_count": 344
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.223333Z",
     "start_time": "2025-06-04T02:14:43.220064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(attention_weights)"
   ],
   "id": "8be298054981555a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6344e-01, 5.0283e-02, 1.9885e-01, 3.4910e-01, 2.3833e-01],\n",
      "        [4.4966e-05, 9.9994e-01, 1.0389e-05, 1.0494e-07, 1.5519e-06],\n",
      "        [1.2761e-01, 2.1395e-02, 1.9418e-01, 4.6106e-01, 1.9576e-01],\n",
      "        [2.5676e-03, 4.0538e-07, 1.5426e-02, 9.5713e-01, 2.4878e-02],\n",
      "        [4.6963e-02, 4.9191e-04, 7.5844e-02, 5.9361e-01, 2.8309e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 345
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Use the weights to compute a weighted sum of values\n",
    "\n",
    "$$\n",
    "\\text{output} = \\text{attention\\_weights} \\cdot V\n",
    "$$\n"
   ],
   "id": "2cb1f4b12af4b5f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.255768Z",
     "start_time": "2025-06-04T02:14:43.253099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final output of self-attention\n",
    "output = attention_weights @ V\n",
    "print(\"shape of output:\", output.shape)\n",
    "print(output)"
   ],
   "id": "429dfd3747ca9748",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output: torch.Size([5, 4])\n",
      "tensor([[-1.0221, -1.1318, -1.0966, -1.2475],\n",
      "        [ 1.6613,  1.7716,  2.1347,  2.5049],\n",
      "        [-1.3064, -1.3985, -1.3982, -1.5418],\n",
      "        [-2.2928, -2.2490, -2.4211, -2.5138],\n",
      "        [-1.6010, -1.6693, -1.7563, -1.9028]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 346
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### build simple Self-Attention",
   "id": "13af5dc1e3eabf07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.291626Z",
     "start_time": "2025-06-04T02:14:43.288326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_K = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_V = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = x @ self.W_Q  # (seq_len, d_out)\n",
    "        K = x @ self.W_K\n",
    "        V = x @ self.W_V\n",
    "\n",
    "        scores = Q @ K.T / K.shape[-1]**0.5  # (seq_len, seq_len)\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        context = weights @ V  # (seq_len, d_out)\n",
    "        return context\n"
   ],
   "id": "52775a368b0ee39c",
   "outputs": [],
   "execution_count": 347
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.323347Z",
     "start_time": "2025-06-04T02:14:43.319847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "sa = SelfAttentionV1(4, 4)\n",
    "output = sa(X)\n",
    "print(output)"
   ],
   "id": "64449b52079c1323",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0221, -1.1318, -1.0966, -1.2475],\n",
      "        [ 1.6613,  1.7716,  2.1347,  2.5049],\n",
      "        [-1.3064, -1.3985, -1.3982, -1.5418],\n",
      "        [-2.2928, -2.2490, -2.4211, -2.5138],\n",
      "        [-1.6010, -1.6693, -1.7563, -1.9028]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 348
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.390098Z",
     "start_time": "2025-06-04T02:14:43.386638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_K = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_V = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.W_Q(x)  # (seq_len, d_out)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) / K.shape[-1]**0.5  # (seq_len, seq_len)\n",
    "        weights = F.softmax(scores, dim=-1)                 # (seq_len, seq_len)\n",
    "\n",
    "        context = weights @ V  # (seq_len, d_out)\n",
    "        return context\n"
   ],
   "id": "c9292e78338a2bd5",
   "outputs": [],
   "execution_count": 349
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.415261Z",
     "start_time": "2025-06-04T02:14:43.411651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "sa = SelfAttentionV2(4, 4)\n",
    "output = sa(X)\n",
    "print(output)"
   ],
   "id": "5016c70fcd738dfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1318, -0.1000, -0.4239, -0.0858],\n",
      "        [-0.0532,  0.2164, -0.8386, -0.1107],\n",
      "        [ 0.2318, -0.2270, -0.4083, -0.0919],\n",
      "        [ 0.4762, -0.5514, -0.2901, -0.0859],\n",
      "        [ 0.0700, -0.0399, -0.3281, -0.0728]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 350
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Casual Attention: Mask future words",
   "id": "a08feecb11056e53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.441763Z",
     "start_time": "2025-06-04T02:14:43.439960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# scores and weights shape: (seq_len, seq_len)\n",
    "print(\"shape of attention_scores matrix: \",attention_scores.shape)\n",
    "print(\"shape of attention_weights matrix: \",attention_weights.shape)\n",
    "\n",
    "# output shape: (seq_len, embedding_dim)\n",
    "print(\"shape of output matrix: \",output.shape)"
   ],
   "id": "be64df46a9baaf59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of attention_scores matrix:  torch.Size([5, 5])\n",
      "shape of attention_weights matrix:  torch.Size([5, 5])\n",
      "shape of output matrix:  torch.Size([5, 4])\n"
     ]
    }
   ],
   "execution_count": 351
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.474198Z",
     "start_time": "2025-06-04T02:14:43.471223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_size = attention_weights.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_size, context_size))\n",
    "print(mask_simple)"
   ],
   "id": "7c1110f875ce3b2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 352
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.508942Z",
     "start_time": "2025-06-04T02:14:43.505454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(attention_weights)\n",
    "masked_weights = attention_weights * mask_simple\n",
    "print(masked_weights)"
   ],
   "id": "f7f12fe27ef62fa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6344e-01, 5.0283e-02, 1.9885e-01, 3.4910e-01, 2.3833e-01],\n",
      "        [4.4966e-05, 9.9994e-01, 1.0389e-05, 1.0494e-07, 1.5519e-06],\n",
      "        [1.2761e-01, 2.1395e-02, 1.9418e-01, 4.6106e-01, 1.9576e-01],\n",
      "        [2.5676e-03, 4.0538e-07, 1.5426e-02, 9.5713e-01, 2.4878e-02],\n",
      "        [4.6963e-02, 4.9191e-04, 7.5844e-02, 5.9361e-01, 2.8309e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.6344e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.4966e-05, 9.9994e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.2761e-01, 2.1395e-02, 1.9418e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.5676e-03, 4.0538e-07, 1.5426e-02, 9.5713e-01, 0.0000e+00],\n",
      "        [4.6963e-02, 4.9191e-04, 7.5844e-02, 5.9361e-01, 2.8309e-01]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 353
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.542486Z",
     "start_time": "2025-06-04T02:14:43.538459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "masked_weights_normalized = masked_weights / torch.sum(masked_weights, dim=-1, keepdim=True)\n",
    "print(masked_weights_normalized)"
   ],
   "id": "6f7bc22b05ed526e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.4967e-05, 9.9996e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.7185e-01, 6.2345e-02, 5.6581e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.6332e-03, 4.1573e-07, 1.5819e-02, 9.8155e-01, 0.0000e+00],\n",
      "        [4.6963e-02, 4.9191e-04, 7.5844e-02, 5.9361e-01, 2.8309e-01]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "execution_count": 354
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dropout: additional mask\n",
    "- scaled: 1 / (1 - `dropout_rate`)\n"
   ],
   "id": "8189638c4e7d4883"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.578928Z",
     "start_time": "2025-06-04T02:14:43.570521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Create a dropout layer with 20% dropout rate\n",
    "dropout = torch.nn.Dropout(0.2)\n",
    "dropout.train()  # Explicitly set to training mode to enable dropout\n",
    "\n",
    "example = torch.ones(100, 100)\n",
    "\n",
    "print(\"Input tensor:\")\n",
    "print(example)\n",
    "\n",
    "# Apply dropout to the input tensor\n",
    "output = dropout(example)\n",
    "\n",
    "print(\"\\nOutput tensor after Dropout:\")\n",
    "print(output)\n",
    "print(f\"\\nNumber of zeros in output: {(output == 0).sum().item()}\")\n",
    "print(f\"Output mean value (should be ~1.0 due to scaling): {output.mean().item():.4f}\")\n"
   ],
   "id": "15e8995a597bce27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "\n",
      "Output tensor after Dropout:\n",
      "tensor([[1.2500, 1.2500, 1.2500,  ..., 0.0000, 1.2500, 1.2500],\n",
      "        [0.0000, 1.2500, 0.0000,  ..., 1.2500, 1.2500, 1.2500],\n",
      "        [1.2500, 1.2500, 1.2500,  ..., 1.2500, 1.2500, 1.2500],\n",
      "        ...,\n",
      "        [1.2500, 1.2500, 0.0000,  ..., 0.0000, 1.2500, 1.2500],\n",
      "        [0.0000, 0.0000, 1.2500,  ..., 0.0000, 1.2500, 1.2500],\n",
      "        [1.2500, 1.2500, 1.2500,  ..., 1.2500, 0.0000, 1.2500]])\n",
      "\n",
      "Number of zeros in output: 1995\n",
      "Output mean value (should be ~1.0 due to scaling): 1.0006\n"
     ]
    }
   ],
   "execution_count": 355
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:14:43.608616Z",
     "start_time": "2025-06-04T02:14:43.604950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"original attention weights: \\n\", attention_weights)\n",
    "torch.manual_seed(123)\n",
    "output = dropout(attention_weights)\n",
    "print(\"Output tensor after Dropout: \\n\", output)\n"
   ],
   "id": "82719649039e7fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original attention weights: \n",
      " tensor([[1.6344e-01, 5.0283e-02, 1.9885e-01, 3.4910e-01, 2.3833e-01],\n",
      "        [4.4966e-05, 9.9994e-01, 1.0389e-05, 1.0494e-07, 1.5519e-06],\n",
      "        [1.2761e-01, 2.1395e-02, 1.9418e-01, 4.6106e-01, 1.9576e-01],\n",
      "        [2.5676e-03, 4.0538e-07, 1.5426e-02, 9.5713e-01, 2.4878e-02],\n",
      "        [4.6963e-02, 4.9191e-04, 7.5844e-02, 5.9361e-01, 2.8309e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Output tensor after Dropout: \n",
      " tensor([[2.0430e-01, 6.2854e-02, 2.4856e-01, 4.3637e-01, 2.9792e-01],\n",
      "        [5.6208e-05, 1.2499e+00, 1.2987e-05, 0.0000e+00, 1.9398e-06],\n",
      "        [0.0000e+00, 2.6744e-02, 2.4272e-01, 5.7633e-01, 2.4469e-01],\n",
      "        [3.2096e-03, 5.0673e-07, 1.9282e-02, 1.1964e+00, 3.1098e-02],\n",
      "        [5.8704e-02, 6.1489e-04, 9.4804e-02, 7.4201e-01, 3.5387e-01]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 356
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Casual Self-Attention code",
   "id": "50a8bb9fb69e21dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:30:09.357321Z",
     "start_time": "2025-06-04T02:30:09.351866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements single-head causal self-attention with optional dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_K = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_V = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create a fixed causal mask (upper triangular) [1 means \"mask\"]\n",
    "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        self.register_buffer(\"mask\", mask.bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (batch_size, seq_len, d_in)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        Q = self.W_Q(x)   # (batch_size, seq_len, d_out)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = Q @ K.transpose(-2, -1) / (d_out ** 0.5)  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # Apply causal mask\n",
    "        scores = scores.masked_fill(self.mask[:seq_len, :seq_len], -torch.inf)\n",
    "\n",
    "        # Compute softmax weights and apply dropout\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # Compute output\n",
    "        output = weights @ V  # (batch_size, seq_len, d_out)\n",
    "        return output\n"
   ],
   "id": "75e61ec943654665",
   "outputs": [],
   "execution_count": 383
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:31:53.156543Z",
     "start_time": "2025-06-04T02:31:53.150522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch = torch.randn(2, 5, 4)  # (batch_size=2, seq_len=5, d_in=4)\n",
    "d_in = 4\n",
    "d_out = 4\n",
    "context_length = batch.size(1)\n",
    "\n",
    "ca = CausalAttention(d_in, d_out, context_length, dropout=0.0)\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n"
   ],
   "id": "75fb28f77047d208",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0487, -0.0112,  0.0449,  0.3506],\n",
      "         [ 0.0439,  0.1278,  0.1848,  0.1733],\n",
      "         [-0.2467, -0.1078,  0.2722,  0.5128],\n",
      "         [-0.1638,  0.0053,  0.3753,  0.3111],\n",
      "         [ 0.0264,  0.1455,  0.3622,  0.0182]],\n",
      "\n",
      "        [[ 0.0960,  0.4257,  1.7419,  0.2045],\n",
      "         [-0.0967,  0.2774,  1.1946,  0.5023],\n",
      "         [ 0.1017,  0.2037,  0.4849,  0.1862],\n",
      "         [-0.0775,  0.1062,  0.3737,  0.3387],\n",
      "         [-0.1181, -0.0113,  0.1070,  0.2743]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "execution_count": 388
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Multi-head Attention\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/24.webp\" width=\"800px\">\n"
   ],
   "id": "71c18a8cec63347c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:22:48.916192Z",
     "start_time": "2025-06-04T02:22:48.909448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements multi-head self-attention by stacking multiple heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CausalAttention(d_in,d_out,context_length,dropout,qkv_bias) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ],
   "id": "e135ed0ca97135cb",
   "outputs": [],
   "execution_count": 363
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:33:25.211551Z",
     "start_time": "2025-06-04T02:33:25.198382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch = torch.randn(2, 5, 4)  # (batch_size=2, seq_len=5, d_in=4)\n",
    "d_in = 4\n",
    "d_out = 4\n",
    "context_length = batch.size(1)\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0,num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n"
   ],
   "id": "e320e7b01e3fce7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0487, -0.0112,  0.0449,  0.3506,  0.1409, -0.3007,  0.1351,\n",
      "           0.1403],\n",
      "         [ 0.0439,  0.1278,  0.1848,  0.1733,  0.1473, -0.3093,  0.0320,\n",
      "          -0.0543],\n",
      "         [-0.2467, -0.1078,  0.2722,  0.5128,  0.2095, -0.3345,  0.0368,\n",
      "           0.0082],\n",
      "         [-0.1638,  0.0053,  0.3753,  0.3111,  0.2014, -0.2889, -0.0797,\n",
      "          -0.1775],\n",
      "         [ 0.0264,  0.1455,  0.3622,  0.0182,  0.1277, -0.1358, -0.0388,\n",
      "          -0.1381]],\n",
      "\n",
      "        [[ 0.0960,  0.4257,  1.7419,  0.2045,  0.5009, -0.1129, -0.5656,\n",
      "          -1.4563],\n",
      "         [-0.0967,  0.2774,  1.1946,  0.5023,  0.5062, -0.4307, -0.3605,\n",
      "          -0.9781],\n",
      "         [ 0.1017,  0.2037,  0.4849,  0.1862,  0.2461, -0.3946, -0.0034,\n",
      "          -0.1947],\n",
      "         [-0.0775,  0.1062,  0.3737,  0.3387,  0.0933, -0.2201,  0.0731,\n",
      "           0.0264],\n",
      "         [-0.1181, -0.0113,  0.1070,  0.2743,  0.1051, -0.3021,  0.0801,\n",
      "           0.1876]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "execution_count": 390
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This version of `MultiHeadAttention` is closer to real-world implementations, such as in GPT models.\n",
    "- Instead of multiple separate `CausalAttention` modules, it splits `Q`, `K`, `V` into multiple heads via reshaping.\n",
    "- It uses a causal mask to ensure autoregressive behavior (no peeking into the future).\n"
   ],
   "id": "f9946bb3409556ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:39:01.816747Z",
     "start_time": "2025-06-04T02:39:01.806171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements multi-head attention by splitting the attention matrix into multiple heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_K = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_V = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        self.register_buffer(\"mask\", mask.bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (batch_size, seq_len, d_in)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Split Q, K, V into multiple heads\n",
    "        # (batch_size, seq_len, d_in) -> (batch_size, seq_len, d_out) ->\n",
    "        # -> (batch_size, seq_len, num_heads, head_dim) -> (batch_size, num_heads, seq_len, head_dim)\n",
    "        Q = self.W_Q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.W_K(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_V(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = Q @ K.transpose(-2, -1) / (d_out ** 0.5)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # Apply causal mask\n",
    "        scores = scores.masked_fill(self.mask[:seq_len, :seq_len], -torch.inf)\n",
    "\n",
    "        # Compute softmax weights and apply dropout\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # Compute output\n",
    "        output = weights @ V  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        # Concatenate heads and project to output dimension\n",
    "        # (batch_size, num_heads, seq_len, head_dim) -> (batch_size, seq_len, num_heads, head_dim)\n",
    "        # ->   (batch_size, seq_len, d_out)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "\n",
    "        # Should be helpful, but not strictly necessary.\n",
    "        output = self.out_proj(output)\n",
    "        return output\n"
   ],
   "id": "6ff7e0f86dd853d0",
   "outputs": [],
   "execution_count": 399
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T02:39:03.971796Z",
     "start_time": "2025-06-04T02:39:03.966213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch = torch.randn(2, 5, 4)  # (batch_size=2, seq_len=5, d_in=4)\n",
    "d_in = 4\n",
    "d_out = 4\n",
    "context_length = batch.size(1)\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, dropout=0.0,num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ],
   "id": "70654944612728eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2657e-02,  9.4098e-02,  5.0433e-01, -3.9059e-02],\n",
      "         [-9.6895e-02,  2.1881e-01,  4.9740e-01, -1.2254e-01],\n",
      "         [ 1.3711e-01,  8.5380e-02,  4.5659e-01,  1.4091e-01],\n",
      "         [-3.8697e-02,  2.3587e-01,  4.2477e-01, -1.3345e-04],\n",
      "         [-8.8685e-02,  2.2154e-01,  4.9020e-01, -1.0253e-01]],\n",
      "\n",
      "        [[-6.3168e-02,  7.6872e-01,  3.6516e-01,  1.5360e-01],\n",
      "         [ 9.7668e-02,  4.4260e-01,  3.8732e-01,  2.5749e-01],\n",
      "         [-1.3098e-01,  2.7777e-01,  4.8771e-01, -1.3195e-01],\n",
      "         [ 4.0018e-02,  2.8622e-01,  4.8036e-01,  4.4917e-02],\n",
      "         [-6.8013e-02,  5.6066e-02,  4.8472e-01, -1.0871e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "execution_count": 402
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
